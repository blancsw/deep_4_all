{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Imports",
   "id": "c11bb4a3c7a10680"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 2. Embeddings\n",
    "\n",
    "BERT utilise plusieurs embeddings :\n",
    "- **Token embeddings** : pour représenter chaque token (mot ou sous-mot).\n",
    "- **Segment embeddings** : pour distinguer les phrases dans une même séquence (phrase A vs phrase B).\n",
    "- **Position embeddings** : pour encoder la position d’un token dans la séquence (indispensable car le Transformer n’a pas de mécanisme récurrent implicite).\n",
    "\n",
    "Ici, on définit une classe `BertEmbeddings` qui fusionne ces trois embeddings :"
   ],
   "id": "f60c83ad5ced2879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size):\n",
    "        super().__init__()\n",
    "        # Embedding pour les tokens\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        # Embedding pour la position\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        # Embedding pour le segment (type de phrase)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        # Normalisation et dropout comme dans BERT\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_length]\n",
    "        token_type_ids: [batch_size, seq_length] (facultatif, sinon 0)\n",
    "        \"\"\"\n",
    "        seq_length = input_ids.size(1)\n",
    "\n",
    "        # Si token_type_ids n'est pas fourni, on crée un tenseur de zéros\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # Position ids: [0, 1, 2, ..., seq_length-1]\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        # Calcul des embeddings\n",
    "        word_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = word_embeds + position_embeds + token_type_embeds\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ],
   "id": "dd5aa32a25b6e21e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 3. Multi-Head Self-Attention\n",
    "\n",
    "La self-attention multi-tête consiste à projeter le même input en plusieurs espaces (Query, Key, Value), appliquer une attention *Scaled Dot-Product*, puis concaténer les sorties pour obtenir un vecteur final.\n"
   ],
   "id": "535b46775c2edc37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Le hidden_size doit être divisible par le nombre de têtes.\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        # Matrices de projection pour Q, K, V\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key   = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Projection finale après la concaténation\n",
    "        self.out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        hidden_states: [batch_size, seq_length, hidden_size]\n",
    "        attention_mask: [batch_size, 1, 1, seq_length] ou [batch_size, seq_length] etc.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "\n",
    "        # Projections\n",
    "        Q = self.query(hidden_states)  # [batch_size, seq_length, hidden_size]\n",
    "        K = self.key(hidden_states)\n",
    "        V = self.value(hidden_states)\n",
    "\n",
    "        # On reshape pour séparer les têtes\n",
    "        # => [batch_size, seq_length, num_heads, head_dim]\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # On transpose pour que les têtes soient sur le deuxième axe\n",
    "        # => [batch_size, num_heads, seq_length, head_dim]\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Calcul des scores d'attention : Q * K^T / sqrt(d)\n",
    "        # Q: [batch_size, num_heads, seq_length, head_dim]\n",
    "        # K: [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        # On transpose K pour avoir la dimension de head_dim en dernière position\n",
    "        # => K: [batch_size, num_heads, head_dim, seq_length]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        # => scores: [batch_size, num_heads, seq_length, seq_length]\n",
    "\n",
    "        # Application du masque d'attention (optionnel)\n",
    "        if attention_mask is not None:\n",
    "            # On suppose que le masque a la même forme ou est broadcastable\n",
    "            # Les positions masquées sont mises à un score très bas (ex: -1e9)\n",
    "            scores = scores + attention_mask\n",
    "\n",
    "        # Softmax sur la dimension des \"keys\"\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Calcul de la sortie en tenant compte des poids d'attention\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        # => [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        # On remet la dimension des têtes et de head_dim ensemble\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        # => [batch_size, seq_length, num_heads, head_dim]\n",
    "        context = context.view(batch_size, seq_length, self.num_heads * self.head_dim)\n",
    "        # => [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        # Projection finale\n",
    "        output = self.out(context)\n",
    "        # => [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        return output"
   ],
   "id": "dbc982faa1e95297"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 4. Feed-Forward\n",
    "\n",
    "Chaque bloc Transformer contient aussi un sous-module feed-forward (MLP) appliqué en parallèle après la self-attention.\n"
   ],
   "id": "1635f4e18d9f54f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.activation = nn.GELU()  # BERT utilise GELU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "8ac9e7f1a9e42bad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 5. Bloc Encoder (Transformer Layer)\n",
    "\n",
    "Un bloc BERT combine :\n",
    "1. **Multi-Head Self-Attention** avec *Add & LayerNorm*.\n",
    "2. **Feed Forward** avec *Add & LayerNorm*."
   ],
   "id": "c09e83e346aab626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(hidden_size, num_heads)\n",
    "        self.attention_layer_norm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "\n",
    "        self.ff = PositionwiseFeedForward(hidden_size, intermediate_size)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Self-attention\n",
    "        attn_output = self.attention(hidden_states, attention_mask)\n",
    "        # Add + Norm\n",
    "        hidden_states = self.attention_layer_norm(hidden_states + self.dropout(attn_output))\n",
    "\n",
    "        # Feed Forward\n",
    "        ff_output = self.ff(hidden_states)\n",
    "        # Add + Norm\n",
    "        hidden_states = self.ff_layer_norm(hidden_states + self.dropout(ff_output))\n",
    "\n",
    "        return hidden_states"
   ],
   "id": "75cd111607f2c56c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 6. L’encodeur BERT complet\n",
    "\n",
    "BERT-Base a 12 couches d’encodeurs, BERT-Large en a 24, etc. Ici on peut choisir un nombre de couches plus réduit pour simplifier.\n"
   ],
   "id": "58c049e7fc272c22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, num_heads, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            BertLayer(hidden_size, num_heads, intermediate_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        return hidden_states"
   ],
   "id": "d47523dab7ce0b39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 7. Modèle BERT complet\n",
    "\n",
    "On assemble les embeddings et l’encodeur."
   ],
   "id": "47b3b667be640883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleBertModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=30522,          # Taille du vocab (ex: BERT base)\n",
    "                 hidden_size=128,           # Dim cachée (768 pour BERT base)\n",
    "                 num_heads=4,              # Nb de têtes (12 pour BERT base)\n",
    "                 num_layers=4,             # Nombre de couches (12 pour BERT base)\n",
    "                 intermediate_size=256,     # Dim intermédiaire (3072 pour BERT base)\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = BertEmbeddings(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            type_vocab_size=type_vocab_size\n",
    "        )\n",
    "\n",
    "        self.encoder = BertEncoder(\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            intermediate_size=intermediate_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_length]\n",
    "        token_type_ids: [batch_size, seq_length]\n",
    "        attention_mask: [batch_size, seq_length] (1 pour token valide, 0 pour token masqué/padding)\n",
    "        \"\"\"\n",
    "        # Embeddings\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "\n",
    "        # Si on a un attention_mask au format [batch_size, seq_length],\n",
    "        # on va le transformer pour qu'il soit broadcastable (ex: [batch_size, 1, 1, seq_length])\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask: 1 => gardé, 0 => masqué\n",
    "            # BERT attend -inf (ex: -1e9) sur positions masquées\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_length]\n",
    "            extended_mask = extended_mask.to(dtype=embedding_output.dtype)  # correspondance de type\n",
    "            extended_mask = (1.0 - extended_mask) * -1e9\n",
    "        else:\n",
    "            extended_mask = None\n",
    "\n",
    "        # Passage par l'encodeur\n",
    "        encoder_output = self.encoder(embedding_output, attention_mask=extended_mask)\n",
    "\n",
    "        return encoder_output"
   ],
   "id": "3aa45a5f029d72d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 8. Exemple d’utilisation\n",
    "\n",
    "On crée un *batch* de données factices : `batch_size=2`, `seq_length=6`."
   ],
   "id": "bace8632c23d931f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Paramètres\n",
    "batch_size = 2\n",
    "seq_length = 6\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = SimpleBertModel(\n",
    "    vocab_size=1000,  # Petit vocab fictif\n",
    "    hidden_size=128,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    intermediate_size=256,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2\n",
    ")\n",
    "\n",
    "# Données fictives\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_length))\n",
    "token_type_ids = torch.zeros(batch_size, seq_length, dtype=torch.long)\n",
    "attention_mask = torch.ones(batch_size, seq_length, dtype=torch.long)\n",
    "\n",
    "# Passage dans le modèle\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "print(\"Shape de la sortie :\", output.shape)\n",
    "# Sortie attendue: [batch_size, seq_length, hidden_size]"
   ],
   "id": "2ff321f5017b4648"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "1. **Embeddings** : BERT combine les embeddings de token, de position et de segment.\n",
    "2. **Multi-Head Self-Attention** : permet à chaque position de s’attendre elle-même sur toutes les autres positions.\n",
    "3. **Add & Norm** : on ajoute la sortie de l’attention à l’entrée (résidual) puis on normalise.\n",
    "4. **Feed-Forward** : applique un MLP (souvent 2 couches) à chaque position indépendamment.\n",
    "5. **Empilement** : BERT comporte plusieurs blocs (12 pour BERT Base).\n",
    "6. **Masque d’attention** : gère les tokens *paddés* (ou masqués dans d’autres tâches).\n",
    "\n",
    "Cette implémentation est **très** simplifiée et n’inclut pas :\n",
    "- Le *pooler* final habituel de BERT (souvent un `Linear + Tanh` sur le premier token `[CLS]`).\n",
    "- Les *head* de classification ou de *masked language modeling*.\n",
    "- Des optimisations (mixed precision, etc.) et d’autres détails d’initialisation fidèles à l’article d’origine.\n",
    "\n",
    "Elle sert surtout à montrer la structure générale d’un *Encoder-Only Transformer* à la BERT."
   ],
   "id": "767deffff80da671"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b29c9acbb7fb6989"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

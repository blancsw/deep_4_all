{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Encodeur Transformer\n",
    "\n",
    "![Transformers](./asset/trasnformers_encoder_decoder.png)\n",
    "\n",
    "Tout d'abord, nous allons examiner l'architecture du Transformer, mais avec uniquement un encodeur.\n",
    "\n",
    "Voici un exemple **simplifié** d’un *Transformer Decoder-Only* (type GPT). L’objectif est de montrer la différence principale avec un *Encoder-Only* : ici, chaque couche applique une *causal self-attention* (unidirectionnelle) qui empêche un token de regarder les tokens futurs. Il n’y a ni encodeur séparé ni cross-attention.\n",
    "\n",
    "Nous allons reprendre l’ossature générale (embeddings, attention multi-tête, feed-forward, etc.) en adaptant :\n",
    "- **Le masque** : on utilise un *causal mask* (triangulaire) plutôt qu’un masque bidirectionnel ou de padding.\n",
    "- **Les couches** : on n’a pas de cross-attention, seulement une couche de *masked (causal) self-attention* suivie d’un feed-forward.\n",
    "\n",
    "![decoder-only](./asset/decoder_only_mask.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Différences clés entre *Encoder-Only* et *Decoder-Only*\n",
    "\n",
    "1. **Masque d’attention** :\n",
    "   - **Encoder-Only (BERT)** : il peut s’attendre sur tout le contexte (bidirectionnel), mais masque souvent des tokens de padding ou masqués.\n",
    "   - **Decoder-Only (GPT)** : causal (unidirectionnel). Chaque token ne peut voir que les tokens précédents (positions antérieures).\n",
    "\n",
    "2. **Architecture** :\n",
    "   - **Encoder-Only (BERT)** : on empile des blocs d’encodeur.\n",
    "   - **Decoder-Only (GPT)** : on empile des blocs de *decoder* qui, en version la plus simple, n’utilise pas de cross-attention (puisqu’il n’y a pas d’encodeur). On y retrouve tout de même la couche de *self-attention* et le feed-forward.\n",
    "\n",
    "3. **Utilisation en génération** :\n",
    "   - **Encoder-Only** : pour la classification, l’extraction de features ou le *masked language modeling*.\n",
    "   - **Decoder-Only** : pour générer du texte (autocomplétion), car il se base sur l’historique (les tokens déjà générés).\n",
    "\n",
    "![summary](./asset/summary.png)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Imports"
   ],
   "id": "fb11ad532a9cbd64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ],
   "id": "771cd9d9f79345e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# 3. Embeddings\n",
    "\n",
    "Comme pour BERT, on garde un embedding de tokens et un embedding de positions.\n",
    "Dans GPT, on ne gère généralement pas de *token_type_embeddings*."
   ],
   "id": "4d038d9e45020045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GPTEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings=512):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-5)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "\n",
    "        # Positions: [0..seq_length-1]\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
    "\n",
    "        # Embeddings\n",
    "        token_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        hidden_states = token_embeddings + position_embeddings\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ],
   "id": "dae3b0bb301f0eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Causal Multi-Head Self-Attention\n",
    "\n",
    "La seule différence majeure avec la *self-attention* classique se situe au niveau du *masque* : on applique un masque triangulaire pour interdire l’attention sur les tokens futurs."
   ],
   "id": "a76101cc6119198d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CausalMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size doit être divisible par num_heads.\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key   = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        hidden_states: [batch_size, seq_length, hidden_size]\n",
    "        attention_mask: [batch_size, 1, seq_length, seq_length] ou None\n",
    "                        Ici, on va principalement se concentrer sur le \"causal mask\"\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "\n",
    "        # Projections linéaires\n",
    "        Q = self.query(hidden_states)\n",
    "        K = self.key(hidden_states)\n",
    "        V = self.value(hidden_states)\n",
    "\n",
    "        # On reshape pour isoler les têtes\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # On transpose pour avoir [batch_size, num_heads, seq_length, head_dim]\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Calcul des scores d'attention : Q * K^T / sqrt(dim)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        # => [batch_size, num_heads, seq_length, seq_length]\n",
    "\n",
    "        # Masque causal : on veut annuler (ou fortement pénaliser) l'attention sur les positions futures.\n",
    "        # On peut créer un masque triangulaire inférieur : shape (seq_length, seq_length)\n",
    "        # True ou 1.0 => autorisé, False ou 0.0 => bloqué\n",
    "        # On suppose un masque [seq_length, seq_length] = 1 pour i >= j et 0 sinon\n",
    "        # On peut le construire et l'ajouter comme un grand score négatif.\n",
    "        causal_mask = torch.tril(torch.ones((seq_length, seq_length), device=hidden_states.device)).view(1, 1, seq_length, seq_length)\n",
    "\n",
    "        # Si on a un attention_mask externe (pour du padding par exemple), on le combine\n",
    "        # L'idée: on veut que la position i ne voie pas la position j > i, et en plus\n",
    "        # on peut masquer certains tokens.\n",
    "        # On fusionne : causal_mask ET attention_mask s'ils sont donnés\n",
    "        # Généralement, on multiplie ou on additionne avec un log prob négatif.\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask est typiquement [batch_size, 1, seq_length, seq_length]\n",
    "            mask = causal_mask * attention_mask\n",
    "        else:\n",
    "            mask = causal_mask\n",
    "\n",
    "        # mask: 1 => autorisé, 0 => bloqué\n",
    "        # On convertit en \"score\" (ici on utilise -1e9 pour bloquer)\n",
    "        mask_value = -1e9\n",
    "        scores = scores.masked_fill(mask == 0, mask_value)\n",
    "\n",
    "        # Softmax\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_length, seq_length]\n",
    "\n",
    "        # On pèse les V\n",
    "        context = torch.matmul(attn_weights, V)  # [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        # On remet la forme [batch_size, seq_length, hidden_size]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Projection finale\n",
    "        output = self.out(context)\n",
    "        return output"
   ],
   "id": "8becec22528a0d34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> **Remarque** : Dans un *Decoder-Only Transformer*, on applique ce masque causal à chaque couche. Ceci assure qu’un token ne peut pas dépendre d’un token futur.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Feed-Forward\n",
    "\n",
    "Inchangé par rapport à BERT (on utilise souvent GELU ou ReLU)."
   ],
   "id": "3701e6612feb7532"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "709cdc6a90310d4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Un bloc de Decoder-Only\n",
    "\n",
    "Un bloc (couche) *Decoder-Only* simple comprend :\n",
    "1. Causal *Self-Attention* + Add & LayerNorm\n",
    "2. Feed Forward + Add & LayerNorm\n",
    "\n",
    "(Comparé au *Decoder Transformer* standard dans le papier original, il peut y avoir une partie cross-attention si on décode depuis un encodeur, mais dans GPT-like, on n’en a pas)."
   ],
   "id": "55516449c70ee6d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.attn = CausalMultiHeadSelfAttention(hidden_size, num_heads)\n",
    "        self.attn_layer_norm = nn.LayerNorm(hidden_size, eps=1e-5)\n",
    "\n",
    "        self.ff = PositionwiseFeedForward(hidden_size, intermediate_size)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_size, eps=1e-5)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Causal Self-Attention\n",
    "        attn_output = self.attn(hidden_states, attention_mask=attention_mask)\n",
    "        hidden_states = self.attn_layer_norm(hidden_states + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-Forward\n",
    "        ff_output = self.ff(hidden_states)\n",
    "        hidden_states = self.ff_layer_norm(hidden_states + self.dropout(ff_output))\n",
    "\n",
    "        return hidden_states"
   ],
   "id": "5417a583f92fbaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7. Le Decoder-Only complet\n",
    "\n",
    "On empile plusieurs `GPTBlock`."
   ],
   "id": "892724400baf1073"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleGPTModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=30522,\n",
    "                 hidden_size=128,\n",
    "                 num_heads=4,\n",
    "                 num_layers=4,\n",
    "                 intermediate_size=256,\n",
    "                 max_position_embeddings=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.embeddings = GPTEmbeddings(vocab_size, hidden_size, max_position_embeddings)\n",
    "\n",
    "        # Bloc(s) GPT\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(hidden_size, num_heads, intermediate_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Tête de sortie (ex: LM Head)\n",
    "        # Le plus souvent, GPT partage le poids de self.word_embeddings\n",
    "        # avec cette couche finale. Pour la démo, on fait juste un linear.\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_length]\n",
    "        attention_mask: [batch_size, 1, seq_length, seq_length] (optionnel, ex. pour padding)\n",
    "        \"\"\"\n",
    "\n",
    "        # Embeddings\n",
    "        hidden_states = self.embeddings(input_ids)\n",
    "\n",
    "        # On envoie dans chaque bloc\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        # Projection finale vers les logits\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # logits: [batch_size, seq_length, vocab_size]\n",
    "        return logits"
   ],
   "id": "584853599fdeef67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# 8. Exemple d’utilisation\n",
    "\n",
    "On crée un *batch* fictif.\n"
   ],
   "id": "1cbebb2e5e0bed90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 2\n",
    "seq_length = 6\n",
    "\n",
    "model = SimpleGPTModel(\n",
    "    vocab_size=1000,\n",
    "    hidden_size=128,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    intermediate_size=256,\n",
    "    max_position_embeddings=512\n",
    ")\n",
    "\n",
    "# Données factices\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_length))\n",
    "\n",
    "# Exemple de masque d’attention (1=token valide, 0=padding)\n",
    "# Pour la causalité, on va en plus générer le masque triangulaire (à l’intérieur du module).\n",
    "attention_mask = torch.ones(batch_size, seq_length, dtype=torch.long)  # pas de padding ici\n",
    "# On convertit en [batch_size, 1, seq_length, seq_length] pour rester cohérent\n",
    "attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # => [2, 1, 1, 6]\n",
    "attention_mask = attention_mask.expand(-1, -1, seq_length, -1)  # => [2, 1, 6, 6]\n",
    "# => 1 partout => pas de blocage (sauf causal). On aurait pu laisser None.\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "print(\"Shape des logits :\", logits.shape)\n",
    "# => [2, 6, 1000]"
   ],
   "id": "e59c80595b105fff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Résumé des différences majeures\n",
    "\n",
    "1. **Masque causal (unidirectionnel)** : Dans un *Decoder-Only*, on masque les positions futures dans la matrice d’attention, alors que dans un *Encoder-Only*, chaque token peut s’attendre sur tous les autres (bidirectionnel).\n",
    "2. **Pas de cross-attention** : Dans BERT, il n’y en a pas car c’est un *Encoder-Only*, et dans ce GPT simplifié non plus, car il n’y a pas d’encodeur à interroger.\n",
    "3. **Tâche typique** : Un *Decoder-Only Transformer* est généralement utilisé pour la génération autoregressive de texte. Chaque nouveau token est prédit en fonction des tokens précédents seulement.\n",
    "4. **Embeddings** : GPT n’utilise pas (en général) de *segment embeddings*.\n",
    "\n",
    "Cette implémentation reste simple et éducative ; dans la pratique, les modèles GPT (GPT-2, GPT-3, etc.) incluent davantage de subtilités (initialisations précises, utilisation systématique du *weight tying* avec l’embedding, optimisations, etc.).\n",
    "\n",
    "> regardont l'implémenation du LLama3 [model](https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src/transformers/models/llama/modeling_llama.py#L750)"
   ],
   "id": "89bb2f74f51a3b3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RNN vs transformers\n",
    "\n",
    "## RNN summary\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Sequential Processing:** RNNs are inherently designed for sequential data processing, making them perfect for time series prediction, natural language processing, and speech recognition.\n",
    "2. **low cost inference:** RNNs tend to require fewer computational resources than Transformer models as they process input sequences step by step rather than in parallel.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Vanishing and Exploding Gradient Problem:** During back-propagation in deep RNNs, gradients are multiplied by the weight matrix at every timestep. This can result in gradients that either explode or vanish, making it challenging to train deep RNNs.\n",
    "2. **Long-term Dependencies:** RNNs struggle to learn long-term dependencies due to the vanishing gradient problem.\n",
    "3. **Cannot Process in Parallel:** The sequential nature of RNNs means they cannot take advantage of modern GPUs which excel in performing parallel operations.\n",
    "\n",
    "![triangle](./asset/rnn-vs-transformer.png)\n"
   ],
   "id": "618e767bd88e3015"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

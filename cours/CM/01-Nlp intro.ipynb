{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q63nwrxI4hRK"
   },
   "source": [
    "# NLP\n",
    "\n",
    "![deep](../asset/deep.png)\n",
    "\n",
    "# Task\n",
    "\n",
    "![deep](../asset/nlp.png)\n",
    "\n",
    "# Inputs\n",
    "\n",
    "In Vision domain we use image.\n",
    "\n",
    "We can use timeseries (array of numbers)\n",
    "\n",
    "**But in NLP wo do we convert text into numeric representation ?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jRhKT5JGxFF"
   },
   "source": [
    "# Bag of words(BOW)\n",
    "\n",
    "![bag](../asset/bag_word.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1675064120517,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "T4pbXD4m6p5k",
    "outputId": "61349412-0f8e-4fde-b781-efb9a7007db3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = [\"j'aime les frites\", \"LISP c'est trop bien !\", \"j'aime les jeux dragon's Lair\"]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Matrix\", X.toarray())\n",
    "print(\"Vocabulary\", vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77dDKVr8ECpR"
   },
   "source": [
    "Get matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1675064883703,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "sCeyN-TqDAdJ",
    "outputId": "9ba1cae3-ff19-4716-c397-2930be14b0c5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DltJU7DSEydO"
   },
   "source": [
    "Display top n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 1013,
     "status": "ok",
     "timestamp": 1675064902622,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "ww3ZjBNMEICa",
    "outputId": "c41ca98d-a063-46f9-cd55-78868691f7ad"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "\n",
    "common_words = get_top_n_words(corpus, 30)\n",
    "df = pd.DataFrame(common_words, columns=['unigram', 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df['unigram'], y=df['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 30 unigrams\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gPgdHMfGy3C"
   },
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1675065239603,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "YaLHDHtkG16G",
    "outputId": "6c159c0c-fc80-4fe4-d2d3-2ec7cd0371d9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"j'aime les frites\",\n",
    "          \"LISP c'est trop bien !\",\n",
    "          \"j'aime les jeux dragon's Lair\",\n",
    "          \"j'adore les frites\",\n",
    "          \"envoyer un mail\",\n",
    "          \"mangé saussise\"]\n",
    "\n",
    "# Vectorise the corpus\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "arr = X.toarray()\n",
    "\n",
    "\n",
    "def simlarity_search(arr, input_text):\n",
    "    # Vectorise the input text\n",
    "    input_text = vectorizer.transform([input_text]).toarray()[0]\n",
    "\n",
    "    # Compute foreach sentence in the corpus the jaccard score\n",
    "    scores = []\n",
    "    for idx in range(arr.shape[0]):\n",
    "        score = jaccard_score(input_text, arr[idx])\n",
    "        scores.append([score, corpus[idx]])\n",
    "\n",
    "    # Sort by score\n",
    "    scores = sorted(scores, key=lambda x: x[0])[::-1]\n",
    "    for score, sentence in scores:\n",
    "        print(f\"{score}: {sentence}\")\n",
    "\n",
    "\n",
    "simlarity_search(arr, \"j'aime manger des frites\")\n",
    "print(\"----------------------\")\n",
    "simlarity_search(arr, \"envoyé des email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGJVDND6L7wD"
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "The problem with text is that there are many different ways to write a word, capital `Cat, cat`. Or conjugation `help`, `helping`, `helped`, `helpful`, punctuation and stop words (the, that, etc).\n",
    "The aim is to reduce the amount of words and their diversity.\n",
    "\n",
    "To do this data cleaning we will use [spacy](https://spacy.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84223,
     "status": "ok",
     "timestamp": 1675065439851,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "FE8Imk5WL6Nh",
    "outputId": "4e2c2bd3-ba96-4dec-9f72-5fe61571f9dd"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2hc0MfkeADc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675065439851,
     "user_tz": -60,
     "elapsed": 17,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"j'aime, les frites\",\n",
    "          \"comment installer un site internet ?\",\n",
    "          \"LISP c'est trop bien !\",\n",
    "          \"j'aime les jeux dragon's Lair\",\n",
    "          \"j'adore les frites\",\n",
    "          \"Envoyer un mail\",\n",
    "          \"mangé: saussise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21323,
     "status": "ok",
     "timestamp": 1675065461160,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "VdsyUqCteOoQ",
    "outputId": "1cb31d5e-66d1-410f-9163-a3a508effe44"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2aHo5dNYpGZ"
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "The target is to split the sentence into tokens\n",
    "\n",
    "`I love to play dragon lait` --> `I`,  `love`,  `to`,  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1675065476675,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "mNlSjVffecRl",
    "outputId": "77c7682b-ae05-46df-8364-f3f0d21df6a0"
   },
   "outputs": [],
   "source": [
    "docs = [nlp(sentence) for sentence in corpus]\n",
    "for token in docs[0]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDoJ-SBjfbwB"
   },
   "source": [
    "## StopWords\n",
    "\n",
    "Stop words are common words that do not contribute much of the information in a text document. Words like `the`, `is`, `a` have less value and add noise to the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1675065516300,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "Nvapb71dfREw",
    "outputId": "9efe9734-d6cb-4668-efbc-90ae0aaf38df"
   },
   "outputs": [],
   "source": [
    "for sentence in docs:\n",
    "    clean_sentence = []\n",
    "    for token in sentence:\n",
    "        if not token.is_stop:\n",
    "            clean_sentence.append(str(token))\n",
    "    print(' '.join(clean_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cksokCb4gO0W"
   },
   "source": [
    "## Punctuation\n",
    "\n",
    "Removing punctuation can be useful. But for other embeding techniques like deeplearning, it is not the best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1675065551649,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "7MNj0njvhHBv",
    "outputId": "23eee26c-38e5-4774-f953-1a7b30bac842"
   },
   "outputs": [],
   "source": [
    "for sentence in docs:\n",
    "    clean_sentence = []\n",
    "    for token in sentence:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            clean_sentence.append(str(token))\n",
    "    print(' '.join(clean_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9Dt83wehvhC"
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "The goal is to converting a word to its root form  `help`, `helping`, `helped`, `helpful`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1675065593866,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "XUQd7iGkiYre",
    "outputId": "dc71de96-8b56-4f73-e98b-785483f673f6"
   },
   "outputs": [],
   "source": [
    "for sentence in docs:\n",
    "    clean_sentence = []\n",
    "    for token in sentence:\n",
    "\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            lem_word = token.lemma_.lower()\n",
    "        else:\n",
    "            lem_word = token.lower_\n",
    "\n",
    "        clean_sentence.append(str(lem_word))\n",
    "\n",
    "    print(' '.join(clean_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUgD8bcOkoSe"
   },
   "source": [
    "# Bag of words with data cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBOdcqoplXPk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675065644765,
     "user_tz": -60,
     "elapsed": 587,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     }
    }
   },
   "outputs": [],
   "source": [
    "clean_sentences = []\n",
    "\n",
    "\n",
    "def clean_sentence(sentence, nlp):\n",
    "    clean_sentence = []\n",
    "    for token in nlp(sentence):\n",
    "\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            lem_word = token.lemma_.lower()\n",
    "        else:\n",
    "            lem_word = token.lower_\n",
    "\n",
    "        clean_sentence.append(str(lem_word))\n",
    "\n",
    "    return ' '.join(clean_sentence)\n",
    "\n",
    "\n",
    "corpus = [\"j'aime les frites\",\n",
    "          \"LISP c'est trop bien !\",\n",
    "          \"j'aime les jeux dragon's Lair\",\n",
    "          \"j'adore les frites\",\n",
    "          \"envoyer un mail\",\n",
    "          \"mangé saussise\"]\n",
    "docs = [clean_sentence(sentence, nlp) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1675065645685,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "XVG1B5bnkyXV",
    "outputId": "3a6357c9-456c-4def-b498-86478cb83e0f"
   },
   "outputs": [],
   "source": [
    "# Vectorise the corpus\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "arr = X.toarray()\n",
    "\n",
    "simlarity_search(arr, clean_sentence(\"j'aime manger des frites\", nlp))\n",
    "print(\"----------------------\")\n",
    "simlarity_search(arr, clean_sentence(\"envoyé des email\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuo-OuEa0oAg"
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF **Vectorizer** and Count **Vectorizer** are both methods used in natural language processing to vectorize text. However, there is a fundamental difference between the two methods.\n",
    "\n",
    "CountVectorizer simply counts the number of times a word appears in a document (using a bag-of-words approach), while TF-IDF Vectorizer takes into account not only how many times a word appears in a document but also how important that word is to the whole corpus.\n",
    "\n",
    "This is done by penalizing words that often appear across all documents, reducing the count of these as these words are likely to be less important.\n",
    "\n",
    "There is no one technique better than the other, it all depends on the application, ultimately. Testing both is important.\n",
    "\n",
    "## How it works\n",
    "\n",
    "Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency:\n",
    "\n",
    "![bag](../asset/tf_idf_formul.png)\n",
    "\n",
    "## Terminology\n",
    "\n",
    "* **t** — term (word)\n",
    "* **d** — document (set of words)\n",
    "* **N** — count of corpus\n",
    "* **corpus** — the total document set\n",
    "\n",
    "## Term Frequency (TF)\n",
    "\n",
    "`tf(t,d) = count of t in d / number of words in d`\n",
    "\n",
    "## Document Frequency\n",
    "\n",
    "`df(t) = occurrence of t in documents`\n",
    "\n",
    "## Inverse Document Frequency(IDF):\n",
    "While computing TF, all terms are considered equally important. However it is known that certain terms, such as “is”, “of”, and “that”, may appear a lot of times but have little importance. \n",
    "\n",
    "`idf(t) = log(N/(df + 1))`\n",
    "\n",
    "## Final formula\n",
    "\n",
    "`tf-idf(t, d) = tf(t, d) * log(N/df)`\n",
    "\n",
    "## Example\n",
    "\n",
    "Sentence A : The car is driven on the road.\n",
    "\n",
    "Sentence B : The truck is driven on the highway.\n",
    "\n",
    "![bag](../asset/tf_idf_example.png)\n",
    "\n",
    "## Vector similarity\n",
    "\n",
    "![bag](../asset/vector_sim.png)\n",
    "\n",
    "Where, a and b are vectors in a multidimensional space.\n",
    "\n",
    "Since the cos(Ø) value is in the range [−1,1] :\n",
    "\n",
    "- −1 value will indicate strongly opposite vectors i.e. no similarity\n",
    "    - \"north\" and \"south\" are opposite\n",
    "- 0 indicates independent (or orthogonal) vectors\n",
    "    - \"dog\" and \"moon\" are generally independent and have no contextual relation.\n",
    "- 1 indicates a high similarity between the vectors \n",
    "    - \"happy\" and  \"joyful\" Both words represent positive emotions and are thus similar in the context of sentiment.\n",
    "\n",
    "![bag](../asset/formule_cosine.png)\n",
    "\n",
    "Where ||A|| is Euclidean norm\n",
    "\n",
    "![bag](../asset/euclide.png)\n",
    "\n",
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1675066001672,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "V8EPiV807383",
    "outputId": "59b879e6-8af7-407e-a597-baa34a10571c"
   },
   "outputs": [],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1675066003300,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "hHbjuiBw-wnu",
    "outputId": "a3ef8442-5386-4221-f1b6-87a23ef3beb2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(docs)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1675066084676,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "XZ6GPLIB_ed5",
    "outputId": "86ec4e0a-6a62-4d74-b851-3a2c6c649bca"
   },
   "outputs": [],
   "source": [
    "def tfidf_simlarity_search(vectorizer, dataset_matrix, dataset, input_text):\n",
    "    # Vectorise the input text\n",
    "    query_vec = vectorizer.transform([input_text])\n",
    "    # Apply cosiune similarity between the dataset and the query vector\n",
    "    results = cosine_similarity(dataset_matrix, query_vec).reshape((-1,))\n",
    "    print(f\"Query: {input_text}\")\n",
    "    for i in results.argsort()[-10:][::-1]:\n",
    "        print(f\"{i + 1} - {dataset[i]}\")\n",
    "\n",
    "\n",
    "query = clean_sentence(\"j'aime manger des frites\", nlp)\n",
    "tfidf_simlarity_search(vectorizer, vectors, docs, query)\n",
    "print(\"----------------------\")\n",
    "query = clean_sentence(\"envoyé des email\", nlp)\n",
    "tfidf_simlarity_search(vectorizer, vectors, docs, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hOTc69Kd0-4"
   },
   "source": [
    "# Classifier\n",
    "\n",
    "## k-NN: A Simple Classifier\n",
    "\n",
    "The k-Nearest Neighbor classifier is by far the most simple machine learning and image classi-\n",
    "fication algorithm. In fact, it’s so simple that it doesn’t actually “learn” anything. Instead, this\n",
    "algorithm directly relies on the distance between feature vectors (which in our case, are the raw\n",
    "RGB pixel intensities of the images).\n",
    "\n",
    "Here the good user [guide](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n",
    "\n",
    "![bag](../asset/knn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1675066342603,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "BGS_MYOEd0--",
    "outputId": "30a54dd8-d448-46a8-f48e-ac3563c9fe20"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train dataset\n",
    "x = [\"j'aime les pomme vert\", \"les orange sont pas top\", \"une grosse poire\", \"la belle poire orange\"]\n",
    "# Multilabel ground truth\n",
    "y = [['apple', 'green'], ['orange'], ['pear', 'green'], ['pear', 'orange']]\n",
    "\n",
    "# Test dataset\n",
    "x_test = [\"pomme vert bio\", \"je suis orange\", \"la belle orange poire\"]\n",
    "y_test = [['apple', 'green'], ['orange'], ['pear', 'orange']]\n",
    "\n",
    "# Encode labels\n",
    "encoder = MultiLabelBinarizer()\n",
    "y_encode = encoder.fit_transform(y)\n",
    "y_test_encode = encoder.transform(y_test)\n",
    "\n",
    "# Creat simple pipline that do tfidf \n",
    "# and train Multilabel classification model with LinearSVC \n",
    "SVC_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=3, weights=\"distance\")),\n",
    "])\n",
    "\n",
    "# train the model using X_dtm & y\n",
    "SVC_pipeline.fit(x, y_encode)\n",
    "# compute the testing accuracy\n",
    "prediction = SVC_pipeline.predict(x_test)\n",
    "print('Test accuracy is {}'.format(accuracy_score(y_test_encode, prediction)))\n",
    "print(classification_report(y_test_encode, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1674206286544,
     "user": {
      "displayName": "Swan Blanc",
      "userId": "06655654812785803318"
     },
     "user_tz": -60
    },
    "id": "G3pmIawLn1Fv",
    "outputId": "277f0375-1f18-4557-c57e-0a9803f5a6a8"
   },
   "outputs": [],
   "source": [
    "print(y_test_encode)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "authorship_tag": "ABX9TyMq1mqHDjZaWCrXVBX0jN/f"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

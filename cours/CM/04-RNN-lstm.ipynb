{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN (Réseaux de neurones récurrents)\n",
    "\n",
    "Un RNN (Recurrent Neural Network) peut être imaginé comme un réseau neuronal où les couches sont empilées verticalement. Chaque couche représente un moment dans le temps, et les flèches entre les couches symbolisent le passage d'informations d'un pas temporel à un autre. Une flèche entre et sort de chaque couche pour indiquer les entrées et sorties à chaque moment.\n",
    "\n",
    "Prenons un exemple : si vous souhaitez prédire la direction dans laquelle une balle se déplace. Avec seulement l'instantané affiché à l'écran, vous ne pourrez pas dire grand-chose. Mais en enregistrant plusieurs positions successives de la balle, vous aurez assez d'informations pour prédire son mouvement.\n",
    "\n",
    "![ball](./asset/ball.gif \"segment\")\n",
    "\n",
    "En résumé, les RNN permettent d'ajouter une **mémoire** à une séquence, par exemple pour comprendre comment une balle se déplace.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications en texte\n",
    "\n",
    "C'est pareil avec le texte. Un texte est simplement une séquence de mots.\n",
    "Les RNN permettent de mieux apprendre les relations entre ces mots.\n",
    "\n",
    "---\n",
    "\n",
    "## La mémoire séquentielle\n",
    "\n",
    "Les RNN excellent dans le traitement des données séquentielles grâce à leur \"mémoire séquentielle\". Voici une analogie :\n",
    "\n",
    "Imaginez-vous récitant l’alphabet :\n",
    "`A B C D E F G H I J K L M N O P Q R S T U V W X Y Z`\n",
    "C'est facile, car cette séquence est ancrée dans votre mémoire depuis l’enfance.\n",
    "\n",
    "Maintenant, essayez de réciter l’alphabet **à l’envers** :\n",
    "`Z Y X W V U T S R Q P O N M L K J I H G F E D C B A`\n",
    "C'est plus difficile parce que cette séquence est moins familière.\n",
    "\n",
    "Si vous commencez à partir d’une lettre comme **F**, vous aurez peut-être du mal au début, mais dès que votre cerveau reconnaît le modèle, la suite se fait naturellement. Ce phénomène résulte de la manière dont vous avez appris : la séquence familière est plus facile à réciter.\n",
    "\n",
    "---\n",
    "\n",
    "## Les Réseaux de Neurones Récurrents\n",
    "\n",
    "Les RNN encapsulent le concept de **mémoire séquentielle**. En revanche, les réseaux neuronaux conventionnels (FFNN - Feed-Forward Neural Network) traitent les entrées indépendamment. Les RNN introduisent des boucles permettant de transmettre l’information entre différentes étapes.\n",
    "\n",
    "### Comparaison visuelle\n",
    "\n",
    "1. **Réseau feed-forward classique :**\n",
    "   ![fdd](./asset/Feed_forward_neural_net.gif \"segment\")\n",
    "\n",
    "2. **Ajout de boucles pour capturer des informations séquentielles :**\n",
    "   <img src=\"./asset/Feed_forward_rnn.gif\" alt=\"image\" width=\"auto\" height=\"500\">\n",
    "\n",
    "3. **Représentation complète des itérations :**\n",
    "   <img src=\"./asset/rnn_struct.png\" alt=\"image\" width=\"auto\" height=\"300\">\n",
    "\n",
    "---\n",
    "\n",
    "## Exemple pratique : Décodage d'une phrase\n",
    "\n",
    "Prenons la phrase : **\"Quelle heure est-il ?\"**\n",
    "\n",
    "**Étape 1 :** Tokenisation (division en séquences).\n",
    "![01](./asset/rnn01.gif \"segment\")\n",
    "\n",
    "**Étape 2 :** Injecter \"Quelle\" dans le RNN.\n",
    "![02](./asset/rnn02.gif \"segment\")\n",
    "\n",
    "**Étape 3 :** Ajouter \"heure\" avec l'état caché précédent (hidden state).\n",
    "![03](./asset/rnn03.gif \"segment\")\n",
    "\n",
    "**Étape 4 :** Répéter le processus avec tous les mots. À la dernière étape, le RNN aura appris la structure de toute la phrase.\n",
    "![04](./asset/rnn04.gif \"segment\")\n",
    "\n",
    "**Étape 5 :** Utiliser la sortie finale pour une tâche, comme une classification d’intention.\n",
    "![05](./asset/rnn05.gif \"segment\")\n",
    "\n",
    "---\n",
    "\n",
    "## Pseudo-code pour un RNN\n",
    "\n",
    "Voici une représentation pseudo-codée du fonctionnement d'un RNN :\n",
    "\n",
    "```python\n",
    "# Python\n",
    "rnn = RNN()\n",
    "ff = FeedForwardNN()\n",
    "hidden_state = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "for word in sentence:\n",
    "    output, hidden_state = rnn(word, hidden_state)\n",
    "\n",
    "prediction = ff(output)\n",
    "```\n",
    "\n",
    "Une opération typique dans la couche linéaire :\n",
    "![rnn_illustrated](./asset/rnn_illustrated.gif \"segment\")\n",
    "\n",
    "---\n",
    "\n",
    "## Le problème du gradient qui disparaît (Vanishing Gradient)\n",
    "\n",
    "Le problème de \"vanishing gradient\" désigne une situation où les gradients deviennent si faibles durant la rétropropagation que les poids sont à peine mis à jour, notamment dans les couches précoces. Cela se produit lorsque des gradients inférieurs à 1 sont multipliés à plusieurs reprises, diminuant exponentiellement leur valeur.\n",
    "Conséquence : difficulté à apprendre des dépendances à long terme dans les données.\n",
    "\n",
    "![rnn gradiant](./asset/rnn_gradiant.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Résumé\n",
    "\n",
    "### **Avantages :**\n",
    "1. **Traitement séquentiel :** Idéal pour les séries temporelles, NLP (traitement du langage naturel), et reconnaissance vocale.\n",
    "2. **Faible coût d'inférence :** Moins gourmand en ressources que Transformateurs car les RNN traitent les séquences pas à pas.\n",
    "\n",
    "### **Inconvénients :**\n",
    "1. **Problèmes de gradient (explosion/disparition) :** Rend difficile l’apprentissage des dépendances sur une longue durée.\n",
    "2. **Dépendances à long terme :** Les RNN classiques peinent à capturer des relations distantes dans les séquences.\n",
    "3. **Pas de calcul parallèle :** À cause de leur nature séquentielle, les RNN n’exploitent pas pleinement les GPU modernes.\n",
    "\n",
    "---\n",
    "\n",
    "# Implémentation Python : Construire un RNN\n",
    "\n",
    "Nous allons développer et entraîner un RNN simple caractérisé par un apprentissage au **niveau des caractères**.\n",
    "L'objectif sera de prédire la langue d'un mot basé sur son orthographe. Les données incluront quelques milliers de noms de famille originaires de 18 langues."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46edbf93df6d84b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.noxrmalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    \"\"\"\n",
    "    Reads the contents of a file and returns them as a list of lines.\n",
    "\n",
    "    :param filename: The name of the file to be read.\n",
    "    :return: A list of lines read from the file.\n",
    "    \"\"\"\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "for filename in findFiles('../dataset/names/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(\"findFiles\", findFiles('../dataset/names/names/*.txt'))\n",
    "print(\"unicodeToAscii\", unicodeToAscii('Ślusàrski'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603bdabb1f9d0661",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have **category_lines**, a dictionary mapping each category (language) to a list of lines (names). We also kept track of **all_categories** (just a list of languages) and **n_categories** for later reference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e221e62543ec4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(category_lines['Italian'][:5])\n",
    "print(\"Number of categories:\", n_categories)\n",
    "print(\"Number of letters:\", n_letters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3090cbb014c531b0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turning Names into tokens\n",
    "\n",
    "Has we see in previous tutorials we will need to convert letters into tensor (vectors) with embedding layer.\n",
    "\n",
    "So we need to convert letter into ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eff020dc6e350e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "letter_to_id = {l: i for i, l in enumerate(all_letters)}\n",
    "id_to_letter = {i: l for i, l in enumerate(all_letters)}\n",
    "print(letter_to_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3241ba01c630f3a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text.\n",
    "\n",
    "    :param text: The input text to be tokenized.\n",
    "    :return: A list of token IDs corresponding to each letter in the input text.\n",
    "    \"\"\"\n",
    "    return [letter_to_id[letter] for letter in text]\n",
    "\n",
    "\n",
    "tokenizer('Bob')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e785e4882db3e2bf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the Network\n",
    "\n",
    "To keep this examples simple we want use batch `[batch size, seq len]`.\n",
    "\n",
    "But pass throw the model only one sequence (one name)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8bfa4ac4db083dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: Size of the vocabulary\n",
    "        :param hidden_size: Size of the hidden layers and embedding layer\n",
    "        :param output_size: number of classes in the dataset\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # output embedding size + hidden_size\n",
    "        self.i2h = nn.Linear(hidden_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden=None):\n",
    "        \"\"\"\n",
    "        :param input_ids: Array of toke ids [seq_len]\n",
    "        :param hidden: (optional) hidden state of previous layer\n",
    "        :return: tuple predicted output [output_size] and hidden state [hidden_size]\n",
    "        \"\"\"\n",
    "        # [seq_len, emb dim]\n",
    "        embedding = self.embedding(input_ids)\n",
    "        # [hidden_size]\n",
    "        hidden = self.initHidden() if hidden is None else hidden\n",
    "        for i in range(embedding.shape[0]):\n",
    "            # Add hidden into the embedding last dimension\n",
    "            # [emb dim + emb dim]\n",
    "            combined = torch.cat((embedding[i], hidden), 0)\n",
    "            hidden = self.i2h(combined) \n",
    "            # The tanh function is a popular choice because it maps its inputs to outputs in the range between\n",
    "            # -1 and 1, maintaining a zero center, and so it helps in reducing the leaning towards extreme predictions.\n",
    "            # This property helps in controlling the exploding gradients problem in the context of RNNs.\n",
    "            hidden = torch.tanh(hidden)\n",
    "            output = self.h2o(hidden)\n",
    "        # Expand 1 dim for the loss function\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_size, requires_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6559fd51eb95457f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# 1-Tokenize the text\n",
    "tokens = torch.tensor(tokenizer('Bob'))\n",
    "print(\"tokens\", tokens.shape)\n",
    "\n",
    "# 2- Creat the first hidden state\n",
    "hidden = torch.zeros(n_hidden)\n",
    "\n",
    "# 3- Pass throw the rnn network\n",
    "output, next_hidden = rnn(tokens, hidden)\n",
    "print(\"next_hidden\", next_hidden.shape)\n",
    "print(\"output\", output.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ddb2d841608b19",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    \"\"\"\n",
    "    Get category from output softmax\n",
    "    :param output: get from softmax\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "\n",
    "# Get the category from the output\n",
    "print(categoryFromOutput(output))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fff6b7b9423e109d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also want a quick way to get a training example (a name and its language):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4955c226bb2ccdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def randomChoice(l):\n",
    "    \"\"\"\n",
    "    :param l: list of elements to choose from\n",
    "    :return: randomly selected element from the given list\n",
    "    \"\"\"\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "\n",
    "def randomTrainingExample():\n",
    "    \"\"\"\n",
    "    Generates a random training example.\n",
    "\n",
    "    :return: a tuple containing the selected category, the selected line, the category tensor, and the tokenized line tensor (input_ids)\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    return category, line, category_tensor, torch.tensor(tokenizer(line))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, input_ids = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line, '/ input_ids =', input_ids)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d52301f077809664",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model\n",
    "\n",
    "Each loop of training will:\n",
    "- Create input tokens\n",
    "- Create a zeroed initial hidden state\n",
    "- Read each letter in and\n",
    "    - Keep hidden state for next letter\n",
    "- Compare final output to target\n",
    "- Back-propagate\n",
    "- Return the output and loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28ab1368f586a32b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 105000\n",
    "print_every = 5000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0.0\n",
    "all_losses = []\n",
    "\n",
    "# If you set this too high, it might explode. If too low, it might not learn\n",
    "learning_rate = 0.01\n",
    "# Now all it takes to train this network is show it a bunch of examples, have it make guesses, and tell it if it’s wrong.\n",
    "# For the loss function **nn.NLLLoss** is appropriate, since the last layer of the RNN is nn.LogSoftmax.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate, momentum=0.9)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "rnn.to(device)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, input_ids = randomTrainingExample()\n",
    "    # Send inputs to GPU\n",
    "    input_ids = input_ids.to(device)\n",
    "    category_tensor = category_tensor.to(device)\n",
    "    \n",
    "    hidden = rnn.initHidden().to(device)\n",
    "    # Zero your gradients for every batch!\n",
    "    # optimizer.zero_grad()\n",
    "    rnn.zero_grad()\n",
    "    # Make predictions for this batch\n",
    "    output, hidden = rnn(input_ids, hidden)\n",
    "\n",
    "    # Compute the loss and its gradients\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update gradiant manualy\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    # for p in rnn.parameters():\n",
    "    #     p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    \n",
    "    \n",
    "    # Clip the gradients\n",
    "    # The torch.nn.utils.clip_grad_norm_(parameters, max_norm) function is used to scale the gradient clipping\n",
    "    # prevents the \"exploding gradients\" problem, which can cause numerical overflow during gradient descent\n",
    "    # backpropagation.\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    current_loss += loss.item()\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        all_losses.append(current_loss / print_every)\n",
    "        print(f'  iter {iter} loss: {current_loss / print_every:.3f}')\n",
    "        current_loss = 0.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507d57430555ba11",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "826cca924d995d19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "448b42f08576ed01",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    \n",
    "    # Get random train examples\n",
    "    category, line, category_tensor, input_ids = randomTrainingExample()\n",
    "    # Init the first hidden satet\n",
    "    hidden = rnn.initHidden().to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    output, hidden = rnn(input_ids, hidden)\n",
    "    # Get the category from the output model\n",
    "    guess, guess_i = categoryFromOutput(softmax(output))\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99f395bde03c516b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM in 5min\n",
    "\n",
    "[ressources](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that excels in remembering long sequences of data, making it excellent for time series prediction, natural language processing, and other sequential tasks.\n",
    "\n",
    "A recurrent neural network works by retaining a form of memory as it processes sequences. It does this by implementing a loop within the network where information can be passed from one step in the sequence to the next.\n",
    "\n",
    "LSTMs improve on the basic RNN structure through a more complex recurrent unit which helps to control the flow of information.\n",
    "\n",
    "LSTMs can overcome the major challenge of remembering long sequences and eliminating the long-term dependency problem which traditional RNNs face. That's why they are often used in deep learning to solve complex sequential problems.\n",
    "\n",
    "![lstm](./asset/lstm.png)\n",
    "\n",
    "## Forget gate\n",
    "\n",
    "**decides what is relevant to keep**\n",
    "\n",
    "First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.\n",
    "\n",
    "![forget](./asset/lstm_forget.gif \"segment\")\n",
    "\n",
    "## Input Gate\n",
    "\n",
    "**decides what information is relevant to add from the current step**\n",
    "\n",
    "To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.\n",
    "\n",
    "![input](./asset/lstm_input.gif \"segment\")\n",
    "\n",
    "## Cell State\n",
    "\n",
    "**Add the input with the forget**\n",
    "\n",
    "Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state.\n",
    "\n",
    "![cell](./asset/lstm_cell.gif \"segment\")\n",
    "\n",
    "## Output Gate\n",
    "\n",
    "**determines what the next hidden state should be**\n",
    "\n",
    "Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.\n",
    "\n",
    "![ouput](./asset/lstm_output.gif \"segment\")\n",
    "\n",
    "\n",
    "# Code\n",
    "\n",
    "Here a pseudocode to illustrate LSTM\n",
    "\n",
    "![lstm_code](./asset/lstm_pseudo_code.png)\n",
    "\n",
    "\n",
    "1. First, the previous hidden state and the current input get concatenated. We’ll call it combine.\n",
    "2. Combine get’s fed into the forget layer. This layer removes non-relevant data.\n",
    "4. A candidate layer is created using combine. The candidate holds possible values to add to the cell state.\n",
    "3. Combine also get’s fed into the input layer. This layer decides what data from the candidate should be added to the new cell state.  \n",
    "5. After computing the forget layer, candidate layer, and the input layer, the cell state is calculated using those vectors and the previous cell state.\n",
    "6. The output is then computed.\n",
    "7. Pointwise multiplying the output and the new cell state gives us the new hidden state.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ac36548f4cb093a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

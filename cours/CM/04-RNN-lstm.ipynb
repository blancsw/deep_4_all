{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN (Recurrent neural networks)\n",
    "\n",
    "A RNN can be visualized as a neural network with layers arranged vertically. Each layer represents a moment in time. An arrow connects each layer to the layer that follows, symbolizing the passage of information from one time step to the other. Another arrow enters and exits each layer, indicating the inputs and outputs at each moment.\n",
    "\n",
    "Let’s also say you want to predict the direction that the ball was moving. So with only the information that you see on the screen, how would you do this? If you record many snapshots of the ball’s position in succession, you will have enough information to make a better prediction.\n",
    "\n",
    "\n",
    "![ball](./asset/ball.gif \"segment\")\n",
    "\n",
    "So to be short RNN can help to add memory into a sequence, like a ball know how the ball is moving\n",
    "\n",
    "## In Text\n",
    "\n",
    "In text is the same. A text is just a sequence of words\n",
    "\n",
    "So RNN will help to learn more relationship between words\n",
    "\n",
    "## Memory\n",
    "\n",
    "Recurrent Neural Networks (RNNs) excel in processing sequence data and making predictions thanks to their trait I call 'sequential memory'. To gain a visceral understanding of what sequential memory entails, consider this analogy:\n",
    "\n",
    "Visualize yourself reciting the alphabet:\n",
    "\n",
    "\n",
    "A B C D E F G H I J K L M N O P Q R S T U V W X Y Z\n",
    "\n",
    "\n",
    "Quite facile, wasn't it? It comes naturally because this specific sequence is ingrained in your memory from a young age.\n",
    "\n",
    "Now, attempt to recite the alphabet backward:\n",
    "\n",
    "\n",
    "Z Y X W V U T S R Q P O N M L K J I H G F E D C B A\n",
    "\n",
    "\n",
    "This task is decidedly more challenging, unless you've undergone explicit practice. The sequence is unfamiliar, hence it's more difficult to recite.\n",
    "Let's try another exercise. Begin at the letter 'F':\n",
    "\n",
    "\n",
    "F...\n",
    "\n",
    "\n",
    "Although the initial few letters might be a struggle, as soon as your brain identifies the pattern, the rest will follow effortlessly.\n",
    "\n",
    "The rationale behind this challenge lies very much in the learning process. You're taught the alphabet as a sequence. Your brain uses sequential memory as a tool for recognizing sequence patterns more efficiently, making familiar sequences easier to recall.\n",
    "\n",
    "The same principle applies to RNNs. Their sequential memory enables them to understand and predict patterns within sequential data.\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "RNNs encapsulate the abstract concept of sequential memory in their operation. But how exactly does an RNN manifest this idea? To understand this, we need to examine a conventional neural network, also known as a feed-forward neural network. Such a network is composed of an input layer, a hidden layer, and an output layer.\n",
    "\n",
    "![fdd](./asset/Feed_forward_neural_net.gif \"segment\")\n",
    "\n",
    "How do we get a feed-forward neural network to be able to use previous information to effect later ones? What if we add a loop in the neural network that can pass prior information forward?\n",
    "\n",
    "\n",
    "<img src=\"./asset/Feed_forward_rnn.gif\" alt=\"image\" width=\"auto\" height=\"500\">\n",
    "\n",
    "\n",
    "Here an other representation of the full iteration in sequence\n",
    "\n",
    "<img src=\"./asset/rnn_struct.png\" alt=\"image\" width=\"auto\" height=\"300\">\n",
    "\n",
    "### Example\n",
    "\n",
    "Lets take the sentence: \"what time is it?\"\n",
    "\n",
    "**1**\n",
    "\n",
    "Split into sequence (tokens)\n",
    "\n",
    "![01](./asset/rnn01.gif \"segment\")\n",
    "\n",
    "**2**\n",
    "\n",
    "Feed “What” into the RNN. The RNN encodes “What” and produces an output.\n",
    "\n",
    "![02](./asset/rnn02.gif \"segment\")\n",
    "\n",
    "**3**\n",
    "\n",
    "For the next step, we feed the word “time” and the hidden state from the previous step. The RNN now has information on both the word “What” and “time.”\n",
    "\n",
    "![03](./asset/rnn03.gif \"segment\")\n",
    "\n",
    "**4**\n",
    "\n",
    "We repeat this process, until the final step. You can see by the final step the RNN has encoded information from all the words in previous steps.\n",
    "\n",
    "![04](./asset/rnn04.gif \"segment\")\n",
    "\n",
    "**5**\n",
    "\n",
    "Since the final output was created from the rest of the sequence, we should be able to take the final output and pass it to the feed-forward layer to classify an intent.\n",
    "\n",
    "![05](./asset/rnn05.gif \"segment\")\n",
    "\n",
    "### Pseudo code\n",
    "\n",
    "```python\n",
    "rnn = RNN()\n",
    "ff = FeedForwardNN()\n",
    "hidden_state = [0.0, 0.0, 0.0, 0.0]\n",
    "for word in sentence:\n",
    "    output, hidden_state = rnn(word, hidden_state)\n",
    "preduction = ff(output)\n",
    "```\n",
    "\n",
    "Here the operation do in the Linear layer\n",
    "\n",
    "![rnn_illustrated](./asset/rnn_illustrated.gif \"segment\")\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "The \"vanishing gradient\" problem in RNNs refers to the situation where the gradients of the loss function become so small during backpropagation that the weights of the network are barely updated, especially for the earlier layers. This happens because the gradient values, which are less than 1, are multiplied together multiple times during backpropagation through time, causing them to diminish exponentially. As a result, the network has difficulty learning and capturing long-range dependencies in the data.\n",
    "\n",
    "![rnn gradiant](./asset/rnn_gradiant.png)\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Sequential Processing:** RNNs are inherently designed for sequential data processing, making them perfect for time series prediction, natural language processing, and speech recognition.\n",
    "2. **low cost inference:** RNNs tend to require fewer computational resources than Transformer models as they process input sequences step by step rather than in parallel.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Vanishing and Exploding Gradient Problem:** During back-propagation in deep RNNs, gradients are multiplied by the weight matrix at every timestep. This can result in gradients that either explode or vanish, making it challenging to train deep RNNs.\n",
    "2. **Long-term Dependencies:** RNNs struggle to learn long-term dependencies due to the vanishing gradient problem.\n",
    "3. **Cannot Process in Parallel:** The sequential nature of RNNs means they cannot take advantage of modern GPUs which excel in performing parallel operations.\n",
    "\n",
    "# RNN in python\n",
    "\n",
    "We will be building and training a basic character-level Recurrent Neural Network (RNN) to classify words.\n",
    "\n",
    "Specifically, we’ll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
    "\n",
    "## Preparing the Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46edbf93df6d84b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.noxrmalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    \"\"\"\n",
    "    Reads the contents of a file and returns them as a list of lines.\n",
    "\n",
    "    :param filename: The name of the file to be read.\n",
    "    :return: A list of lines read from the file.\n",
    "    \"\"\"\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "for filename in findFiles('../dataset/names/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(\"findFiles\", findFiles('../dataset/names/names/*.txt'))\n",
    "print(\"unicodeToAscii\", unicodeToAscii('Ślusàrski'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603bdabb1f9d0661",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have **category_lines**, a dictionary mapping each category (language) to a list of lines (names). We also kept track of **all_categories** (just a list of languages) and **n_categories** for later reference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e221e62543ec4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(category_lines['Italian'][:5])\n",
    "print(\"Number of categories:\", n_categories)\n",
    "print(\"Number of letters:\", n_letters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3090cbb014c531b0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Turning Names into tokens\n",
    "\n",
    "Has we see in previous tutorials we will need to convert letters into tensor (vectors) with embedding layer.\n",
    "\n",
    "So we need to convert letter into ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eff020dc6e350e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "letter_to_id = {l: i for i, l in enumerate(all_letters)}\n",
    "id_to_letter = {i: l for i, l in enumerate(all_letters)}\n",
    "print(letter_to_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3241ba01c630f3a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text.\n",
    "\n",
    "    :param text: The input text to be tokenized.\n",
    "    :return: A list of token IDs corresponding to each letter in the input text.\n",
    "    \"\"\"\n",
    "    return [letter_to_id[letter] for letter in text]\n",
    "\n",
    "\n",
    "tokenizer('Bob')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e785e4882db3e2bf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the Network\n",
    "\n",
    "To keep this examples simple we want use batch `[batch size, seq len]`.\n",
    "\n",
    "But pass throw the model only one sequence (one name)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8bfa4ac4db083dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: Size of the vocabulary\n",
    "        :param hidden_size: Size of the hidden layers and embedding layer\n",
    "        :param output_size: number of classes in the dataset\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # output embedding size + hidden_size\n",
    "        self.i2h = nn.Linear(hidden_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden=None):\n",
    "        \"\"\"\n",
    "        :param input_ids: Array of toke ids [seq_len]\n",
    "        :param hidden: (optional) hidden state of previous layer\n",
    "        :return: tuple predicted output [output_size] and hidden state [hidden_size]\n",
    "        \"\"\"\n",
    "        # [seq_len, emb dim]\n",
    "        embedding = self.embedding(input_ids)\n",
    "        # [hidden_size]\n",
    "        hidden = self.initHidden() if hidden is None else hidden\n",
    "        for i in range(embedding.shape[0]):\n",
    "            # Add hidden into the embedding last dimension\n",
    "            # [emb dim + emb dim]\n",
    "            combined = torch.cat((embedding[i], hidden), 0)\n",
    "            hidden = self.i2h(combined) \n",
    "            # The tanh function is a popular choice because it maps its inputs to outputs in the range between\n",
    "            # -1 and 1, maintaining a zero center, and so it helps in reducing the leaning towards extreme predictions.\n",
    "            # This property helps in controlling the exploding gradients problem in the context of RNNs.\n",
    "            hidden = torch.tanh(hidden)\n",
    "            output = self.h2o(hidden)\n",
    "        # Expand 1 dim for the loss function\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_size, requires_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6559fd51eb95457f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# 1-Tokenize the text\n",
    "tokens = torch.tensor(tokenizer('Bob'))\n",
    "print(\"tokens\", tokens.shape)\n",
    "\n",
    "# 2- Creat the first hidden state\n",
    "hidden = torch.zeros(n_hidden)\n",
    "\n",
    "# 3- Pass throw the rnn network\n",
    "output, next_hidden = rnn(tokens, hidden)\n",
    "print(\"next_hidden\", next_hidden.shape)\n",
    "print(\"output\", output.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ddb2d841608b19",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    \"\"\"\n",
    "    Get category from output softmax\n",
    "    :param output: get from softmax\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "\n",
    "# Get the category from the output\n",
    "print(categoryFromOutput(output))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fff6b7b9423e109d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also want a quick way to get a training example (a name and its language):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4955c226bb2ccdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def randomChoice(l):\n",
    "    \"\"\"\n",
    "    :param l: list of elements to choose from\n",
    "    :return: randomly selected element from the given list\n",
    "    \"\"\"\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "\n",
    "def randomTrainingExample():\n",
    "    \"\"\"\n",
    "    Generates a random training example.\n",
    "\n",
    "    :return: a tuple containing the selected category, the selected line, the category tensor, and the tokenized line tensor (input_ids)\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    return category, line, category_tensor, torch.tensor(tokenizer(line))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, input_ids = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line, '/ input_ids =', input_ids)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d52301f077809664",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model\n",
    "\n",
    "Each loop of training will:\n",
    "- Create input tokens\n",
    "- Create a zeroed initial hidden state\n",
    "- Read each letter in and\n",
    "    - Keep hidden state for next letter\n",
    "- Compare final output to target\n",
    "- Back-propagate\n",
    "- Return the output and loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28ab1368f586a32b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 105000\n",
    "print_every = 5000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0.0\n",
    "all_losses = []\n",
    "\n",
    "# If you set this too high, it might explode. If too low, it might not learn\n",
    "learning_rate = 0.01\n",
    "# Now all it takes to train this network is show it a bunch of examples, have it make guesses, and tell it if it’s wrong.\n",
    "# For the loss function **nn.NLLLoss** is appropriate, since the last layer of the RNN is nn.LogSoftmax.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate, momentum=0.9)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "rnn.to(device)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, input_ids = randomTrainingExample()\n",
    "    # Send inputs to GPU\n",
    "    input_ids = input_ids.to(device)\n",
    "    category_tensor = category_tensor.to(device)\n",
    "    \n",
    "    hidden = rnn.initHidden().to(device)\n",
    "    # Zero your gradients for every batch!\n",
    "    # optimizer.zero_grad()\n",
    "    rnn.zero_grad()\n",
    "    # Make predictions for this batch\n",
    "    output, hidden = rnn(input_ids, hidden)\n",
    "\n",
    "    # Compute the loss and its gradients\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update gradiant manualy\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    # for p in rnn.parameters():\n",
    "    #     p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    \n",
    "    \n",
    "    # Clip the gradients\n",
    "    # The torch.nn.utils.clip_grad_norm_(parameters, max_norm) function is used to scale the gradient clipping\n",
    "    # prevents the \"exploding gradients\" problem, which can cause numerical overflow during gradient descent\n",
    "    # backpropagation.\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    current_loss += loss.item()\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        all_losses.append(current_loss / print_every)\n",
    "        print(f'  iter {iter} loss: {current_loss / print_every:.3f}')\n",
    "        current_loss = 0.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507d57430555ba11",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "826cca924d995d19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "448b42f08576ed01",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    \n",
    "    # Get random train examples\n",
    "    category, line, category_tensor, input_ids = randomTrainingExample()\n",
    "    # Init the first hidden satet\n",
    "    hidden = rnn.initHidden().to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    output, hidden = rnn(input_ids, hidden)\n",
    "    # Get the category from the output model\n",
    "    guess, guess_i = categoryFromOutput(softmax(output))\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99f395bde03c516b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM in 5min\n",
    "\n",
    "[ressources](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that excels in remembering long sequences of data, making it excellent for time series prediction, natural language processing, and other sequential tasks.\n",
    "\n",
    "A recurrent neural network works by retaining a form of memory as it processes sequences. It does this by implementing a loop within the network where information can be passed from one step in the sequence to the next.\n",
    "\n",
    "LSTMs improve on the basic RNN structure through a more complex recurrent unit which helps to control the flow of information.\n",
    "\n",
    "LSTMs can overcome the major challenge of remembering long sequences and eliminating the long-term dependency problem which traditional RNNs face. That's why they are often used in deep learning to solve complex sequential problems.\n",
    "\n",
    "![lstm](./asset/lstm.png)\n",
    "\n",
    "## Forget gate\n",
    "\n",
    "**decides what is relevant to keep**\n",
    "\n",
    "First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.\n",
    "\n",
    "![forget](./asset/lstm_forget.gif \"segment\")\n",
    "\n",
    "## Input Gate\n",
    "\n",
    "**decides what information is relevant to add from the current step**\n",
    "\n",
    "To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.\n",
    "\n",
    "![input](./asset/lstm_input.gif \"segment\")\n",
    "\n",
    "## Cell State\n",
    "\n",
    "**Add the input with the forget**\n",
    "\n",
    "Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state.\n",
    "\n",
    "![cell](./asset/lstm_cell.gif \"segment\")\n",
    "\n",
    "## Output Gate\n",
    "\n",
    "**determines what the next hidden state should be**\n",
    "\n",
    "Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.\n",
    "\n",
    "![ouput](./asset/lstm_output.gif \"segment\")\n",
    "\n",
    "\n",
    "# Code\n",
    "\n",
    "Here a pseudocode to illustrate LSTM\n",
    "\n",
    "![lstm_code](./asset/lstm_pseudo_code.png)\n",
    "\n",
    "\n",
    "1. First, the previous hidden state and the current input get concatenated. We’ll call it combine.\n",
    "2. Combine get’s fed into the forget layer. This layer removes non-relevant data.\n",
    "4. A candidate layer is created using combine. The candidate holds possible values to add to the cell state.\n",
    "3. Combine also get’s fed into the input layer. This layer decides what data from the candidate should be added to the new cell state.  \n",
    "5. After computing the forget layer, candidate layer, and the input layer, the cell state is calculated using those vectors and the previous cell state.\n",
    "6. The output is then computed.\n",
    "7. Pointwise multiplying the output and the new cell state gives us the new hidden state.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ac36548f4cb093a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

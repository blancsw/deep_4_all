{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Apprentissage Paramétrisé\n",
    "\n",
    "Le classificateur k-NN (k-Nearest Neighbors) – un modèle d'apprentissage automatique si simple **qu'il ne réalise aucun \"apprentissage\" au sens propre.**\n",
    "Nous n'avons qu'à stocker les données d'entraînement dans le modèle, puis effectuer les prédictions au moment du test en comparant les points de données test avec les données d'entraînement.\n",
    "\n",
    "## Comment \"apprendre\" quelque chose ?\n",
    "\n",
    "### Paramétrisation\n",
    "La **paramétrisation** est le processus de définition des paramètres nécessaires d’un modèle donné.\n",
    "\n",
    "### Fonction de Perte (Loss Function)\n",
    "Une fonction de perte quantifie dans quelle mesure les étiquettes prédites sont en accord avec les étiquettes de vérité terrain.\n",
    "**Illustration :**\n",
    "\n",
    "\n",
    "![Fonction de perte](./asset/loss.png)\n",
    "\n",
    "### Poids et biais (Weights and Biases)\n",
    "- La **matrice de poids**, généralement notée **W**, et le **vecteur de biais**, noté **b**, sont appelés paramètres du classificateur.\n",
    "Ces paramètres sont ajustés lors de l’entraînement pour augmenter l’exactitude des classifications, en fonction du retour de la fonction de perte.\n",
    "\n",
    "#### Biais\n",
    "Un **valeur de biais permet de décaler la fonction d'activation vers la gauche ou la droite**, ce qui peut être indispensable pour un apprentissage réussi.\n",
    "Par exemple :\n",
    "Sans biais :\n",
    "\n",
    "![Sans biais](./asset/without_bias.png)\n",
    "\n",
    "Avec biais (décalage vers la droite) :\n",
    "\n",
    "![Avec biais](./asset/with_bias.png)\n",
    "\n",
    "Ainsi, le réseau peut produire une sortie de 0 lorsque **x = 2** grâce au biais ajouté.\n",
    "\n",
    "# Classification Linéaire\n",
    "\n",
    "Supposons que notre ensemble d’entraînement soit représenté par **xi**, où chaque image possède une étiquette de classe associée **yi**.\n",
    "Avec :\n",
    "- **i = 1, ..., N** (N points de données)\n",
    "- **yi = 1, ..., K** (K classes uniques possibles)\n",
    "\n",
    "Cela signifie que nous avons **N** points de données de dimensionnalité **D**, répartis dans **K catégories uniques**.\n",
    "\n",
    "### Fonction de Score\n",
    "Nous cherchons une fonction de score **f** qui associe chaque image à des scores de classe.\n",
    "Une méthode simple consiste à utiliser un mappage linéaire :\n",
    "\n",
    "**f(xi, W, b) = W · xi + b**\n",
    "où :\n",
    "- **W · xi** désigne le produit scalaire entre **W** et **xi**.\n",
    "\n",
    "### Illustration : Produit Scalaire\n",
    "Voici une illustration simple du produit scalaire :\n",
    "\n",
    "![Illustration produit scalaire](./asset/parameters_learn.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q opencv-contrib-python matplotlib numpy scikit-learn torch torchvision nbconvert",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "RifxESXonWn1"
   },
   "cell_type": "markdown",
   "source": "## Demo"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JQosCAJ9oZKh"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialisation des paramètres de la couche linéaire.\n",
    "        # La matrice de poids (\"weights\") est initialisée de manière aléatoire en utilisant une distribution normale.\n",
    "        # Les poids ont des dimensions correspondant respectivement aux entrées (input_dim) et aux sorties (output_dim).\n",
    "        self.weights = np.random.randn(input_dim, output_dim)\n",
    "\n",
    "        # Le biais (\"bias\") est également initialisé de manière aléatoire avec une taille équivalente au nombre de sorties (output_dim).\n",
    "        # Chaque biais est associé à une sortie particulière.\n",
    "        self.bias = np.random.randn(output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # La méthode \"forward\" effectue une passe avant de la couche (c'est-à-dire un calcul simple des activations).\n",
    "        # Le produit matriciel entre les entrées (\"inputs\") et les poids (\"weights\") est calculé,\n",
    "        # suivi de l'ajout du biais (\"bias\") correspondant à chaque sortie.\n",
    "        # Renvoie les activations de sortie.\n",
    "        return np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "\n",
    "labels = [\"dog\", \"cat\", \"panda\"]\n",
    "np.random.seed(1)\n",
    "\n",
    "# Taille d'une image carrée (par exemple, 32x32 pixels pour une seule dimension, largeur ou hauteur).\n",
    "image_size = 32\n",
    "\n",
    "# Par défaut, les images sont en niveaux de gris avec un seul canal de couleur.\n",
    "color_chan = 1\n",
    "\n",
    "# Initialisation d'une instance de la classe LinearLayer.\n",
    "# Les dimensions d'entrée sont le produit de la taille de l'image et du nombre de canaux (taille totale de l'image aplatie en un vecteur unique).\n",
    "# Les dimensions de sortie correspondent au nombre de catégories/labels possibles (3 : \"dog\", \"cat\", \"panda\").\n",
    "# image_size * image_size * color_chan = 1024\n",
    "layer = LinearLayer(image_size * image_size * color_chan, len(labels))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfpnRfnQx5Ns",
    "outputId": "321c43cf-3256-418f-93de-df08d174eb52"
   },
   "source": [
    "print(layer.weights.shape)\n",
    "layer.weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YMXrTUVpBjo",
    "outputId": "e2ce6afe-e5c7-474d-db51-09f30928c548"
   },
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# load our example image, resize it, and then flatten it into our\n",
    "# \"feature vector\" representation\n",
    "orig = cv2.imread(\"./asset/old_dog.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(orig, cmap='gray')\n",
    "plt.show()\n",
    "x = cv2.resize(orig, (32, 32)).flatten()\n",
    "# 32 * 32 (1024,) features\n",
    "print(x.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hc1cdoa5pFyw"
   },
   "source": [
    "# Calcul des scores de sortie en effectuant une passe avant avec la méthode forward de la couche.\n",
    "# Cela inclut deux étapes principales :\n",
    "# 1. Produit matriciel entre les poids de la couche et les images (vecteurs d'entrée \"x\").\n",
    "# 2. Ajout des biais à chaque sortie pour introduire une translation et permettre un décalage dans les activations.\n",
    "scores = layer.forward(x)\n",
    "for (label, score) in zip(labels, scores):\n",
    "    print(\"[INFO] {}: {:.2f}\".format(label, score))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Descente de Gradient (Optimiseur) en 5 minutes\n",
    "![gradient](./asset/gradiant.png)\n",
    "\n",
    "### Explication simple :\n",
    "- À gauche, vous voyez Bob le robot.\n",
    "- La tâche de Bob est d'explorer notre **paysage de perte** pour descendre jusqu'au **bas du bassin**.\n",
    "- Cependant, Bob ne dispose que d'un seul capteur : sa **fonction de perte (L)**.\n",
    "\n",
    "Ce capteur lui permet de prendre ses paramètres, **W** et **b**, et de calculer une **fonction de perte (L)**.\n",
    "Mais Bob n'a absolument **aucune idée dans quelle direction** il doit bouger pour atteindre le fond du bassin.\n",
    "\n",
    "### **Quelle est la solution ?**\n",
    "**La réponse : appliquer la descente de gradient.**\n",
    "Pour cela, Bob doit simplement suivre la **pente** donnée par le **gradient** par rapport aux dimensions **W** et **b**.\n",
    "\n",
    "Mais qu'est-ce que le **gradient**, exactement ?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Le Gradient : Définition et Intuition\n",
    "\n",
    "### Définition mathématique :\n",
    "Le **gradient** est un **vecteur** qui indique la direction du plus grand taux de variation d'une fonction.\n",
    "- Si notre fonction est une **fonction de perte** ( L(W, b) ), le gradient pointe dans la direction où ( L ) augmente le plus rapidement.\n",
    "- Donc, pour minimiser ( L ), nous allons dans la direction opposée au gradient.\n",
    "\n",
    "En termes simples :\n",
    "Le gradient agit comme une **boussole** qui indique la direction pour réduire la perte.\n",
    "\n",
    "### Formule générale :\n",
    "Pour une fonction ( L(x_1, x_2, ..., x_n) ), le gradient est défini comme :\n",
    "\n",
    "$$\n",
    "\\nabla L = \\left[ \\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2}, \\dots, \\frac{\\partial L}{\\partial x_n} \\right]\n",
    "$$\n",
    "\n",
    "Où chaque composante correspond à la dérivée partielle par rapport à une dimension ( x_i ).\n",
    "Ce vecteur indique dans quelle direction modifier chaque paramètre pour réduire ( L ).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Intuition géométrique :\n",
    "1. Imaginez une colline ou une vallée dans un espace tridimensionnel.\n",
    "2. Le gradient vous donne la **pente** la plus raide de la colline.\n",
    "3. Descendre cette pente vous rapproche du point le plus bas (minimum).\n",
    "\n",
    "Dans un espace multidimensionnel, le concept est similaire, mais il devient difficile à visualiser. Cependant, le gradient joue toujours le rôle de \"pente généralisée\".\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Rôle clé en apprentissage automatique :\n",
    "- **But principal : Minimiser la perte.**\n",
    "- Le gradient indique comment ajuster les paramètres, comme les poids (**W**) et les biais (**b**), pour réduire les erreurs.\n",
    "- En appliquant l'algorithme de **descente de gradient**, on itère pas à pas jusqu'à trouver l'ensemble de paramètres qui minimise la fonction de perte ( L ).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Pseudo-code illustratif : Descente de Gradient\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    Wgradient = evaluate_gradient(loss, data, W)  # Calcul du gradient\n",
    "    W += -alpha * Wgradient                      # Mise à jour des paramètres avec le gradient\n",
    "```\n",
    "\n",
    "### Détails :\n",
    "- **evaluate_gradient** calcule le vecteur du gradient. Par exemple, si ( **W** ) a 10 dimensions, nous obtenons un vecteur avec 10 valeurs (1 pour chaque dimension).\n",
    "- La variable **Wgradient** contient les dérivées partielles pour chaque dimension.\n",
    "- Nous modifions chaque paramètre ( W_i ) en suivant la direction **opposée** au gradient (multiplié par le taux d'apprentissage alpha)\n",
    "\n",
    "$$\n",
    "\\alpha\\\n",
    "$$.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Ajustement du **taux d'apprentissage** alpha :\n",
    "- **alpha trop élevé :** Avance vite, mais risque de diverger ou de manquer le minimum global.\n",
    "- **alpha trop faible :** Peut converger lentement, très long à entraîner.\n",
    "- **Objectif :** Trouver une valeur équilibrée pour alpha.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Exemple : Descente de Gradient appliquée à une fonction d'activation Sigmoïde\n",
    "\n",
    "### Sigmoïde : Une fonction couramment utilisée\n",
    "La fonction sigmoïde est définie comme :\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Elle a une forme en \"S\" et est utilisée pour \"aplatir\" les valeurs d'entrée dans un intervalle entre 0 et 1. Voici une représentation graphique :\n",
    "\n",
    "![sigmoid](./asset/sigmoid_activation.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize la matrice de poids (weights) et le vecteur de biais (bias) avec des valeurs aléatoires\n",
    "        # Les poids sont initialisés avec des petites valeurs proches de zéro dans le but d'éviter des gradients explosifs\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01  # On réduit l'échelle des poids\n",
    "        self.bias = np.zeros(output_dim)  # Les biais sont initialisés à zéro\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Implémentation de la fonction d'activation sigmoid : f(x) = 1 / (1 + exp(-x))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        # Calcul de la dérivée de la fonction sigmoid\n",
    "        # La dérivée de sigmoid est : f'(x) = f(x) * (1 - f(x)), où f(x) est sigmoid\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # On calcule la sortie de la couche linéaire suivie de la fonction d'activation sigmoid\n",
    "        # 1. Produit matriciel entre les entrées et les poids (inputs * weights)\n",
    "        # 2. Ajout du biais\n",
    "        # 3. Application de l'activation sigmoid\n",
    "        return self.sigmoid(np.dot(inputs, self.weights) + self.bias)\n",
    "\n",
    "    def backward(self, batch_x, batch_y, preds, learning_rate):\n",
    "        # Rétropropagation pour ajuster les poids et les biais en fonction des erreurs calculées\n",
    "\n",
    "        # Étape 1 : Calcul de l'erreur entre les prédictions (preds) et les vraies valeurs (batch_y)\n",
    "        error = preds - batch_y\n",
    "\n",
    "        # Une métrique de performance - Somme des carrés des erreurs (Loss quadratique)\n",
    "        loss = np.sum(error ** 2)  # Ici, on retourne ultérieurement cette \"perte\" pour évaluer la performance\n",
    "\n",
    "        # Étape 2 : Calcul du gradient pour la mise à jour des paramètres\n",
    "        # On multiplie l'erreur par la dérivée de la sigmoid pour évaluer l'erreur propagée\n",
    "        d = self.sigmoid_derivative(preds) * error\n",
    "\n",
    "        # Calcul du gradient des poids : multiplication matricielle entre (batch_x.T) et d\n",
    "        gradient = batch_x.T.dot(d)\n",
    "\n",
    "        # Calcul du gradient pour le biais : somme de l'erreur propagée sur le batch\n",
    "        bias_gradiant = np.sum(d, axis=0)\n",
    "\n",
    "        # Étape 3 : Mise à jour des poids et des biais avec la descente de gradient\n",
    "        # Règle de l'ajustement: paramètres nouveaux = paramètres anciens - (learning_rate * gradient)\n",
    "        self.weights = self.weights - learning_rate * gradient\n",
    "        self.bias = self.bias - learning_rate * bias_gradiant\n",
    "\n",
    "        # Retourner la perte (loss) pour le suivi de la convergence\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlFwhyVVzQS9"
   },
   "source": [
    "### Build small dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YmJBqyVozRud"
   },
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Génère un problème de classification binaire avec 1 000 points de données,\n",
    "# où chaque point est représenté par un vecteur de 2 caractéristiques (dimensions).\n",
    "# make_blobs permet de créer artificiellement des données regroupées en \"clusters\"\n",
    "# et bien adaptées aux algorithmes de classification supervisée.\n",
    "X, y = make_blobs(n_samples=1000,  # Nombre total de points de données\n",
    "                  centers=2,  # Nombre de classes ou de clusters (ici, 2 classes)\n",
    "                  n_features=2,  # Nombre de caractéristiques par point de données (2D)\n",
    "                  random_state=22)\n",
    "\n",
    "# Comme notre algorithme (implémentation actuelle) attend que y soit en 2 dimensions,\n",
    "# on ajoute une dimension supplémentaire à y en utilisant np.newaxis.\n",
    "# Cela transforme y d'un vecteur 1D (ex. [0, 1, 0, ...]) en un tableau 2D (ex. [[0], [1], [0], ...]).\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "# Affiche la forme de X : X est une matrice contenant 1 000 points de données (samples),\n",
    "# chaque point ayant deux dimensions ou caractéristiques.\n",
    "print(X.shape)  # (1000, 2)\n",
    "\n",
    "# Affiche la forme de y : y est un vecteur 2D contenant 1 000 labels (1 par point),\n",
    "# où chaque label appartient à une des deux classes, représentées par les valeurs 0 ou 1.\n",
    "print(y.shape)  # (1000, 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQJ3J2unzcO3",
    "outputId": "0e010144-5204-424c-fe08-2a6e42360cf7"
   },
   "source": [
    "# Scatter plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y[:, 0])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_MHSC9Dzvts"
   },
   "source": [
    "# Split du jeu de donnée en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDfwatgZz9LN"
   },
   "source": [
    "### Creat train loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Création d'une couche linéaire prenant comme entrée des vecteurs avec 2 caractéristiques\n",
    "# (X.shape[1] correspond au nombre de colonnes/métriques de la matrice d'entrée X),\n",
    "# et produisant une seule sortie (1 label, par ex., pour une tâche de classification binaire).\n",
    "layer = LinearLayer(X.shape[1], 1)\n",
    "\n",
    "# Définition du taux d'apprentissage (learning rate)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Nombre d'itérations (epochs) que l'algorithme parcourra sur les données pour entraîner le modèle.\n",
    "num_epochs = 100\n",
    "\n",
    "# Boucle principale d'entraînement : passe à travers le dataset `num_epochs` fois.\n",
    "for epoch in range(num_epochs):\n",
    "    # Étape 1 : Propagation avant (Forward pass)\n",
    "    # Calcul des prédictions en passant les données d'entraînement (X_train)\n",
    "    # à travers la couche linéaire et la fonction d'activation (sigmoid).\n",
    "    preds = layer.forward(X_train)\n",
    "\n",
    "    # Étape 2 : Rétropropagation (Backward pass)\n",
    "    # Calcul des gradients des poids et des biais, mise à jour des paramètres\n",
    "    # avec la descente de gradient, et calcul de la perte (loss).\n",
    "    # Le `loss` sert à quantifier l'écart entre les prédictions et les vraies valeurs (y_train).\n",
    "    loss = layer.backward(X_train, y_train, preds, learning_rate)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "        print(f\"\"\"Y train\n",
    "{y_train[:4]}  # Affichage des 4 premières vraies valeurs (y_train)\n",
    "Y preds\n",
    "{np.round(preds[:4], 4)}\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSFZDysw10dC"
   },
   "source": "### Evaluation du model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfHUQACn1-46",
    "outputId": "d0245d3e-9ea2-4b44-993a-5a8bf7efbc2d"
   },
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# evaluate our model\n",
    "preds = layer.forward(X_test)\n",
    "print(classification_report(y_test, np.around(preds)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEQ6JoSh3dop"
   },
   "source": "[un très bon article concernent la décente de gradiant](https://www.charlesbordet.com/fr/gradient-descent/#comment-ca-marche-)\n"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#  Transformers\n",
    "\n",
    "Scaled Dot-Product Attention Math:\n",
    "\n",
    "![Scaled Dot-Product Attention](../asset/scale_dot_product.png)\n",
    "\n",
    "Paper view:\n",
    "\n",
    "![paper scaled](../asset/scaled_dot.png)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3513f78befff843"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "    query, key, value must have matching leading dimensions.\n",
    "    key, value must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    query: A request to the model in the form of a vector, shape == (..., seq_len_q, depth)\n",
    "    key: The entire dataset the model is trained on, shape == (..., seq_len_k, depth)\n",
    "    value: A vector containing information about the relevance of each key, shape == (..., seq_len_v, depth)\n",
    "    mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    scaled_attention: the multiplied output as a result of the query and relevant keys, shape == (..., seq_len_q, depth_v)\n",
    "    attention_weights: the amount of attention given to each key, shape == (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the dot product\n",
    "    matmul_qk = np.dot(query, key.T)\n",
    "\n",
    "    # Scale the dot product\n",
    "    depth = query.shape[-1]\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "\n",
    "    # Add the mask, if available\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # Softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "\n",
    "    scaled_attention = np.dot(attention_weights, value)\n",
    "\n",
    "    return scaled_attention, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T20:37:10.921101700Z",
     "start_time": "2024-02-14T20:37:10.891441700Z"
    }
   },
   "id": "12e90c4cb8fb9596",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "We creat a sequence of 3 words with 2 dimension embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe48c74d33d61c56"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.08412591]\n",
      " [0.98668512 1.01394796]\n",
      " [0.98931693 1.01391173]\n",
      " [1.         1.08412591]]\n",
      "[[1.14579473e-02 9.55838542e-02 7.97374344e-01 9.55838542e-02]\n",
      " [2.03348558e-04 1.41513064e-02 9.84808921e-01 8.36423531e-04]\n",
      " [2.02820414e-04 1.41145522e-02 9.82251144e-01 3.43148384e-03]\n",
      " [1.14579473e-02 9.55838542e-02 7.97374344e-01 9.55838542e-02]]\n"
     ]
    }
   ],
   "source": [
    "q = np.array([[1, 0], [0, 2], [1, 1], [1, 0]])\n",
    "k = np.array([[1, 2], [4, 5], [7, 8], [4, 3]])\n",
    "v = np.array([[1, 0], [0, 2], [1, 1], [2, 1]])\n",
    "\n",
    "output, weights = scaled_dot_product_attention(q, k, v, None)\n",
    "\n",
    "print(output)\n",
    "print(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T20:37:15.690779300Z",
     "start_time": "2024-02-14T20:37:15.674776200Z"
    }
   },
   "id": "4fed321496727702",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-head Attention\n",
    "\n",
    "In the Multi-Head Attention mechanism, the traditional scaled dot product attention operation is applied multiple times in parallel. The output values are then concatenated and linearly transformed to result in the final value.\n",
    "\n",
    "![multihead](../asset/multihead.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3290a551b77bcca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            n_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim    #512 dim\n",
    "        self.n_heads = n_heads   #8\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d\n",
    "       \n",
    "        #key,query and value matrixes    #64 x 64   \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "\n",
    "    def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key : key vector\n",
    "           query : query vector\n",
    "           value : value vector\n",
    "           mask: mask for decoder\n",
    "        \n",
    "        Returns:\n",
    "           output vector from multihead attention\n",
    "        \"\"\"\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        \n",
    "        # query dimension can change in decoder during inference. \n",
    "        # so we cant take general seq_length\n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        # 32x10x512\n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "       \n",
    "        k = self.key_matrix(key)       # (32x10x8x64)\n",
    "        q = self.query_matrix(query)   \n",
    "        v = self.value_matrix(value)\n",
    "\n",
    "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n",
    "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "       \n",
    "        # computes attention\n",
    "        # adjust key for matrix multiplication\n",
    "        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)\n",
    "        product = torch.matmul(q, k_adjusted)  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)\n",
    "      \n",
    "        \n",
    "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
    "        if mask is not None:\n",
    "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        #divising by square root of key dimension\n",
    "        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)\n",
    "\n",
    "        #applying softmax\n",
    "        scores = F.softmax(product, dim=-1)\n",
    " \n",
    "        #mutiply with value matrix\n",
    "        scores = torch.matmul(scores, v)  ##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) \n",
    "        \n",
    "        #concatenated output\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n",
    "        \n",
    "        output = self.out(concat) #(32,10,512) -> (32,10,512)\n",
    "       \n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:27:43.695503100Z",
     "start_time": "2024-02-14T21:27:43.684363600Z"
    }
   },
   "id": "afaeebf34c06177d",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 10, 512])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 512\n",
    "sequence_length = 10\n",
    "x = torch.randn(1, sequence_length, embed_dim)\n",
    "\n",
    "heads = MultiHeadAttention(embed_dim, 8)\n",
    "heads(x, x, x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:27:45.156749400Z",
     "start_time": "2024-02-14T21:27:45.104113Z"
    }
   },
   "id": "6e1476aaeeee4366",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder\n",
    "\n",
    "![trasnformers](../asset/trasnformers.png)\n",
    "\n",
    "\n",
    "Let's see the implementation of BERT [model](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert/modeling_bert.py#L556)\n",
    "\n",
    "\n",
    "# RNN vs transformers\n",
    "\n",
    "## RNN summary\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Sequential Processing:** RNNs are inherently designed for sequential data processing, making them perfect for time series prediction, natural language processing, and speech recognition.\n",
    "2. **low cost inference:** RNNs tend to require fewer computational resources than Transformer models as they process input sequences step by step rather than in parallel.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Vanishing and Exploding Gradient Problem:** During back-propagation in deep RNNs, gradients are multiplied by the weight matrix at every timestep. This can result in gradients that either explode or vanish, making it challenging to train deep RNNs.\n",
    "2. **Long-term Dependencies:** RNNs struggle to learn long-term dependencies due to the vanishing gradient problem.\n",
    "3. **Cannot Process in Parallel:** The sequential nature of RNNs means they cannot take advantage of modern GPUs which excel in performing parallel operations.\n",
    "\n",
    "![triangle](../asset/rnn-vs-transformer.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d804d1f6d14fe020"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

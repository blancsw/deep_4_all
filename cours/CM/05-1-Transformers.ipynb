{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RNN vs transformers\n",
    "\n",
    "## RNN summary\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Sequential Processing:** RNNs are inherently designed for sequential data processing, making them perfect for time series prediction, natural language processing, and speech recognition.\n",
    "2. **low cost inference:** RNNs tend to require fewer computational resources than Transformer models as they process input sequences step by step rather than in parallel.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Vanishing and Exploding Gradient Problem:** During back-propagation in deep RNNs, gradients are multiplied by the weight matrix at every timestep. This can result in gradients that either explode or vanish, making it challenging to train deep RNNs.\n",
    "2. **Long-term Dependencies:** RNNs struggle to learn long-term dependencies due to the vanishing gradient problem.\n",
    "3. **Cannot Process in Parallel:** The sequential nature of RNNs means they cannot take advantage of modern GPUs which excel in performing parallel operations.\n",
    "\n",
    "![triangle](./asset/rnn-vs-transformer.png)\n"
   ],
   "id": "d804d1f6d14fe020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ab1fa7a8b56f2cc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

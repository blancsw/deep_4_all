{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embedding\n",
    "\n",
    "Word Embedding is a type of word representation that allows words with similar meaning to have a similar representation. It is a learned representation for text where words that have the same meaning have a similar representation. The process of creating word embeddings is to train a model on a large body of text using unsupervised learning so that it learns to predict a word given its context, or vice versa. This results in embeddings where words that are close together in the high-dimensional embedding space are expected to be semantically similar.\n",
    "\n",
    "Notably, word embeddings do not just assign each word to a unique vector, but they actually represent words in a way that captures semantic or syntactic similarity, based on the text corpus the model is trained on. Embedding vectors are often hundreds of dimensions and are good at capturing nuanced relationships between words.\n",
    "\n",
    "![word_embed](../asset/word_embed.png)\n",
    "\n",
    "## Embedding layer\n",
    "\n",
    "An Embedding Layer in machine learning is a layer that creates word embeddings from input sequences by mapping words or integers to dense vectors of real numbers. It's often used in natural language processing tasks where the layer takes in a 2D input of integer sequences (word indices) and on output provides 3D floating point tensor sequences. Each individual sequence in the 3D output tensor will have the same length as the original input sequence, but each word or integer in the sequence is now represented by a dense vector. These vectors capture semantic relationships between words, and their dimensionality is a hyperparameter that can be tuned for specific tasks.\n",
    "\n",
    "### Very basic usage\n",
    "\n",
    "Embedding is just a simple lookup table. Link a index to a trainable vector."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acd382d3b251606c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "\n",
    "# 2 words in vocab \"hello\" and \"world\"\n",
    "# 5 dimensional embeddings\n",
    "embeds = nn.Embedding(num_embeddings=2, embedding_dim=5)\n",
    "\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "\n",
    "print(hello_embed)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eb248361b1f0817",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get train parameters\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3729db8c57169d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for param in embeds.parameters():\n",
    "    print(param)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a147c1b00b18fce0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just for fun set weight to a fix value\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445679a4b1b15fbe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embedding_lookup = torch.tensor([\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [0, 1, 1, 1, 0],\n",
    "], dtype=torch.float32)\n",
    "embeds.weight = nn.Parameter(embedding_lookup)\n",
    "embeds.weight"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "132f51809cc92340",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, if I select index 0 or 1, I get my embedding_lookup line.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603ebf902b1c23de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(embeds(torch.tensor([0])))\n",
    "print(embeds(torch.tensor([1])))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e55098ecab0d9fba",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPT embedding\n",
    "\n",
    "Let's get the very first GPT model and see how the embedding layer size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1fb3b0d9955d9d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "print(\"vocab size\", tokenizer.vocab_size)\n",
    "\n",
    "# expected Embedding(50257, 768)\n",
    "# 50257 = vocabulary size\n",
    "# 768 = number of features\n",
    "print(\"Embedding size\", model.wte)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff43cf92ccb96fd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "[Let's look at the code for this model](https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/models/gpt2/modeling_gpt2.py#L667)\n",
    "\n",
    "## Train first embedding layer\n",
    "\n",
    "For this part we will train our first embedding layer on disney park reviews !\n",
    "\n",
    "To start we will ony train this layer on letter instance of words\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea5beba390d7e9da"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and take subset of the dataset\n",
    "df = pd.read_csv(\"data/disney_review/train.csv\")[:5000]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1e9ce6061cdd8c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Count each deffrent values of rating\n",
    "df['Rating'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7e2a7aaded336c1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get reviews\n",
    "reviews = df[\"Review_Text\"].values.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e239d6e2169b3309",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will creat an sequence of letter based on sentences\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "aba decides a\n",
    "```\n",
    "\n",
    "will produce\n",
    "\n",
    "```\n",
    "[\n",
    "  ('a', 'b'),\n",
    "  ('b', 'a'),\n",
    "  ('a', ' '),\n",
    "  (' ', 'd'),\n",
    "  ('d', 'e'),\n",
    "  ('e', 'c'),\n",
    "  ('c', 'i'),\n",
    "  ('i', 'd'),\n",
    "  ('d', 'e'),\n",
    "  ('e', 's'),\n",
    "  ('s', ' '),\n",
    "  (' ', 'a'),\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f634a4191a83c5be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import re\n",
    "\n",
    "\n",
    "def sliding_window(txt):\n",
    "    for i in range(len(txt) - 1):\n",
    "        yield txt[i], txt[i + 1]\n",
    "\n",
    "\n",
    "window = []\n",
    "for title in reviews:\n",
    "    # Get only a to z and 0 to 9 letters and numerb\n",
    "    title = re.sub('[^a-zA-Z0-9]+', '', title.lower())\n",
    "    window.append(sliding_window(title))\n",
    "window = list(it.chain(*window))\n",
    "\n",
    "# Number of window\n",
    "print(len(window))\n",
    "# Get first 5 example\n",
    "window[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c2804b661785f58",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets do a one hot encoding so a letter = an id (like sql table id)\n",
    "\n",
    "```\n",
    "{' ': 2,\n",
    " 'a': 0,\n",
    " 'b': 1,\n",
    " 'c': 5,\n",
    " 'd': 3,\n",
    " 'e': 4,\n",
    " 'g': 8,\n",
    " 'i': 6,\n",
    " 'l': 16,\n",
    " 'm': 12,\n",
    " 'n': 9,\n",
    " 'o': 11,\n",
    " 'r': 15,\n",
    " 's': 7,\n",
    " 't': 10,\n",
    " 'u': 13,\n",
    " 'y': 14}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe8ce2e3ac7aa2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mapping = {c: i for i, c in enumerate(pd.DataFrame(window)[0].unique())}\n",
    "integers_in = np.array([mapping[w[0]] for w in window])\n",
    "integers_out = np.array([mapping[w[1]] for w in window])\n",
    "\n",
    "print(\"Shape of input\", integers_in.shape)\n",
    "print(\"Input example\", integers_in[0], integers_out[0])\n",
    "print(\"Show generate mapping\\n\", mapping)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bddf0e797566eeef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The dataset class\n",
    "The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: __getitem__, and __len__. The get-item function has to return the i-th data point in the dataset, while the len function returns the size of the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f779327cd0196cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class NextLetterDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, integers_in, integers_out):\n",
    "        self.integers_in = integers_in\n",
    "        self.integers_out = integers_out\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data pin dataset\n",
    "        return len(self.integers_in)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.integers_in[idx]\n",
    "        data_label = self.integers_out[idx]\n",
    "        return torch.tensor(data_point), torch.tensor(data_label, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f926f9dabe6b07fe",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build first embedding model\n",
    "\n",
    "We will build a simple network to do next letter prediction\n",
    "\n",
    "![next letter](../asset/next_letter_prediction.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a57ac7803b21667"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NextLetterPrediction(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(NextLetterPrediction, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.embedding(x))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8115c1c470321512",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot letter before the train\n",
    "\n",
    "Let's plot letter embedding before to train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ea191b7e45c59e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = NextLetterPrediction(vocab_size=len(mapping),\n",
    "                             # For X and Y plot\n",
    "                             embedding_size=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff312a08c307874",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "idx_to_calc = list(mapping.values())\n",
    "idx_to_calc = np.array([idx_to_calc]).T\n",
    "\n",
    "translator = {v: k for k, v in mapping.items()}\n",
    "preds = model.embedding(torch.tensor(idx_to_calc)).detach().numpy()\n",
    "plt.scatter(preds[:, 0, 0], preds[:, 0, 1], alpha=0)\n",
    "for i, idx in enumerate(idx_to_calc):\n",
    "    plt.text(preds[i, 0, 0], preds[i, 0, 1], translator[idx[0]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35d26f4694ee313",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train loop\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8e9c4b621c62168"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Init the dataset into the DataLoader\n",
    "dataset = NextLetterDataset(integers_in, integers_out)\n",
    "trainloader = data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63fdfddb974e19fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "epoches = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Load the model to cuda device in train mode\n",
    "model.to(device)\n",
    "model.train()\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(epoches):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53ee061e0c27b2e1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idx_to_calc = list(mapping.values())\n",
    "idx_to_calc = np.array([idx_to_calc]).T\n",
    "\n",
    "translator = {v: k for k, v in mapping.items()}\n",
    "preds = model.embedding(torch.tensor(idx_to_calc).to(device)).cpu().detach().numpy()\n",
    "plt.scatter(preds[:, 0, 0], preds[:, 0, 1], alpha=0)\n",
    "for i, idx in enumerate(idx_to_calc):\n",
    "    plt.text(preds[i, 0, 0], preds[i, 0, 1], translator[idx[0]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1092ac5af7839cb3",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

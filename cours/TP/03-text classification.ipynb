{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP : Classification de critiques de films avec AI\n",
    "\n",
    "Dans ce travail pratique, nous allons construire un modèle de machine learning pour la classification de critiques de films. Notre modèle sera entrainé à prédire si une critique de film est positive ou négative en utilisant uniquement des couches Embedding et Linear sur le jeu de données allocine.\n",
    "\n",
    "## Objectifs du TP\n",
    "\n",
    "1. Comprendre les concepts de base de l'apprentissage automatique tels que l'entraînement, la validation et le test de modèles.\n",
    "1. Apprendre à préparer les données pour le traitement par des algorithmes d'apprentissage automatique.\n",
    "1. Comprendre et implémenter un modèle de classification de texte simple à l'aide des couches Embedding et Linear.\n",
    "1. Évaluer les performances d'un modèle de machine learning.\n",
    "\n",
    "## Guide\n",
    "\n",
    "1. Préparation des données: Importez le jeu de données allocine et préparez-le pour le traitement. Le jeu de données devrait être divisé en ensemble d'entraînement, de validation et de test.\n",
    "1. Implémentation du modèle: Implémentez votre propre version du modèle de classification de texte. Assurez-vous de n'utiliser que les couches Embedding et Linear.\n",
    "1. Entrainement du modèle: Entrainez le modèle en utilisant l'ensemble d'entraînement et vérifiez sa performance à l'aide de l'ensemble de validation.\n",
    "1. Evaluation du modèle: Évaluez le modèle final à l'aide de l'ensemble de test.\n",
    "1. Interprétation des résultats: Analysez les résultats obtenus. Comment le modèle performe-t-il? Quels sont les aspects du modèle qui pourraient être améliorés?\n",
    "\n",
    "# Code de base"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9262641b1de01de1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allocine\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d101b54da33050e0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset['review'][0], train_dataset['label'][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb7c138f622431c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the spacy model for the tokenizer\n",
    "!python -m spacy download fr_core_news_sm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db969ddb57202998",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how to creat a simple tokenizer with vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3dc50528faf33c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language=\"fr\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text.lower())\n",
    "\n",
    "texts = [\"Bonjour je vais très bien\", \"Le lisp et trop cool !\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(texts), specials=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(f\"Vocabulary: {vocab.get_itos()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efa86d5c121e6ffb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To test hour vocabulary\n",
    "\n",
    "sentence = \"TOTO je vais très bien le lisp TOTO\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer(sentence)\n",
    "\n",
    "# Convert tokens to indices via the vocab\n",
    "encoded_sentence = [vocab[token] for token in tokens]\n",
    "\n",
    "print(f\"Encoded sentence: {encoded_sentence}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3734fedcb4c038",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

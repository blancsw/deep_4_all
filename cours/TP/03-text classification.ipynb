{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP : Classification de critiques de films avec AI\n",
    "\n",
    "Dans ce travail pratique, nous allons construire un modèle de machine learning pour la classification de critiques de films. Notre modèle sera entrainé à prédire si une critique de film est positive ou négative en utilisant uniquement des couches Embedding et Linear sur le jeu de données allocine.\n",
    "\n",
    "## Objectifs du TP\n",
    "\n",
    "1. Comprendre les concepts de base de l'apprentissage automatique tels que l'entraînement, la validation et le test de modèles.\n",
    "1. Apprendre à préparer les données pour le traitement par des algorithmes d'apprentissage automatique.\n",
    "1. Comprendre et implémenter un modèle de classification de texte simple à l'aide des couches Embedding et Linear.\n",
    "1. Évaluer les performances d'un modèle de machine learning.\n",
    "\n",
    "## Guide\n",
    "\n",
    "1. Préparation des données: Importez le jeu de données allocine et préparez-le pour le traitement. Le jeu de données devrait être divisé en ensemble d'entraînement, de validation et de test.\n",
    "1. Implémentation du modèle: Implémentez votre propre version du modèle de classification de texte. Assurez-vous de n'utiliser que les couches Embedding et Linear.\n",
    "1. Entrainement du modèle: Entrainez le modèle en utilisant l'ensemble d'entraînement et vérifiez sa performance à l'aide de l'ensemble de validation.\n",
    "1. Evaluation du modèle: Évaluez le modèle final à l'aide de l'ensemble de test.\n",
    "1. Interprétation des résultats: Analysez les résultats obtenus. Comment le modèle performe-t-il? Quels sont les aspects du modèle qui pourraient être améliorés?\n",
    "\n",
    "# Code de base"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9262641b1de01de1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-26T09:10:29.679389100Z",
     "start_time": "2024-01-26T09:10:24.104404100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['review', 'label'],\n        num_rows: 160000\n    })\n    validation: Dataset({\n        features: ['review', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['review', 'label'],\n        num_rows: 20000\n    })\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allocine\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T09:10:35.398610900Z",
     "start_time": "2024-01-26T09:10:35.391075500Z"
    }
   },
   "id": "d101b54da33050e0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "('Si vous cherchez du cinéma abrutissant à tous les étages,n\\'ayant aucune peur du cliché en castagnettes et moralement douteux,\"From Paris with love\" est fait pour vous.Toutes les productions Besson,via sa filière EuropaCorp ont de quoi faire naître la moquerie.Paris y est encore une fois montrée comme une capitale exotique,mais attention si l\\'on se dirige vers la banlieue,on y trouve tout plein d\\'intégristes musulmans prêts à faire sauter le caisson d\\'une ambassadrice américaine.Nauséeux.Alors on se dit qu\\'on va au moins pouvoir apprécier la déconnade d\\'un classique buddy-movie avec le jeune agent aux dents longues obligé de faire équipe avec un vieux lou complètement timbré.Mais d\\'un côté,on a un Jonathan Rhys-meyers fayot au possible,et de l\\'autre un John Travolta en total délire narcissico-badass,crâne rasé et bouc proéminent à l\\'appui.Sinon,il n\\'y a aucun scénario.Seulement,des poursuites débiles sur l\\'autoroute,Travolta qui étale 10 mecs à l\\'arme blanche en 8 mouvements(!!)ou laisse son associé se faire démolir la tronche pendant qu\\'il scrute à la jumelle.Ca pourrait être un plaisir coupable,tellement c\\'est \"hénaurme\",c\\'est juste de la daube dans la droite lignée d\\'un \"Transporteur\",\"Taken\"ou \"Banlieue 13\".',\n 0)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['review'][0], train_dataset['label'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T09:10:53.345203500Z",
     "start_time": "2024-01-26T09:10:53.083711100Z"
    }
   },
   "id": "bcb7c138f622431c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/bin/bash: line 1: python: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Get the spacy model for the tokenizer\n",
    "!python -m spacy download fr_core_news_sm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T09:26:44.865966700Z",
     "start_time": "2024-01-26T09:26:44.604791400Z"
    }
   },
   "id": "db969ddb57202998",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how to creat a simple tokenizer with vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3dc50528faf33c2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['<unk>', '<pad>', '<s>', '</s>', '!', 'bien', 'bonjour', 'cool', 'et', 'je', 'le', 'lisp', 'trop', 'très', 'vais']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language=\"fr\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text.lower())\n",
    "\n",
    "texts = [\"Bonjour je vais très bien\", \"Le lisp et trop cool !\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(texts), specials=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(f\"Vocabulary: {vocab.get_itos()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T09:32:01.756854500Z",
     "start_time": "2024-01-26T09:32:00.900196900Z"
    }
   },
   "id": "efa86d5c121e6ffb",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sentence: [0, 9, 14, 13, 5, 10, 11, 0]\n"
     ]
    }
   ],
   "source": [
    "# To test hour vocabulary\n",
    "\n",
    "sentence = \"TOTO je vais très bien le lisp TOTO\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer(sentence)\n",
    "\n",
    "# Convert tokens to indices via the vocab\n",
    "encoded_sentence = [vocab[token] for token in tokens]\n",
    "\n",
    "print(f\"Encoded sentence: {encoded_sentence}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T09:32:11.725766400Z",
     "start_time": "2024-01-26T09:32:11.655239500Z"
    }
   },
   "id": "eb3734fedcb4c038",
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

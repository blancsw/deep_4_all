{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sujet de TP : Fine-tuning, Déploiement et Interface d’un Modèle Multimodal\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Vous allez travailler sur le fine-tuning d’un modèle multimodal de génération conditionnelle, en utilisant les ressources gratuites de Google Colab ou Kaggle (avec deux GPU T4). L’objectif est de créer une application capable de traiter des entrées multimodales (texte et image) ou un dataset uni-modal (text uniquement), de publier le model sur Hugging Face Hub, puis de déployer le modèle via l’outil [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/en/index). Enfin, vous développerez une interface (web et ou API REST) pour exploiter votre application.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "Model de a utilisez:\n",
    "\n",
    "- multimodal text & image: https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct\n",
    "- text only\n",
    "    - petit: https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct\n",
    "    - moyen: https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct\n",
    "\n",
    "1. **Fine-tuning sur 2 GPU T4**\n",
    "   - Configurer l’environnement dans Google Colab ou Kaggle afin d’utiliser deux GPU T4.\n",
    "   - Modifier le notebook de fine-tuning pour exploiter la parallélisation multi-GPU (en utilisant par exemple `accelerate` avec `notebook_launcher`).\n",
    "\n",
    "2. **Création ou choix d’un dataset**\n",
    "   - Vous choisirez ou créerez un dataset (multimodal ou non) adapté à la tâche de génération conditionnelle (exemple: questions-réponses avec images ou uniquement texte).\n",
    "   - Préparez et documentez la procédure de préparation et de prétraitement des données.\n",
    "\n",
    "3. **Push sur Hugging Face Hub**\n",
    "   - Une fois le fine-tuning terminé, poussez votre modèle finement ajusté sur le [Hugging Face Hub](https://huggingface.co/). Aide: https://huggingface.co/docs/transformers/model_sharing\n",
    "\n",
    "4. **Réalisation de l’inférence**\n",
    "   - Configurez et testez l’inférence de votre modèle en utilisant [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/en/index).\n",
    "\n",
    "5. **Création d’une Interface ou API REST**\n",
    "   - Développez une interface web simple (par exemple avec Streamlit, Gradio ou Flask) ou une API REST (avec FastAPI ou Flask) qui permet d’interagir avec votre modèle (saisie d’une question, upload d’une image le cas échéant, affichage de la réponse générée).\n",
    "\n",
    "6. **Rédaction d’un Article**\n",
    "   - Rédigez un article détaillé présentant votre travail:\n",
    "     - Présentez l’application développée, le choix du dataset (créé ou choisi) et les raisons de ce choix.\n",
    "     - Expliquez la démarche de fine-tuning (choix des hyperparamètres, utilisation de techniques comme QLoRA, LORA, gradient checkpointing, etc.).\n",
    "     - Décrivez le processus de déploiement sur le Hub et l’inférence via Text Generation Inference.\n",
    "     - Discutez des challenges rencontrés et des pistes d’amélioration.\n",
    "     - Laissez parler votre créativité !\n",
    "\n",
    "## Livrables attendus\n",
    "\n",
    "1. **Un repository GitHub** :\n",
    "   - Code de l’Interface / API REST et notebook\n",
    "   - Un notebook ou script python (ou plusieurs fichiers) contenant le code du fine-tuning, la configuration multi-GPU et le push vers le Hugging Face Hub.\n",
    "\n",
    "1. **Article Technique** :\n",
    "   - Un document (PDF ou Markdown) détaillant l’approche, les choix techniques et vos observations.\n",
    "\n",
    "1. **Lien vers le Modèle sur le Hub** :\n",
    "   - Le lien vers le modèle finement ajusté et poussé sur le Hugging Face Hub.\n",
    "\n",
    "## Conseils et Astuces\n",
    "\n",
    "- **Documentation** :\n",
    "    - Consultez la documentation de [Accelerate](https://huggingface.co/docs/accelerate/index) pour la gestion multi-GPU\n",
    "    - [Hugging Face Hub](https://huggingface.co/docs/transformers/model_sharing) pour le push du modèle.\n",
    "    - [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/en/index)\n",
    "- **Gestion des ressources** : Les GPU T4 ont des capacités limitées en VRAM, veillez à optimiser le batch size, la gradient accumulation et éventuellement utiliser des techniques comme la quantification ou le gradient checkpointing.\n",
    "- **Créativité** : N’hésitez pas à personnaliser le projet en ajoutant des fonctionnalités (exemple : support d’un autre type de média, interface plus interactive, etc.) et en expliquant vos choix.\n",
    "\n",
    "---\n",
    "\n",
    "Laissez libre cours à votre imagination et bonne réalisation de ce TP XD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9262641b1de01de1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de484d2367d5ea29"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Not working\n",
    "Some bugs with PEFT LORA model"
   ],
   "id": "f383a94b0974eb57"
  },
  {
   "id": "b4477a0d55d8390c",
   "cell_type": "code",
   "source": "!pip install -q accelerate datasets peft bitsandbytes tensorboard trl wandb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "20fa1068-dd7c-4fe2-99f6-cf6c386bb84a",
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()  # Write a config file\n",
    "os._exit(00)  # Restart the notebook"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "adc33658-b177-49e0-a98d-77ab0485f1da",
   "cell_type": "code",
   "source": "# accelerate default config path\n!cat ~/.cache/huggingface/accelerate/default_config.yaml",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:28:17.705907Z",
     "iopub.execute_input": "2025-02-08T15:28:17.706223Z",
     "iopub.status.idle": "2025-02-08T15:28:17.825047Z",
     "shell.execute_reply.started": "2025-02-08T15:28:17.706192Z",
     "shell.execute_reply": "2025-02-08T15:28:17.823969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "{\n  \"compute_environment\": \"LOCAL_MACHINE\",\n  \"debug\": false,\n  \"distributed_type\": \"MULTI_GPU\",\n  \"downcast_bf16\": false,\n  \"enable_cpu_affinity\": false,\n  \"machine_rank\": 0,\n  \"main_training_function\": \"main\",\n  \"mixed_precision\": \"no\",\n  \"num_machines\": 1,\n  \"num_processes\": 2,\n  \"rdzv_backend\": \"static\",\n  \"same_network\": false,\n  \"tpu_use_cluster\": false,\n  \"tpu_use_sudo\": false,\n  \"use_cpu\": false\n}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "id": "98325654-acb0-4805-b35d-ffb7a1ffebc0",
   "cell_type": "code",
   "source": "!nvidia-smi",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:28:19.303638Z",
     "iopub.execute_input": "2025-02-08T15:28:19.304042Z",
     "iopub.status.idle": "2025-02-08T15:28:19.524488Z",
     "shell.execute_reply.started": "2025-02-08T15:28:19.303997Z",
     "shell.execute_reply": "2025-02-08T15:28:19.523369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Sat Feb  8 15:28:19 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   72C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "7c2d2ea451fe4ba0",
   "cell_type": "markdown",
   "source": "# Wandb\n\nCreat token and account: https://wandb.ai/home",
   "metadata": {}
  },
  {
   "id": "f7ca3b46-8432-4648-bcde-a9167e7e65fb",
   "cell_type": "code",
   "source": "import wandb\nimport getpass\n\ntoken = getpass.getpass()\nwandb.login(key=token)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "6ec4266ecdfecbc0",
   "cell_type": "code",
   "source": [
    "from accelerate import PartialState\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Specify the checkpoint for SmolLM2 and set the device.\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "# Load the tokenizer and model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint,\n",
    "                                          add_eos_token=True,\n",
    "                                          add_bos_token=True)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # For multi-GPU setups, consider using device_map=\"auto\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint,\n",
    "            device_map={\"\": PartialState().process_index},\n",
    "            attention_dropout=0.1,\n",
    "            )\n",
    "    return model"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:28:26.168067Z",
     "iopub.execute_input": "2025-02-08T15:28:26.168488Z",
     "iopub.status.idle": "2025-02-08T15:28:34.094278Z",
     "shell.execute_reply.started": "2025-02-08T15:28:26.168447Z",
     "shell.execute_reply": "2025-02-08T15:28:34.093610Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "id": "37633c01c195e6c9",
   "cell_type": "markdown",
   "source": "# Dataset\n\nJson structure output: https://huggingface.co/datasets/ChristianAzinn/json-training",
   "metadata": {}
  },
  {
   "id": "9dd463d66d84fae8",
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    ds = load_dataset(\"ChristianAzinn/json-training\")\n",
    "    # Perform Train-Test Split\n",
    "    split_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    # Access train and test splits\n",
    "    train_dataset = split_ds[\"train\"]\n",
    "    test_dataset = split_ds[\"test\"]\n",
    "    return train_dataset, test_dataset"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:28:34.095419Z",
     "iopub.execute_input": "2025-02-08T15:28:34.095766Z",
     "iopub.status.idle": "2025-02-08T15:28:34.099948Z",
     "shell.execute_reply.started": "2025-02-08T15:28:34.095733Z",
     "shell.execute_reply": "2025-02-08T15:28:34.099174Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "id": "ddf37774132f48c2",
   "cell_type": "code",
   "source": [
    "# Set the response template to match the chat format.\n",
    "# (Ensure this string exactly matches the beginning of the assistant's response as output by apply_chat_template.)\n",
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "instruction_template = \"<|im_start|>user\\n\"\n",
    "PROMPT_TEMPLATE = \"\"\"Query: {query}\n",
    "\n",
    "schema:\n",
    "{schema}\"\"\"\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"\n",
    "    Converts each example into a conversation string using the tokenizer's chat template.\n",
    "    Assumes each example contains lists under \"instruction\" and \"output\".\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"query\"])):\n",
    "        # Build a conversation with a user message and an assistant reply.\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":    \"system\",\n",
    "                \"content\": \"You are are an expert in generate json structure based on user query and schema.\"\n",
    "                },\n",
    "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(query=example[\"query\"][i], schema=example[\"schema\"][i])},\n",
    "            # Note: It is important that the assistant message content here does not\n",
    "            # include the assistant marker, because the chat template will insert it.\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"][i]}\n",
    "            ]\n",
    "        # Use the chat template to generate the formatted text.\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def get_dataloarder():\n",
    "    # Create the data collator.\n",
    "    # It will search for the response_template (here \"Assistant:\") in the formatted text\n",
    "    # and ensure that only tokens after that marker contribute to the loss.\n",
    "    return DataCollatorForCompletionOnlyLM(response_template,\n",
    "                                           instruction_template=instruction_template,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           mlm=False)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T12:39:23.770120Z",
     "start_time": "2025-02-08T12:39:23.763820Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:28:34.101661Z",
     "iopub.execute_input": "2025-02-08T15:28:34.102035Z",
     "iopub.status.idle": "2025-02-08T15:28:34.125116Z",
     "shell.execute_reply.started": "2025-02-08T15:28:34.102005Z",
     "shell.execute_reply": "2025-02-08T15:28:34.124346Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "id": "cdc523383c666853",
   "cell_type": "markdown",
   "source": "# SFT Trainer config",
   "metadata": {}
  },
  {
   "id": "be4a6a400207624e",
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "OUTPUT_NAME = checkpoint.split(\"/\")[-1] + \"-structure-output\"\n",
    "OUTPUT_DIR = \"/kaggle/working/\" + OUTPUT_NAME\n",
    "\n",
    "\n",
    "def get_trainer(model, train_dataset, test_dataset, data_collator):\n",
    "    # Note that r, in the figure above, is a hyperparameter here that we can use to specify the rank of the low-rank matrices used for adaptation.\n",
    "    # A smaller r leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation.\n",
    "    # This can lead to faster training and potentially reduced computational requirements.\n",
    "    # However, with a smaller r, the capacity of the low-rank matrix to capture task-specific information decreases.\n",
    "    # This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher r.\n",
    "    lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=['o_proj', 'k_proj', 'q_proj', \"v_proj\"],\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            )\n",
    "    return SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            args=SFTConfig(\n",
    "                    per_device_train_batch_size=2,\n",
    "                    gradient_accumulation_steps=4,\n",
    "                    warmup_steps=100,\n",
    "                    max_steps=1000,\n",
    "                    learning_rate=0.0002,\n",
    "                    lr_scheduler_type=\"cosine\",\n",
    "                    weight_decay=0.01,\n",
    "                    bf16=True,\n",
    "                    logging_strategy=\"steps\",\n",
    "                    logging_steps=10,\n",
    "                    output_dir=OUTPUT_DIR,\n",
    "                    optim=\"paged_adamw_8bit\",\n",
    "                    seed=42,\n",
    "                    run_name=f\"train-{OUTPUT_NAME}\",\n",
    "                    report_to=\"wandb\",\n",
    "                    save_steps=10,\n",
    "                    save_total_limit=4,\n",
    "                    ),\n",
    "            peft_config=lora_config,\n",
    "            formatting_func=formatting_prompts_func,\n",
    "            data_collator=data_collator)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T12:39:36.058057Z",
     "start_time": "2025-02-08T12:39:28.735403Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:36:24.680173Z",
     "iopub.execute_input": "2025-02-08T15:36:24.680517Z",
     "iopub.status.idle": "2025-02-08T15:36:24.686395Z",
     "shell.execute_reply.started": "2025-02-08T15:36:24.680493Z",
     "shell.execute_reply": "2025-02-08T15:36:24.685607Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "id": "d97b2cd2954f98af",
   "cell_type": "code",
   "source": [
    "from transformers import is_torch_xpu_available, is_torch_npu_available\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "\n",
    "\n",
    "def main(mixed_precision=\"fp16\", seed: int = 42, batch_size: int = 64):\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision=mixed_precision)\n",
    "    model = get_model()\n",
    "    model = accelerator.prepare(model)\n",
    "    train_dataset, test_dataset = get_dataset()\n",
    "    data_collator = get_dataloarder()\n",
    "    trainer = get_trainer(model, train_dataset, test_dataset, data_collator)\n",
    "    trainer.train()\n",
    "    # Save fine tuned Lora Adaptor \n",
    "    trainer.model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_checkpoint\"))\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    if is_torch_xpu_available():\n",
    "        torch.xpu.empty_cache()\n",
    "    elif is_torch_npu_available():\n",
    "        torch.npu.empty_cache()\n",
    "    else:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(OUTPUT_DIR, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    output_merged_dir = os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\")\n",
    "    model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T12:54:55.142905Z",
     "start_time": "2025-02-08T12:39:42.727752Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:37:33.667302Z",
     "iopub.execute_input": "2025-02-08T15:37:33.667633Z",
     "iopub.status.idle": "2025-02-08T15:37:33.673641Z",
     "shell.execute_reply.started": "2025-02-08T15:37:33.667604Z",
     "shell.execute_reply": "2025-02-08T15:37:33.672808Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "id": "51c53c72-d52b-4a75-a25a-7826125f7b2e",
   "cell_type": "code",
   "source": "from accelerate import notebook_launcher",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:36:34.712755Z",
     "iopub.execute_input": "2025-02-08T15:36:34.713080Z",
     "iopub.status.idle": "2025-02-08T15:36:34.716906Z",
     "shell.execute_reply.started": "2025-02-08T15:36:34.713051Z",
     "shell.execute_reply": "2025-02-08T15:36:34.715939Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "id": "be253591-5893-4d54-8f9a-769217c6ce6b",
   "cell_type": "code",
   "source": "args = (\"fp16\", 42, 64)\nnotebook_launcher(main, args, num_processes=2)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-08T15:37:35.757009Z",
     "iopub.execute_input": "2025-02-08T15:37:35.757352Z",
     "iopub.status.idle": "2025-02-08T15:38:09.349284Z",
     "shell.execute_reply.started": "2025-02-08T15:37:35.757320Z",
     "shell.execute_reply": "2025-02-08T15:38:09.348074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Launching training on 2 GPUs.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:112: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:112: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/16515 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4664279ed1114d85bbe6fea18766d157"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "W0208 15:37:39.148000 160 torch/multiprocessing/spawn.py:160] Terminating process 240 via signal SIGTERM\nW0208 15:38:09.180000 160 torch/multiprocessing/spawn.py:168] Unable to shutdown process 240 via SIGTERM , forcefully exiting via SIGKILL\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] failed (exitcode: 1) local_rank: 0 (pid: 238) of fn: main (start_method: fork)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] Traceback (most recent call last):\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 687, in _poll\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     self._pc.join(-1)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 203, in join\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] torch.multiprocessing.spawn.ProcessRaisedException: \nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] \nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] -- Process 0 terminated with the following error:\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] Traceback (most recent call last):\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     fn(i, *args)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 611, in _wrap\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     ret = record(fn)(*args_)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     return f(*args, **kwargs)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"<ipython-input-12-da4fc2d8818d>\", line 11, in main\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     trainer = get_trainer(model, train_dataset, test_dataset, data_collator)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"<ipython-input-7-ea41278868f3>\", line 20, in get_trainer\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     return SFTTrainer(\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py\", line 165, in wrapped_func\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     return func(*args, **kwargs)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 265, in __init__\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     train_dataset = self._prepare_dataset(\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 380, in _prepare_dataset\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     return self._prepare_non_packed_dataloader(\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 455, in _prepare_non_packed_dataloader\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     tokenized_dataset = dataset.map(tokenize, **map_kwargs)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3073, in map\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     for rank, done, content in Dataset._map_single(**dataset_kwargs):\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3476, in _map_single\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     batch = apply_function_on_filtered_inputs(\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3338, in apply_function_on_filtered_inputs\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 416, in tokenize\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     element[dataset_text_field] if formatting_func is None else formatting_func(element),\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]   File \"<ipython-input-5-06b340e6ef8a>\", line 30, in formatting_prompts_func\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732]     text = tokenizer.apply_chat_template(messages, tokenize=False)\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] NameError: name 'tokenizer' is not defined\nE0208 15:38:09.296000 160 torch/distributed/elastic/multiprocessing/api.py:732] \n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mChildFailedError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-9837e1f30209>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"fp16\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m42\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mnotebook_launcher\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_processes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py\u001B[0m in \u001B[0;36mnotebook_launcher\u001B[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001B[0m\n\u001B[1;32m    243\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mis_torch_version\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\">=\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m                         \u001B[0mlaunch_config_kwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"log_line_prefix_template\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlog_line_prefix_template\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 245\u001B[0;31m                     \u001B[0melastic_launch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mLaunchConfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mlaunch_config_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mentrypoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfunction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    246\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mProcessRaisedException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0;34m\"Cannot re-initialize CUDA in forked subprocess\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 138\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlaunch_agent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_config\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_entrypoint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    139\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\u001B[0m in \u001B[0;36mlaunch_agent\u001B[0;34m(config, entrypoint, args)\u001B[0m\n\u001B[1;32m    267\u001B[0m             \u001B[0;31m# @record will copy the first error (root cause)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m             \u001B[0;31m# to the error file of the launcher process.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 269\u001B[0;31m             raise ChildFailedError(\n\u001B[0m\u001B[1;32m    270\u001B[0m                 \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mentrypoint_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m                 \u001B[0mfailures\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfailures\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mChildFailedError\u001B[0m: \n============================================================\nmain FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-02-08_15:37:38\n  host      : 78f327634f51\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 238)\n  error_file: /tmp/torchelastic_ltj10a5p/none_z2bp4pjt/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"<ipython-input-12-da4fc2d8818d>\", line 11, in main\n      trainer = get_trainer(model, train_dataset, test_dataset, data_collator)\n    File \"<ipython-input-7-ea41278868f3>\", line 20, in get_trainer\n      return SFTTrainer(\n    File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py\", line 165, in wrapped_func\n      return func(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 265, in __init__\n      train_dataset = self._prepare_dataset(\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 380, in _prepare_dataset\n      return self._prepare_non_packed_dataloader(\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 455, in _prepare_non_packed_dataloader\n      tokenized_dataset = dataset.map(tokenize, **map_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\n      out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3073, in map\n      for rank, done, content in Dataset._map_single(**dataset_kwargs):\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3476, in _map_single\n      batch = apply_function_on_filtered_inputs(\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3338, in apply_function_on_filtered_inputs\n      processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 416, in tokenize\n      element[dataset_text_field] if formatting_func is None else formatting_func(element),\n    File \"<ipython-input-5-06b340e6ef8a>\", line 30, in formatting_prompts_func\n      text = tokenizer.apply_chat_template(messages, tokenize=False)\n  NameError: name 'tokenizer' is not defined\n  \n============================================================"
     ],
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\nmain FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-02-08_15:37:38\n  host      : 78f327634f51\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 238)\n  error_file: /tmp/torchelastic_ltj10a5p/none_z2bp4pjt/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"<ipython-input-12-da4fc2d8818d>\", line 11, in main\n      trainer = get_trainer(model, train_dataset, test_dataset, data_collator)\n    File \"<ipython-input-7-ea41278868f3>\", line 20, in get_trainer\n      return SFTTrainer(\n    File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py\", line 165, in wrapped_func\n      return func(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 265, in __init__\n      train_dataset = self._prepare_dataset(\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 380, in _prepare_dataset\n      return self._prepare_non_packed_dataloader(\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 455, in _prepare_non_packed_dataloader\n      tokenized_dataset = dataset.map(tokenize, **map_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\n      out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3073, in map\n      for rank, done, content in Dataset._map_single(**dataset_kwargs):\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3476, in _map_single\n      batch = apply_function_on_filtered_inputs(\n    File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3338, in apply_function_on_filtered_inputs\n      processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\", line 416, in tokenize\n      element[dataset_text_field] if formatting_func is None else formatting_func(element),\n    File \"<ipython-input-5-06b340e6ef8a>\", line 30, in formatting_prompts_func\n      text = tokenizer.apply_chat_template(messages, tokenize=False)\n  NameError: name 'tokenizer' is not defined\n  \n============================================================",
     "output_type": "error"
    }
   ],
   "execution_count": 13
  },
  {
   "id": "22753b825ec99281",
   "cell_type": "markdown",
   "source": "# inference",
   "metadata": {}
  },
  {
   "id": "d885b58fa1ce3cef",
   "cell_type": "code",
   "source": "import torch\nimport gc\n\n\ndef clear_hardwares():\n    torch.clear_autocast_cache()\n    torch.cuda.ipc_collect()\n    torch.cuda.empty_cache()\n    gc.collect()\n\n\nclear_hardwares()\nclear_hardwares()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "111c896503677e9b",
   "cell_type": "markdown",
   "source": "## Lora adaptater",
   "metadata": {}
  },
  {
   "id": "5a8d344198fc0ff2",
   "cell_type": "code",
   "source": "base_model = AutoModelForCausalLM.from_pretrained(\"SmolLM2-135M-Instruct-structure-output\", return_dict=True,\n                                                  device_map='auto', token='')\ntokenizer = AutoTokenizer.from_pretrained(new_model, max_length=max_seq_length)\nmodel = PeftModel.from_pretrained(base_model, new_model)\ndel base_model",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "d3630c39397636dd",
   "cell_type": "markdown",
   "source": "## None lora",
   "metadata": {}
  },
  {
   "id": "3be83d7a3fbeed60",
   "cell_type": "code",
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR).to(device)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "f7429862c409ccdf",
   "cell_type": "code",
   "source": "test_json_schema = \"\"\"{\n  \"type\": \"object\",\n  \"properties\": {\n    \"weather_data\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"year\": { \"type\": \"integer\" },\n          \"station\": { \"type\": \"string\" },\n          \"temperature\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"min\": { \"type\": \"number\" },\n              \"max\": { \"type\": \"number\" }\n            },\n            \"required\": [\"min\", \"max\"]\n          },\n          \"events\": {\n            \"type\": \"array\",\n            \"items\": { \"type\": \"string\" }\n          }\n        },\n        \"required\": [\"year\", \"station\", \"temperature\", \"events\"]\n      }\n    },\n    \"required\": [\"weather_data\"]\n  }\n}\"\"\"\n\ntest_query = \"Provide a detailed breakdown of meteorological data recorded in the city of Berlin from 2015 to 2020. The data should include the year, meteorological station, temperature ranges (minimum and maximum), and any significant events.\"\n\ntest_response = \"\"\"{\n  \"weather_data\": [\n    {\n      \"year\": 2015,\n      \"station\": \"Berlin Central Station\",\n      \"temperature\": { \"min\": -5.2, \"max\": 35.1 },\n      \"events\": [\"Heavy snowfall in January\", \"Heatwave in July\"]\n    },\n    {\n      \"year\": 2017,\n      \"station\": \"Berlin East Station\",\n      \"temperature\": { \"min\": -4.0, \"max\": 32.8 },\n      \"events\": [\"Thunderstorms in April\", \"Flooding in June\"]\n    },\n    {\n      \"year\": 2020,\n      \"station\": \"Berlin West Station\",\n      \"temperature\": { \"min\": -3.9, \"max\": 36.5 },\n      \"events\": [\"Drought in September\", \"Blizzards in February\"]\n    }\n  ]\n}\"\"\"\n\nmessages = [\n    {\n        \"role\":    \"system\",\n        \"content\": \"You are are an expert in generate json structure based on user query and schema.\"\n        },\n    {\n        \"role\":    \"user\",\n        \"content\": PROMPT_TEMPLATE.format(query=test_query, schema=test_json_schema)\n        },\n    ]",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T13:04:22.005498Z",
     "start_time": "2025-02-08T13:04:21.997997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "ebdddc84e21bdfdf",
   "cell_type": "code",
   "source": "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\nprint(\"----------------- Generated text -----------------\")\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T13:04:45.164581Z",
     "start_time": "2025-02-08T13:04:26.366726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}

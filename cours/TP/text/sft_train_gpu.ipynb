{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q -U accelerate datasets peft transformers trl wandb",
   "id": "b4477a0d55d8390c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from accelerate import PartialState\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Specify the checkpoint for SmolLM2 and set the device.\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "\n",
    "# Load the tokenizer and model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# device = \"cuda\"  # or \"cpu\" for CPU usage\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "# model = model.to(device)\n",
    "# For multi-GPU setups, consider using device_map=\"auto\":\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        device_map=\"auto\", # {\"\": PartialState().process_index}\n",
    "        )"
   ],
   "id": "6ec4266ecdfecbc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer",
   "id": "4cf9e398a3a33cc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model",
   "id": "ff7fd8071c7f156c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "Json structure output: https://huggingface.co/datasets/ChristianAzinn/json-training"
   ],
   "id": "37633c01c195e6c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ChristianAzinn/json-training\")\n",
    "# Perform Train-Test Split\n",
    "split_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access train and test splits\n",
    "train_dataset = split_ds[\"train\"]\n",
    "test_dataset = split_ds[\"test\"]"
   ],
   "id": "9dd463d66d84fae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset",
   "id": "8ceed737fd63f0f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the response template to match the chat format.\n",
    "# (Ensure this string exactly matches the beginning of the assistant's response as output by apply_chat_template.)\n",
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "instruction_template = \"<|im_start|>user\\n\"\n",
    "PROMPT_TEMPLATE = \"\"\"Query: {query}\n",
    "\n",
    "schema:\n",
    "{schema}\"\"\"\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"\n",
    "    Converts each example into a conversation string using the tokenizer's chat template.\n",
    "    Assumes each example contains lists under \"instruction\" and \"output\".\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"query\"])):\n",
    "        # Build a conversation with a user message and an assistant reply.\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":    \"system\",\n",
    "                \"content\": \"You are are an expert in generate json structure based on user query and schema.\"\n",
    "                },\n",
    "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(query=example[\"query\"][i], schema=example[\"schema\"][i])},\n",
    "            # Note: It is important that the assistant message content here does not\n",
    "            # include the assistant marker, because the chat template will insert it.\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"][i]}\n",
    "            ]\n",
    "        # Use the chat template to generate the formatted text.\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "# Create the data collator.\n",
    "# It will search for the response_template (here \"Assistant:\") in the formatted text\n",
    "# and ensure that only tokens after that marker contribute to the loss.\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                           instruction_template=instruction_template,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           mlm=False)"
   ],
   "id": "ddf37774132f48c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer.apply_chat_template([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I am good, thank you.\"}\n",
    "    ], tokenize=False)"
   ],
   "id": "793ac796b6f1284c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lora Config",
   "id": "a96feb2a6e00d710"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# Note that r, in the figure above, is a hyperparameter here that we can use to specify the rank of the low-rank matrices used for adaptation.\n",
    "# A smaller r leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation.\n",
    "# This can lead to faster training and potentially reduced computational requirements.\n",
    "# However, with a smaller r, the capacity of the low-rank matrix to capture task-specific information decreases.\n",
    "# This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher r.\n",
    "lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=['o_proj', 'k_proj', 'q_proj', \"v_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        )"
   ],
   "id": "da1f50aa45ee897a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Wandb\n",
    "\n",
    "Creat token and account: https://wandb.ai/home"
   ],
   "id": "7c2d2ea451fe4ba0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ],
   "id": "7c2ddcdf716fe186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SFT Trainer config",
   "id": "cdc523383c666853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "OUTPUT_DIR = checkpoint.split(\"/\")[-1] + \"-structure-output\"\n",
    "\n",
    "# setup the trainer\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        args=SFTConfig(\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_steps=100,\n",
    "                max_steps=1000,\n",
    "                learning_rate=0.0002,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=150,\n",
    "                weight_decay=0.01,\n",
    "                bf16=True,\n",
    "                logging_strategy=\"steps\",\n",
    "                logging_steps=10,\n",
    "                output_dir=\"./\" + OUTPUT_DIR,\n",
    "                optim=\"paged_adamw_8bit\",\n",
    "                seed=42,\n",
    "                run_name=f\"train-{OUTPUT_DIR}\",\n",
    "                report_to=\"wandb\",\n",
    "                save_steps=31,\n",
    "                save_total_limit=4,\n",
    "                ),\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=collator,\n",
    "        )"
   ],
   "id": "be4a6a400207624e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Start fine-tuning.\n",
    "trainer.train()\n",
    "\n",
    "# TODO\n",
    "\n",
    "\"\"\"\n",
    "    # Save fine tuned Lora Adaptor\n",
    "    trainer.model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_checkpoint\"))\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    if is_torch_xpu_available():\n",
    "        torch.xpu.empty_cache()\n",
    "    elif is_torch_npu_available():\n",
    "        torch.npu.empty_cache()\n",
    "    else:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(OUTPUT_DIR, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    output_merged_dir = os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\")\n",
    "    model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\"\"\""
   ],
   "id": "d97b2cd2954f98af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# inference",
   "id": "22753b825ec99281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "def clear_hardwares():\n",
    "    torch.clear_autocast_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "clear_hardwares()\n",
    "clear_hardwares()"
   ],
   "id": "d885b58fa1ce3cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lora adaptater",
   "id": "111c896503677e9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"SmolLM2-135M-Instruct-structure-output\", return_dict=True,\n",
    "                                                  device_map='auto', token='')\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model, max_length=max_seq_length)\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "del base_model"
   ],
   "id": "5a8d344198fc0ff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## None lora",
   "id": "d3630c39397636dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR).to(device)"
   ],
   "id": "3be83d7a3fbeed60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_json_schema = \"\"\"{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"weather_data\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"year\": { \"type\": \"integer\" },\n",
    "          \"station\": { \"type\": \"string\" },\n",
    "          \"temperature\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"min\": { \"type\": \"number\" },\n",
    "              \"max\": { \"type\": \"number\" }\n",
    "            },\n",
    "            \"required\": [\"min\", \"max\"]\n",
    "          },\n",
    "          \"events\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": { \"type\": \"string\" }\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"year\", \"station\", \"temperature\", \"events\"]\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"weather_data\"]\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "test_query = \"Provide a detailed breakdown of meteorological data recorded in the city of Berlin from 2015 to 2020. The data should include the year, meteorological station, temperature ranges (minimum and maximum), and any significant events.\"\n",
    "\n",
    "test_response = \"\"\"{\n",
    "  \"weather_data\": [\n",
    "    {\n",
    "      \"year\": 2015,\n",
    "      \"station\": \"Berlin Central Station\",\n",
    "      \"temperature\": { \"min\": -5.2, \"max\": 35.1 },\n",
    "      \"events\": [\"Heavy snowfall in January\", \"Heatwave in July\"]\n",
    "    },\n",
    "    {\n",
    "      \"year\": 2017,\n",
    "      \"station\": \"Berlin East Station\",\n",
    "      \"temperature\": { \"min\": -4.0, \"max\": 32.8 },\n",
    "      \"events\": [\"Thunderstorms in April\", \"Flooding in June\"]\n",
    "    },\n",
    "    {\n",
    "      \"year\": 2020,\n",
    "      \"station\": \"Berlin West Station\",\n",
    "      \"temperature\": { \"min\": -3.9, \"max\": 36.5 },\n",
    "      \"events\": [\"Drought in September\", \"Blizzards in February\"]\n",
    "    }\n",
    "  ]\n",
    "}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":    \"system\",\n",
    "        \"content\": \"You are are an expert in generate json structure based on user query and schema.\"\n",
    "        },\n",
    "    {\n",
    "        \"role\":    \"user\",\n",
    "        \"content\": PROMPT_TEMPLATE.format(query=test_query, schema=test_json_schema)\n",
    "        },\n",
    "    ]"
   ],
   "id": "f7429862c409ccdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "print(\"----------------- Generated text -----------------\")\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "ebdddc84e21bdfdf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

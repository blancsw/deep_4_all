{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q -U accelerate datasets peft transformers trl wandb bitsandbytes",
   "id": "b4477a0d55d8390c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Specify the checkpoint for SmolLM2 and set the device.\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "# Load the tokenizer and model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# For multi-GPU setups, consider using device_map=\"auto\":\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint,\n",
    "        device_map=\"auto\",  # {\"\": PartialState().process_index}\n",
    "        )"
   ],
   "id": "6ec4266ecdfecbc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer",
   "id": "4cf9e398a3a33cc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model",
   "id": "ff7fd8071c7f156c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "Json structure output: https://huggingface.co/datasets/ChristianAzinn/json-training"
   ],
   "id": "37633c01c195e6c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ChristianAzinn/json-training\")\n",
    "# Perform Train-Test Split\n",
    "split_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access train and test splits\n",
    "train_dataset = split_ds[\"train\"]\n",
    "test_dataset = split_ds[\"test\"]"
   ],
   "id": "9dd463d66d84fae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset",
   "id": "8ceed737fd63f0f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the response template to match the chat format.\n",
    "# (Ensure this string exactly matches the beginning of the assistant's response as output by apply_chat_template.)\n",
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "instruction_template = \"<|im_start|>user\\n\"\n",
    "PROMPT_TEMPLATE = \"\"\"Query: {query}\n",
    "\n",
    "schema:\n",
    "{schema}\"\"\"\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"\n",
    "    Converts each example into a conversation string using the tokenizer's chat template.\n",
    "    Assumes each example contains lists under \"instruction\" and \"output\".\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"query\"])):\n",
    "        # Build a conversation with a user message and an assistant reply.\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\":    \"system\",\n",
    "                \"content\": \"You are are an expert in generate json structure based on user query and schema.\"\n",
    "                },\n",
    "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(query=example[\"query\"][i], schema=example[\"schema\"][i])},\n",
    "            # Note: It is important that the assistant message content here does not\n",
    "            # include the assistant marker, because the chat template will insert it.\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"][i]}\n",
    "            ]\n",
    "        # Use the chat template to generate the formatted text.\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "# Create the data collator.\n",
    "# It will search for the response_template (here \"Assistant:\") in the formatted text\n",
    "# and ensure that only tokens after that marker contribute to the loss.\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                           instruction_template=instruction_template,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           mlm=False)"
   ],
   "id": "ddf37774132f48c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer.apply_chat_template([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I am good, thank you.\"}\n",
    "    ], tokenize=False)"
   ],
   "id": "793ac796b6f1284c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lora Config",
   "id": "a96feb2a6e00d710"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# Note that r, in the figure above, is a hyperparameter here that we can use to specify the rank of the low-rank matrices used for adaptation.\n",
    "# A smaller r leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation.\n",
    "# This can lead to faster training and potentially reduced computational requirements.\n",
    "# However, with a smaller r, the capacity of the low-rank matrix to capture task-specific information decreases.\n",
    "# This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher r.\n",
    "lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=['o_proj', 'k_proj', 'q_proj', \"v_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        )"
   ],
   "id": "da1f50aa45ee897a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Wandb\n",
    "\n",
    "Creat token and account: https://wandb.ai/home"
   ],
   "id": "7c2d2ea451fe4ba0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "import getpass\n",
    "\n",
    "token = getpass.getpass()\n",
    "wandb.login(key=token)"
   ],
   "id": "7c2ddcdf716fe186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SFT Trainer config",
   "id": "cdc523383c666853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "OUTPUT_DIR = checkpoint.split(\"/\")[-1] + \"-structure-output\"\n",
    "\n",
    "# setup the trainer\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        args=SFTConfig(\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_steps=100,\n",
    "                max_steps=1000,\n",
    "                learning_rate=0.0002,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=150,\n",
    "                weight_decay=0.01,\n",
    "                bf16=True,\n",
    "                logging_strategy=\"steps\",\n",
    "                logging_steps=10,\n",
    "                output_dir=\"./\" + OUTPUT_DIR,\n",
    "                optim=\"paged_adamw_8bit\",\n",
    "                seed=42,\n",
    "                run_name=f\"train-{OUTPUT_DIR}\",\n",
    "                report_to=\"wandb\",\n",
    "                save_steps=31,\n",
    "                save_total_limit=4,\n",
    "                ),\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=collator,\n",
    "        )"
   ],
   "id": "be4a6a400207624e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import is_torch_xpu_available, is_torch_npu_available\n",
    "import torch\n",
    "\n",
    "# Lancement du processus d'entraînement du modèle.\n",
    "# Ici, 'trainer.train()' déclenche la phase de fine-tuning,\n",
    "# dans laquelle les paramètres du modèle sont ajustés sur une tâche spécifique\n",
    "# en utilisant des données d'entraînement pertinentes.\n",
    "trainer.train()\n",
    "\n",
    "# Une fois l'entraînement terminé, on sauvegarde l'adaptateur LoRA (fine-tuning léger).\n",
    "# LoRA (Low-Rank Adaptation) est une technique destinée à fine-tuner les grands\n",
    "# modèles en modifiant uniquement un sous-ensemble restreint de paramètres.\n",
    "final_checkpoint_dir = os.path.join(OUTPUT_DIR, \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(final_checkpoint_dir)\n",
    "\n",
    "# Nettoyage des ressources mémoire pour libérer l'espace GPU ou autres accélérateurs,\n",
    "# ce qui est utile avant de fusionner l'adaptateur LoRA avec le modèle de base.\n",
    "del model  # Suppression explicite du modèle de la mémoire.\n",
    "\n",
    "# Vider les caches des accélérateurs (XPU, NPU ou GPU en fonction de la disponibilité).\n",
    "# Cela optimise l'utilisation future des ressources.\n",
    "if is_torch_xpu_available():\n",
    "    torch.xpu.empty_cache()  # Vide les caches spécifiques pour XPU.\n",
    "elif is_torch_npu_available():\n",
    "    torch.npu.empty_cache()  # Vide les caches spécifiques pour NPU.\n",
    "else:\n",
    "    torch.cuda.empty_cache()  # Vide les caches GPU standard.\n",
    "\n",
    "# Chargement du modèle adapté (en incluant l'adaptateur LoRA) pour effectuer une fusion\n",
    "# avec le modèle de base. Cela permet de sauvegarder un modèle autonome optimisé.\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Chargement du modèle préalablement sauvegardé depuis le répertoire OUTPUT_DIR.\n",
    "# Les paramètres 'device_map' et 'torch_dtype' permettent d'optimiser le chargement :\n",
    "# - 'device_map=\"auto\"' ajuste automatiquement le placement sur le GPU, CPU ou autre.\n",
    "# - 'torch_dtype=torch.bfloat16' utilise un format numérique bfloat16, qui réduit\n",
    "#    la mémoire nécessaire tout en maintenant des performances stables.\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        OUTPUT_DIR,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "# Fusion de l'adaptateur LoRA directement dans le modèle de base,\n",
    "# afin de produire un modèle final unique tout en réduisant ses redondances.\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Sauvegarde du modèle fusionné dans un répertoire spécifique.\n",
    "# 'safe_serialization=True' garantit que le modèle est stocké au format sûr,\n",
    "# pour une compatibilité future et une intégrité des données.\n",
    "output_merged_dir = os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\")\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ],
   "id": "d97b2cd2954f98af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# inference",
   "id": "22753b825ec99281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR).to(device)"
   ],
   "id": "3be83d7a3fbeed60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_json_schema = \"\"\"{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"weather_data\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"year\": { \"type\": \"integer\" },\n",
    "          \"station\": { \"type\": \"string\" },\n",
    "          \"temperature\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"min\": { \"type\": \"number\" },\n",
    "              \"max\": { \"type\": \"number\" }\n",
    "            },\n",
    "            \"required\": [\"min\", \"max\"]\n",
    "          },\n",
    "          \"events\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": { \"type\": \"string\" }\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"year\", \"station\", \"temperature\", \"events\"]\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"weather_data\"]\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "test_query = \"Provide a detailed breakdown of meteorological data recorded in the city of Berlin from 2015 to 2020. The data should include the year, meteorological station, temperature ranges (minimum and maximum), and any significant events.\"\n",
    "\n",
    "test_response = \"\"\"{\n",
    "  \"weather_data\": [\n",
    "    {\n",
    "      \"year\": 2015,\n",
    "      \"station\": \"Berlin Central Station\",\n",
    "      \"temperature\": { \"min\": -5.2, \"max\": 35.1 },\n",
    "      \"events\": [\"Heavy snowfall in January\", \"Heatwave in July\"]\n",
    "    },\n",
    "    {\n",
    "      \"year\": 2017,\n",
    "      \"station\": \"Berlin East Station\",\n",
    "      \"temperature\": { \"min\": -4.0, \"max\": 32.8 },\n",
    "      \"events\": [\"Thunderstorms in April\", \"Flooding in June\"]\n",
    "    },\n",
    "    {\n",
    "      \"year\": 2020,\n",
    "      \"station\": \"Berlin West Station\",\n",
    "      \"temperature\": { \"min\": -3.9, \"max\": 36.5 },\n",
    "      \"events\": [\"Drought in September\", \"Blizzards in February\"]\n",
    "    }\n",
    "  ]\n",
    "}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":    \"system\",\n",
    "        \"content\": \"You are are an expert in generate json structure based on user query and schema.\"\n",
    "        },\n",
    "    {\n",
    "        \"role\":    \"user\",\n",
    "        \"content\": PROMPT_TEMPLATE.format(query=test_query, schema=test_json_schema)\n",
    "        },\n",
    "    ]"
   ],
   "id": "f7429862c409ccdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "print(\"----------------- Generated text -----------------\")\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "ebdddc84e21bdfdf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

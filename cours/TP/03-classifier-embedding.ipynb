{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TP : Classification de critiques IMDb avec PyTorch\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- **Manipuler le dataset IMDb** en utilisant la librairie [datasets](https://huggingface.co/docs/datasets).\n",
    "- **Prétraiter le texte** : tokenisation, construction d’un vocabulaire, conversion des textes en séquences d’indices et mise à la même longueur (padding).\n",
    "- **Construire un modèle simple** en PyTorch composé d’une couche d’**embedding** et d’une couche dense.\n",
    "- **Entraîner et évaluer le modèle** sur un problème de classification binaire (critique positive / critique négative).\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Nous souhaitons construire un modèle de classification de critiques de films. Pour cela, nous allons :\n",
    "- Charger et découper le dataset IMDb.\n",
    "- Préparer les données pour les faire passer dans un modèle PyTorch.\n",
    "- Construire un réseau de neurones simple qui se compose d’une couche d’embedding (pour transformer chaque mot en vecteur) et d’une couche dense (pour réaliser la classification).\n",
    "- Entraîner le modèle et évaluer ses performances.\n",
    "\n",
    "---\n",
    "\n",
    "## Questions\n",
    "\n",
    "- **Simplicité du modèle :** Ce modèle ne prend en compte que l’information globale par une moyenne des embeddings. Quelles seraient les limites de cette approche ?\n",
    "- **Prétraitement du texte :** Quels outils (par exemple, spaCy, nltk) pourraient être utilisés pour améliorer la tokenisation et la gestion du vocabulaire ?\n",
    "- **Améliorations possibles :** Proposez des idées afin d'améliorez ce modèle, ajouter des layers ou autre modification"
   ],
   "id": "94ef562416214e7b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install -q -U torch datasets numpy torchtext",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Chargement du dataset \"imdb\" depuis HuggingFace\n",
    "raw_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "# Split stratifié 50/50\n",
    "dataset = raw_dataset.train_test_split(stratify_by_column=\"label\", test_size=0.5, seed=42)\n",
    "train_dataset_raw = dataset[\"train\"]\n",
    "test_dataset_raw = dataset[\"test\"]\n",
    "\n",
    "print(\"Nombre d'échantillons dans le train set :\", len(train_dataset_raw))\n",
    "print(\"Nombre d'échantillons dans le test set  :\", len(test_dataset_raw))"
   ],
   "id": "a11b485613fc7f97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Exemple de review :\", train_dataset_raw[0][\"text\"][:300], \"...\")\n",
    "print(\"Label (0 = négatif, 1 = positif) :\", train_dataset_raw[0][\"label\"])"
   ],
   "id": "646cc97f5d7600fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prétraitement des données avec torchtext\n",
    "\n",
    "Pour préparer les textes :\n",
    "- **Tokenisation** : nous utiliserons le [tokenizer](https://huggingface.co/docs/transformers/en/main_classes/tokenizer) de Hugging Face\n",
    "- **Construction du vocabulaire** : à partir des tokens du jeu d’entraînement, nous alons créez un dictionnaire associant chaque mot à un identifiant numérique. Pensez à réserver un index pour les tokens inconnus (`<UNK>`) et pour le padding (`<PAD>`).\n",
    "- **Transformation** : convertissez chaque critique en une séquence d’indices.\n",
    "- **Padding / Troncature** : pour obtenir des séquences de taille fixe (par exemple, 256 tokens)."
   ],
   "id": "6281f1d70e8fd401"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# Paramètres de prétraitement\n",
    "MAX_SEQ_LEN = 256  # longueur maximale de la séquence\n",
    "MIN_FREQ = 2  # fréquence minimale pour intégrer un mot dans le vocabulaire\n",
    "\n",
    "# Création d'un tokenizer WordLevel\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Définition des tokens spéciaux\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\"]\n",
    "\n",
    "# Entraîneur qui prendra en compte uniquement les tokens apparaissant au moins MIN_FREQ fois\n",
    "trainer = WordLevelTrainer(special_tokens=special_tokens, min_frequency=MIN_FREQ)\n",
    "\n",
    "# Récupération des textes d'entraînement\n",
    "texts = [example[\"text\"] for example in raw_dataset]\n",
    "\n",
    "# Entraînement du tokenizer sur ces textes\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "# Activation de la troncature et du padding pour obtenir des séquences de longueur fixe\n",
    "tokenizer.enable_truncation(max_length=MAX_SEQ_LEN)\n",
    "tokenizer.enable_padding(length=MAX_SEQ_LEN,\n",
    "                         pad_id=tokenizer.token_to_id(\"[PAD]\"),\n",
    "                         pad_token=\"[PAD]\")\n",
    "\n",
    "# Affichage de la taille du vocabulaire\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"Taille du vocabulaire: {vocab_size}\")"
   ],
   "id": "2f1eb1cf20991228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ont test le tokenizer pour voir les différents tokens\n",
    "print(tokenizer.encode(\"Hello worl !\").tokens[:10])\n",
    "print(tokenizer.encode(\"Hello worl !\").ids[:10])"
   ],
   "id": "54a6704d73435054",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Création du Dataset PyTorch",
   "id": "f1a68a19285f7a5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, raw_dataset: datasets.arrow_dataset.Dataset, tokenizer: Tokenizer):\n",
    "        self.raw_dataset = raw_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.raw_dataset[idx][\"text\"]\n",
    "        label = self.raw_dataset[idx][\"label\"]\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        return torch.tensor(encoded.ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Création des datasets PyTorch\n",
    "train_dataset = IMDBDataset(train_dataset_raw, tokenizer)\n",
    "test_dataset = IMDBDataset(test_dataset_raw, tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ],
   "id": "eb6bedb12083cabb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Entraînement et évaluation du modèle",
   "id": "194ece40a213b1a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    # Passer le modèle en mode apprentissage (entraînement)\n",
    "    # Cette étape permet d'activer certains mécanismes spécifiques à l'entraînement,\n",
    "    # comme le dropout ou la normalisation par batch.\n",
    "    model.train()\n",
    "\n",
    "    # Initialisation des accumulateurs pour la perte totale,\n",
    "    # le nombre de prédictions correctes et le total des échantillons.\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Boucle principale traitant chaque lot (batch) d'échantillons dans le DataLoader\n",
    "    for sequences, labels in loader:\n",
    "        # Charger les données et leurs étiquettes associées sur le même appareil\n",
    "        # (GPU ou CPU) pour calculs.\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Réinitialisation des gradients cumulés de l'optimiseur.\n",
    "        # Cela empêche d'accumuler des gradients des itérations précédentes.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Passage avant (forward pass) : le modèle génère des prédictions à partir des séquences.\n",
    "        outputs = model(sequences)\n",
    "\n",
    "        # Calcul de la perte (erreur) entre les prédictions du modèle et les étiquettes vraies.\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Passage arrière (backward pass) : calcul des gradients pour les poids du modèle.\n",
    "        loss.backward()\n",
    "\n",
    "        # Mise à jour des poids du modèle avec l'optimiseur selon le gradient calculé.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumuler la perte totale pour cette époque.\n",
    "        # La perte est pondérée par la taille du lot actuel pour obtenir une moyenne correcte plus tard.\n",
    "        epoch_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "        # Convertir les prédictions du modèle en étiquettes de classe en sélectionnant\n",
    "        # l'indice de la classe avec la probabilité la plus élevée.\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        # Ajouter le nombre de prédictions correctes pour ce lot.\n",
    "        # Cela compare les étiquettes prédites avec les étiquettes réelles.\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "        # Mettre à jour le nombre total d'échantillons traités.\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Retourner la perte moyenne et la précision pour cette époque.\n",
    "    # La perte moyenne est la perte totale divisée par le total d'échantillons.\n",
    "    # La précision est le ratio des prédictions correctes sur toutes les données.\n",
    "    return epoch_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluation  et similaire au train mais il n'y a pas de backward pass, nous utilisons un dataset de test\n",
    "    \"\"\"\n",
    "    # Passer le modèle en mode évaluation. Pendant cette phase, certaines couches,\n",
    "    # comme le dropout ou la normalisation par batch, sont désactivées.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialisation des accumulateurs pour les métriques d'évaluation.\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Empêcher la mise à jour des gradients (désactivation de autograd).\n",
    "    # Cela permet d'économiser de la mémoire et d'accélérer l'exécution.\n",
    "    with torch.no_grad():\n",
    "        # Boucle principale traitant chaque lot (batch) d'échantillons dans le DataLoader\n",
    "        for sequences, labels in loader:\n",
    "            # Charger les données et leurs étiquettes sur le même appareil\n",
    "            # (GPU ou CPU) pour calculs.\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Passage avant : le modèle prédit les étiquettes à partir des séquences.\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Calcul de la perte entre les prédictions et les étiquettes réelles.\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumuler la perte totale pour cette époque, pondérée par la taille du lot actuel.\n",
    "            epoch_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "            # Convertir les prédictions du modèle en indices correspondants à la classe\n",
    "            # avec la probabilité maximale.\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            # Ajouter le nombre de prédictions correctes pour ce lot.\n",
    "            # Cela compare les étiquettes prédites avec les étiquettes réelles.\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            # Mettre à jour le nombre total d'échantillons traités.\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Retourner la perte moyenne et la précision pour cette phase d'évaluation.\n",
    "    return epoch_loss / total, correct / total"
   ],
   "id": "96c5ef14fe049308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Definitions du model",
   "id": "a04da677b85d9265"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_class: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        pass"
   ],
   "id": "8e8960a539e8a5e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train loop",
   "id": "67ddf49d0bc3e9a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device : {device}\")\n",
    "EMBED_DIM = 50\n",
    "NUM_CLASS = 2  # IMDb est un dataset binaire (0: négatif, 1: positif)\n",
    "pad_idx = tokenizer.token_to_id(\"[PAD]\")\n",
    "model = SimpleClassifier(vocab_size, EMBED_DIM, NUM_CLASS, pad_idx)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "NUM_EPOCHS = 5"
   ],
   "id": "2dcb75e16393214e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test de votre model avent de lancer un train pour debug les potentielles erreur de dimensions",
   "id": "bb8d374c3db28586"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dummy_input = torch.tensor([tokenizer.encode(\"Hello world !\").ids]).to(device)\n",
    "print(model(dummy_input))"
   ],
   "id": "356185574ebb3e9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc * 100:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc * 100:.2f}%\")"
   ],
   "id": "c0f036c41889424e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

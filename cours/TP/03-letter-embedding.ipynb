{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP: Next Letter Prediction\n",
    "\n",
    "Dans ce TP, vous allez entraîner un modèle pour prédire le caractère suivant.\n",
    "\n",
    "## Objectifs du TP\n",
    "\n",
    "1. Modifier le prétraitement du dataset\n",
    "2. Convertir des tokens en id afin de créer un vocabulaire\n",
    "3. Se familiariser avec la couche d'embedding\n",
    "4. Modifier un réseau pour l'entraîner d'une autre manière\n",
    "\n",
    "## Guide\n",
    "\n",
    "### 1\n",
    "\n",
    "Modifier le code existant afin de donner une lettre d'entrée au modèle:\n",
    "\n",
    "Example: Le chat\n",
    "\n",
    "```\n",
    "'l', 'e' --> model --> ' '\n",
    "'e', ' ' --> model --> 'c'\n",
    "' ', 'c' --> model --> 'h'\n",
    "```\n",
    "\n",
    "### 2\n",
    "\n",
    "Même chose que la question 1, mais en passant avec 3 lettres en entrée:\n",
    "\n",
    "Example: Le chat\n",
    "\n",
    "```\n",
    "'l', 'e', ' ' --> model --> 'c'\n",
    "'e', ' ', 'c' --> model --> 'h'\n",
    "```\n",
    "\n",
    "### 3\n",
    "\n",
    "Maintenant, nous allons essayer une autre technique d'entraînement. L'idée sera de masquer aléatoirement une des 3 lettres en entrée et le modèle devra deviner laquelle c'est.\n",
    "\n",
    "Example: Le chat\n",
    "\n",
    "```\n",
    "'l', '[MASK]', ' ' --> model --> 'e'\n",
    "'e', ' ', '[MASK]' --> model --> 'c'\n",
    "```\n",
    "\n",
    "Pour cela, ajouter un nouvel token id 0 (ou autre chose) afin de représenter le token de masque.\n",
    "\n",
    "### 4\n",
    "\n",
    "Tentez d'augmenter la dimension de `embedding_size` et ajoutez des couches `torch.nn.Linear` supplémentaires.\n",
    "Cela vous permettra de voir comment la loss évolue.\n",
    "\n",
    "# Code de base\n",
    "\n",
    "Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b1b8ae264e3b7f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and take subset of the dataset\n",
    "df = pd.read_csv(\"../CM/data/disney_review/train.csv\")[:5000]\n",
    "# Get reviews\n",
    "reviews = df[\"Review_Text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare the datas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df23a4b1bb4ec85"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import re\n",
    "\n",
    "\n",
    "def sliding_window(txt):\n",
    "    \"\"\"\n",
    "    Generates a sliding window of consecutive character pairs from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    txt (str): The input text.\n",
    "\n",
    "    Yields:\n",
    "    tuple: A pair of consecutive characters from the text. The first character is at index i and the second character\n",
    "    is at index i + 1.\n",
    "\n",
    "    Example:\n",
    "    >>> for pair in sliding_window(\"hello\"):\n",
    "    ...     print(pair)\n",
    "    ...\n",
    "    ('h', 'e')\n",
    "    ('e', 'l')\n",
    "    ('l', 'l')\n",
    "    ('l', 'o')\n",
    "    \"\"\"\n",
    "    for i in range(len(txt) - 1):\n",
    "        yield txt[i], txt[i + 1]\n",
    "\n",
    "\n",
    "window = []\n",
    "for title in reviews:\n",
    "    # Get only a to z and 0 to 9 letters and numerb\n",
    "    title = re.sub('[^a-zA-Z0-9 ]+', '', title.lower())\n",
    "    window.append(sliding_window(title))\n",
    "window = list(it.chain(*window))\n",
    "\n",
    "# Number of window\n",
    "print(len(window))\n",
    "# Get first 5 example\n",
    "window[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f347453310ed54",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "creat letter to ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a495e9d3991e92b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mapping = {c: i for i, c in enumerate(pd.DataFrame(window)[0].unique())}\n",
    "# Get the input char\n",
    "integers_in = np.array([mapping[w[0]] for w in window])\n",
    "# Get the output char to predict\n",
    "integers_out = np.array([mapping[w[1]] for w in window])\n",
    "\n",
    "print(\"Shape of input\", integers_in.shape)\n",
    "print(\"Input example\", integers_in[0], integers_out[0])\n",
    "print(\"Show generate mapping\\n\", mapping)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e6eaff290bcac42",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creat the dataset class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "121055b37895cb8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class NextLetterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    NextLetterDataset\n",
    "\n",
    "    A custom dataset class for next letter prediction.\n",
    "\n",
    "    Attributes:\n",
    "        integers_in (list): A list of integers representing input data.\n",
    "        integers_out (list): A list of integers representing output data.\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of data points in the dataset.\n",
    "        __getitem__(idx: int): Returns the idx-th data point and its corresponding label.\n",
    "\n",
    "    Example usage:\n",
    "        integers_in = [1, 2, 3]\n",
    "        integers_out = [7, 8, 9]\n",
    "        dataset = NextLetterDataset(integers_in, integers_in2, integers_out)\n",
    "        print(len(dataset))  # Output: 3\n",
    "        print(dataset[0])  # Output: (tensor(1), tensor(7))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, integers_in, integers_out):\n",
    "        self.integers_in = integers_in\n",
    "        self.integers_out = integers_out\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data pin dataset\n",
    "        return len(self.integers_in)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.integers_in[idx]\n",
    "        data_label = self.integers_out[idx]\n",
    "        return torch.tensor(data_point), torch.tensor(data_label, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c429c6f7840ff7d5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff39e9ea8bdf38b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class NextLetterPrediction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    This class represents a next letter prediction model based on a neural network architecture.\n",
    "\n",
    "    Attributes:\n",
    "    - embedding: An instance of torch.nn.Embedding representing the embedding layer.\n",
    "    - fc: An instance of torch.nn.Linear representing the fully connected layer.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, vocab_size, embedding_size):\n",
    "        Initializes the NextLetterPrediction class with the given vocabulary size and embedding size.\n",
    "\n",
    "        Parameters:\n",
    "            - vocab_size: An integer representing the size of the vocabulary (number of characters).\n",
    "            - embedding_size: An integer representing the size of the embedding layer.\n",
    "\n",
    "    - forward(self, x):\n",
    "        Performs a forward pass on the model.\n",
    "\n",
    "        Parameters:\n",
    "            x: The input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            The output tensor of shape (batch_size, vocab_size).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(NextLetterPrediction, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input char tensor of shape (batch_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Next letter prediction tensor of shape (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        # shape: (batch_size, embedding_size)\n",
    "        x = F.relu(self.embedding(x))\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84f9768e5c34a040",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = NextLetterPrediction(vocab_size=len(mapping),\n",
    "                             # For X and Y plot\n",
    "                             embedding_size=2)\n",
    "\n",
    "# Init the dataset into the DataLoader\n",
    "dataset = NextLetterDataset(integers_in, integers_out)\n",
    "trainloader = data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c5e9554e3c1e9d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10a244d56a255f49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "epoches = 1\n",
    "\n",
    "# Get cpu/gpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Load the model to cuda device in train mode\n",
    "model.to(device)\n",
    "model.train()\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(epoches):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "        # shape: (batch_size)\n",
    "        inputs = inputs.to(device)\n",
    "        # shape: (batch_size)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f170182c3d760747",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idx_to_calc = list(mapping.values())\n",
    "idx_to_calc = np.array([idx_to_calc]).T\n",
    "\n",
    "translator = {v: k for k, v in mapping.items()}\n",
    "preds = model.embedding(torch.tensor(idx_to_calc).to(device)).cpu().detach().numpy()\n",
    "plt.scatter(preds[:, 0, 0], preds[:, 0, 1], alpha=0)\n",
    "for i, idx in enumerate(idx_to_calc):\n",
    "    plt.text(preds[i, 0, 0], preds[i, 0, 1], translator[idx[0]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f069bc53b9aa906b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.mask_token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b763591438d98b89",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate datasets peft bitsandbytes tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T20:21:06.058083Z","iopub.execute_input":"2025-02-05T20:21:06.058265Z","iopub.status.idle":"2025-02-05T20:21:14.673987Z","shell.execute_reply.started":"2025-02-05T20:21:06.058247Z","shell.execute_reply":"2025-02-05T20:21:14.673145Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget -q -O smolvlm.py https://raw.githubusercontent.com/blancsw/deep_4_all/refs/heads/main/cours/TP/smolvlm.py\n!cat smolvlm.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T20:42:54.185279Z","iopub.execute_input":"2025-02-05T20:42:54.185665Z","iopub.status.idle":"2025-02-05T20:42:54.575875Z","shell.execute_reply.started":"2025-02-05T20:42:54.185628Z","shell.execute_reply":"2025-02-05T20:42:54.574857Z"}},"outputs":[{"name":"stdout","text":"import torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration, Trainer, TrainingArguments\n\nUSE_QLORA = False\n\nmodel_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n\n\ndef main():\n    ds = load_dataset('merve/vqav2-small', trust_remote_code=True)\n    split_ds = ds[\"validation\"].train_test_split(test_size=0.5)\n    train_ds = split_ds[\"train\"]\n    processor = AutoProcessor.from_pretrained(\n            model_id\n            )\n    lora_config = LoraConfig(\n            r=8,\n            lora_alpha=8,\n            lora_dropout=0.1,\n            # nous entrainons seulment les linear layer du multi head attention\n            # https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src/transformers/models/llama/modeling_llama.py#L254\n            target_modules=['down_proj', 'o_proj', 'k_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj'],\n            use_dora=False if USE_QLORA else True,\n            init_lora_weights=\"gaussian\"\n            )\n    lora_config.inference_mode = False\n    if USE_QLORA:\n        bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16\n                )\n\n    model = Idefics3ForConditionalGeneration.from_pretrained(\n            model_id,\n            quantization_config=bnb_config if USE_QLORA else None,\n            # you can use flash_attention_2 if you are on newer GPU than T4\n            _attn_implementation=\"eager\"\n            )\n    model.add_adapter(lora_config)\n    model.enable_adapters()\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, lora_config)\n    print(model.get_nb_trainable_parameters())\n\n    image_token_id = processor.tokenizer.additional_special_tokens_ids[\n        processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n\n    def collate_fn(examples):\n        texts = []\n        images = []\n        for example in examples:\n            image = example[\"image\"]\n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            question = example[\"question\"]\n            answer = example[\"multiple_choice_answer\"]\n            messages = [\n                {\n                    \"role\":    \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n                        {\"type\": \"image\"},\n                        {\"type\": \"text\", \"text\": question}\n                        ]\n                    },\n                {\n                    \"role\":    \"assistant\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": answer}\n                        ]\n                    }\n                ]\n            text = processor.apply_chat_template(messages, add_generation_prompt=False)\n            texts.append(text.strip())\n            images.append([image])\n\n        batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n        labels = batch[\"input_ids\"].clone()\n        labels[labels == processor.tokenizer.pad_token_id] = -100\n        labels[labels == image_token_id] = -100\n        batch[\"labels\"] = labels\n\n        return batch\n\n    model_name = model_id.split(\"/\")[-1]\n\n    training_args = TrainingArguments(\n            num_train_epochs=1,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=4,\n            warmup_steps=50,\n            learning_rate=1e-4,\n            weight_decay=0.01,\n            logging_steps=25,\n            save_strategy=\"steps\",\n            save_steps=250,\n            save_total_limit=1,\n            optim=\"paged_adamw_8bit\",  # for 8-bit, keep this, else adamw_hf\n            bf16=True,  # underlying precision for 8bit\n            output_dir=f\"./{model_name}-vqav2\",\n            hub_model_id=f\"{model_name}-vqav2\",\n            report_to=\"tensorboard\",\n            remove_unused_columns=False,\n            gradient_checkpointing=True\n            )\n\n    trainer = Trainer(\n            model=model,\n            args=training_args,\n            data_collator=collate_fn,\n            train_dataset=train_ds,\n            )\n    trainer.train()\n\nif __name__ == \"__main__\":\n    main()","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 smolvlm.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T20:42:59.330141Z","iopub.execute_input":"2025-02-05T20:42:59.330445Z","iopub.status.idle":"2025-02-05T20:45:53.728028Z","shell.execute_reply.started":"2025-02-05T20:42:59.330415Z","shell.execute_reply":"2025-02-05T20:45:53.727025Z"}},"outputs":[{"name":"stdout","text":"2025-02-05 20:43:08.985202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-05 20:43:08.987225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-05 20:43:09.008822: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-05 20:43:09.009222: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-05 20:43:09.015578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-05 20:43:09.015582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nREADME.md: 100%|███████████████████████████████| 403/403 [00:00<00:00, 2.01MB/s]\nvalidation-00000-of-00007.parquet: 100%|█████| 484M/484M [00:11<00:00, 42.9MB/s]\nvalidation-00001-of-00007.parquet: 100%|█████| 486M/486M [00:11<00:00, 41.9MB/s]\nvalidation-00002-of-00007.parquet: 100%|█████| 483M/483M [00:11<00:00, 42.3MB/s]\nvalidation-00003-of-00007.parquet: 100%|█████| 486M/486M [00:11<00:00, 41.5MB/s]\nvalidation-00004-of-00007.parquet: 100%|█████| 475M/475M [00:11<00:00, 41.3MB/s]\nvalidation-00005-of-00007.parquet: 100%|█████| 484M/484M [00:11<00:00, 42.3MB/s]\nvalidation-00006-of-00007.parquet: 100%|█████| 479M/479M [00:11<00:00, 41.5MB/s]\nGenerating validation split: 100%|█| 21435/21435 [00:05<00:00, 3643.01 examples/\nprocessor_config.json: 100%|██████████████████| 68.0/68.0 [00:00<00:00, 304kB/s]\nchat_template.json: 100%|██████████████████████| 429/429 [00:00<00:00, 1.93MB/s]\npreprocessor_config.json: 100%|████████████████| 486/486 [00:00<00:00, 2.34MB/s]\ntokenizer_config.json: 100%|████████████████| 28.2k/28.2k [00:00<00:00, 649kB/s]\nvocab.json: 100%|████████████████████████████| 801k/801k [00:00<00:00, 15.4MB/s]\nmerges.txt: 100%|████████████████████████████| 466k/466k [00:00<00:00, 9.70MB/s]\ntokenizer.json: 100%|██████████████████████| 3.55M/3.55M [00:00<00:00, 19.4MB/s]\nadded_tokens.json: 100%|███████████████████| 4.74k/4.74k [00:00<00:00, 17.6MB/s]\nspecial_tokens_map.json: 100%|█████████████| 1.07k/1.07k [00:00<00:00, 6.49MB/s]\nSome kwargs in processor config are unused and will not have any effect: image_seq_len. \nSome kwargs in processor config are unused and will not have any effect: image_seq_len. \nconfig.json: 100%|█████████████████████████| 7.35k/7.35k [00:00<00:00, 24.9MB/s]\nmodel.safetensors: 100%|█████████████████████| 513M/513M [00:12<00:00, 41.6MB/s]\ngeneration_config.json: 100%|███████████████████| 136/136 [00:00<00:00, 959kB/s]\n(3067776, 259552704)\n(3067776, 259552704)\n  0%|                                                   | 0/670 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/working/smolvlm.py\", line 120, in <module>\n[rank1]:     main()\n[rank1]:   File \"/kaggle/working/smolvlm.py\", line 117, in main\n[rank1]:     trainer.train()\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2164, in train\n[rank1]:     return inner_training_loop(\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2522, in _inner_training_loop\n[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3688, in training_step\n[rank1]:     self.accelerator.backward(loss, **kwargs)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2248, in backward\n[rank1]:     loss.backward(**kwargs)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n[rank1]:     torch.autograd.backward(\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n[rank1]:     _engine_run_backward(\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 307, in apply\n[rank1]:     return user_fn(self, *args)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 321, in backward\n[rank1]:     torch.autograd.backward(outputs_with_grad, args_with_grad)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n[rank1]:     _engine_run_backward(\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank1]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.\n[rank1]: Parameter at index 737 with name base_model.model.model.text_model.layers.29.mlp.down_proj.lora_magnitude_vector.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/smolvlm.py\", line 120, in <module>\n[rank0]:     main()\n[rank0]:   File \"/kaggle/working/smolvlm.py\", line 117, in main\n[rank0]:     trainer.train()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2164, in train\n[rank0]:     return inner_training_loop(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2522, in _inner_training_loop\n[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3688, in training_step\n[rank0]:     self.accelerator.backward(loss, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2248, in backward\n[rank0]:     loss.backward(**kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n[rank0]:     torch.autograd.backward(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n[rank0]:     _engine_run_backward(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 307, in apply\n[rank0]:     return user_fn(self, *args)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 321, in backward\n[rank0]:     torch.autograd.backward(outputs_with_grad, args_with_grad)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n[rank0]:     _engine_run_backward(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank0]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.\n[rank0]: Parameter at index 737 with name base_model.model.model.text_model.layers.29.mlp.down_proj.lora_magnitude_vector.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.\n  0%|                                                   | 0/670 [00:47<?, ?it/s]\n[rank0]:[W205 20:45:51.446900387 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0205 20:45:52.321000 158 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 167 closing signal SIGTERM\nE0205 20:45:52.636000 158 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 168) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"/usr/local/bin/accelerate\", line 8, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n    args.func(args)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1159, in launch_command\n    multi_gpu_launcher(args)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 793, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nsmolvlm.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-02-05_20:45:52\n  host      : c77eabf725ff\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 168)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n","output_type":"stream"}],"execution_count":21}]}
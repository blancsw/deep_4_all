{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP : Comparaison des modèles LSTM et RNN pour la classification de critiques de films Allociné\n",
    "\n",
    "## Contexte :\n",
    "\n",
    "En traitement automatique du langage naturel (NLP), les modèles récurrents comme les RNN (Réseaux de Neurones Récurrents) et les LSTM (Long Short Term Memory) sont couramment utilisés pour les tâches de classification de texte.\n",
    "\n",
    "Dans ce TP, vous allez comparer la performance de ces deux types de modèles sur une tâche de classification de critiques de films du site Allociné.\n",
    "\n",
    "Les modèles seront entraînés à prédire si une critique est positive ou négative en fonction du texte de la critique.\n",
    "\n",
    "## Objectifs :\n",
    "\n",
    "1. **Préparation des données :**\n",
    "    - Utilisez la librairie `datasets` pour charger le jeu de données `allocine`.\n",
    "    - Effectuez le prétraitement nécessaire sur les critiques (par exemple, la tokenisation).\n",
    "\n",
    "2. **Entraînement des modèles :**\n",
    "    - Implémentez un modèle RNN et un modèle LSTM en utilisant `PyTorch`.\n",
    "    - Entraînez les deux modèles sur vos données d'entraînement.\n",
    "\n",
    "3. **Évaluation des modèles :**\n",
    "    - Évaluez la performance de vos modèles sur vos données de test.\n",
    "    - Comparez la performance des deux modèles. Quel modèle a le mieux performé ? Pouvez-vous expliquer pourquoi ?\n",
    "\n",
    "4. **Interprétation des résultats:** Quels sont les aspects du modèle qui pourraient être améliorés?\n",
    "\n",
    "## Consignes :\n",
    "\n",
    "Vous êtes libres de choisir l'architecture exacte de vos modèles (nombre de couches, taille des couches, etc.), mais gardez à l'esprit que l'objectif est de comparer un RNN et un LSTM.\n",
    "  \n",
    "## Ressources :\n",
    "\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [Librairie `datasets`](https://huggingface.co/docs/datasets/)\n",
    "\n",
    "## Bonne chance !\n",
    "\n",
    "# Code de base"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9262641b1de01de1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allocine\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d101b54da33050e0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset['review'][0], train_dataset['label'][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb7c138f622431c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the spacy model for the tokenizer\n",
    "!python -m spacy download fr_core_news_sm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db969ddb57202998",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how to creat a simple tokenizer with vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3dc50528faf33c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language=\"fr\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text.lower())\n",
    "\n",
    "\n",
    "texts = [\"Bonjour je vais très bien\", \"Le lisp et trop cool !\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(texts), specials=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(f\"Vocabulary: {vocab.get_itos()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efa86d5c121e6ffb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To test hour vocabulary\n",
    "\n",
    "sentence = \"TOTO je vais très bien le lisp TOTO\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer(sentence)\n",
    "\n",
    "# Convert tokens to indices via the vocab\n",
    "encoded_sentence = [vocab[token] for token in tokens]\n",
    "\n",
    "print(f\"Encoded sentence: {encoded_sentence}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3734fedcb4c038",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM & RNN USAGE\n",
    "\n",
    "Voici un code simple pour utilisez un RNN et LSTM\n",
    "\n",
    "### RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b9f733f6cf53db2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 5\n",
    "sequence_length = 10\n",
    "embedding_dim = 8\n",
    "num_layers = 2\n",
    "hidden_size = 20\n",
    "# On crée des tokens aléatoire pour la demo (int entre 0 et vocab_size)\n",
    "random_tokens = [random.randint(0, vocab_size) for _ in range(sequence_length)]\n",
    "# ON crée notre Embedding num_embeddings et le nombres de mots dans notre vocabulaire\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=embedding_dim)\n",
    "\n",
    "# Instantiate an RNN with input size equal to embedding_dim, hidden state size 20, and 2 layers.\n",
    "rnn = nn.RNN(input_size=embedding_dim,\n",
    "             hidden_size=hidden_size,\n",
    "             num_layers=num_layers,\n",
    "             # [batch, seq len, features]\n",
    "             batch_first=True)\n",
    "\n",
    "# Initialize the hidden state.\n",
    "# [num_layers, batch size, hidden_size]\n",
    "h_0 = torch.randn(num_layers, 1, hidden_size)\n",
    "\n",
    "# Forward propagate the RNN with input and initial hidden state.\n",
    "embeddings = embedding(torch.tensor([random_tokens]))\n",
    "print(\"embeddings\", embeddings.size())\n",
    "output, hn = rnn(embeddings, h_0)\n",
    "print(\"Output: \", output.shape)\n",
    "print(\"hn: \", hn.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14a751b76061d14b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8029e42a03bd9a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "sequence_length = 10\n",
    "embedding_dim = 8\n",
    "num_layers = 2\n",
    "hidden_size = 20\n",
    "random_tokens = [random.randint(0, vocab_size) for _ in range(sequence_length)]\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=embedding_dim)\n",
    "\n",
    "# Instantiate an LSTM with input size equal to embedding_dim, hidden state size 20, and 2 layers.\n",
    "lstm = nn.LSTM(input_size=embedding_dim,\n",
    "               hidden_size=hidden_size,\n",
    "               num_layers=num_layers,\n",
    "               # [batch, seq len, features]\n",
    "               batch_first=True)\n",
    "\n",
    "# Initialize the hidden state and cell state.\n",
    "# [num_layers, batch size, hidden_size]\n",
    "h_0 = torch.randn(num_layers, 1, hidden_size)\n",
    "c_0 = torch.randn(num_layers, 1, hidden_size)\n",
    "\n",
    "# Forward propagate the RNN with input and initial hidden state.\n",
    "embeddings = embedding(torch.tensor([random_tokens]))\n",
    "print(\"embeddings\", embeddings.size())\n",
    "output, (hn, cn) = lstm(embeddings, (h_0, c_0))\n",
    "print(\"Output: \", output.shape)\n",
    "print(\"hn: \", hn.shape)\n",
    "print(\"cn: \", cn.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30eb56169e948035",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

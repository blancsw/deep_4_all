{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP: Next Letter Prediction\n",
    "\n",
    "Dans ce TP, vous allez entraîner un modèle pour prédire le caractère suivant.\n",
    "\n",
    "## Objectifs du TP\n",
    "\n",
    "1. Modifier le prétraitement du dataset\n",
    "2. Convertir des tokens en id afin de créer un vocabulaire\n",
    "3. Se familiariser avec la couche d'embedding\n",
    "4. Modifier un réseau pour l'entraîner d'une autre manière\n",
    "\n",
    "## Guide\n",
    "\n",
    "### 1\n",
    "\n",
    "Modifier le code existant afin de donner une lettre d'entrée au modèle:\n",
    "\n",
    "Example: Le chat\n",
    "\n",
    "```\n",
    "'l', 'e' --> model --> ' '\n",
    "'e', ' ' --> model --> 'c'\n",
    "' ', 'c' --> model --> 'h'\n",
    "```\n",
    "\n",
    "### 2\n",
    "\n",
    "Même chose que la question 1, mais en passant avec 3 lettres en entrée:\n",
    "\n",
    "Example: Le chat\n",
    "\n",
    "```\n",
    "'l', 'e', ' ' --> model --> 'c'\n",
    "'e', ' ', 'c' --> model --> 'h'\n",
    "```\n",
    "\n",
    "### 3\n",
    "\n",
    "Maintenant, nous allons essayer une autre technique d'entraînement. L'idée sera de masquer aléatoirement une des 3 lettres en entrée et le modèle devra deviner laquelle c'est.\n",
    "\n",
    "Example: Le chat\n",
    "\n",
    "```\n",
    "'l', '[MASK]', ' ' --> model --> 'e'\n",
    "'e', ' ', '[MASK]' --> model --> 'c'\n",
    "```\n",
    "\n",
    "Pour cela, ajouter un nouvel token id 0 (ou autre chose) afin de représenter le token de masque.\n",
    "\n",
    "Crée une nouvelle classe `data.Dataset`.\n",
    "\n",
    "### 4\n",
    "\n",
    "Tentez d'augmenter la dimension de `embedding_size` et ajoutez des couches `torch.nn.Linear` supplémentaires.\n",
    "Cela vous permettra de voir comment la loss évolue.\n",
    "\n",
    "# Code de base\n",
    "\n",
    "Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b1b8ae264e3b7f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and take subset of the dataset\n",
    "df = pd.read_csv(\"../../CM/data/disney_review/train.csv\")[:5000]\n",
    "# Get reviews\n",
    "reviews = df[\"Review_Text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare the datas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df23a4b1bb4ec85"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import re\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH = 3\n",
    "\n",
    "\n",
    "def sliding_window(txt, input_len):\n",
    "    \"\"\"\n",
    "    Generates a sliding window of consecutive character pairs from the given text.\n",
    "\n",
    "    Parameters:\n",
    "    txt (str): The input text.\n",
    "\n",
    "    Yields:\n",
    "    tuple: A pair of consecutive characters from the text. The first character is at index i and the second character\n",
    "    is at index i + 1.\n",
    "\n",
    "    Example:\n",
    "    >>> for pair in sliding_window(\"hello\"):\n",
    "    ...     print(pair)\n",
    "    ...\n",
    "    ('h', 'e')\n",
    "    ('e', 'l')\n",
    "    ('l', 'l')\n",
    "    ('l', 'o')\n",
    "    \"\"\"\n",
    "    for i in range(len(txt) - input_len):\n",
    "        yield txt[i:i + input_len], txt[i + input_len]\n",
    "\n",
    "\n",
    "window = []\n",
    "for title in reviews:\n",
    "    # Get only a to z and 0 to 9 letters and numerb\n",
    "    title = re.sub('[^a-zA-Z0-9 ]+', '', title.lower())\n",
    "    window.append(sliding_window(title, INPUT_SEQUENCE_LENGTH))\n",
    "window = list(it.chain(*window))\n",
    "\n",
    "# Number of window\n",
    "print(len(window))\n",
    "# Get first 5 example\n",
    "window[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f347453310ed54",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "creat letter to ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a495e9d3991e92b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocabulary = \"\"\n",
    "# Loop over all windows\n",
    "for w in window:\n",
    "    # Creat a big string to get all the uniq chars\n",
    "    vocabulary += w[0]\n",
    "\n",
    "# convert to set to remove all duplicated char\n",
    "vocabulary = set(vocabulary)\n",
    "\n",
    "# Creat mapping char to is id\n",
    "# Add i+1 because the id 0 will be the mask token\n",
    "mapping = {c: i + 1 for i, c in enumerate(vocabulary)}\n",
    "# Creat the mask token\n",
    "mapping['[MASK]'] = 0\n",
    "\n",
    "integers_in = []\n",
    "integers_out = []\n",
    "for w in window:\n",
    "    # Loop over our char sequence, to convert into ids\n",
    "    sequence = []\n",
    "    for char in w[0]:\n",
    "        sequence.append(mapping[char])\n",
    "    integers_out.append(mapping[w[1]])\n",
    "    integers_in.append(sequence)\n",
    "\n",
    "integers_in = np.asarray(integers_in)\n",
    "integers_out = np.asarray(integers_out)\n",
    "print(\"Shape of input (number of examples, seq len of the input)\", integers_in.shape)\n",
    "print(\"Input example\", integers_in[0], integers_out[0])\n",
    "print(\"Show generate mapping\\n\", mapping)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e6eaff290bcac42",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creat the dataset class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "121055b37895cb8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class NextLetterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    NextLetterDataset\n",
    "\n",
    "    A custom dataset class for next letter prediction.\n",
    "\n",
    "    Attributes:\n",
    "        integers_in (list): A list of integers representing input data.\n",
    "        integers_out (list): A list of integers representing output data.\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of data points in the dataset.\n",
    "        __getitem__(idx: int): Returns the idx-th data point and its corresponding label.\n",
    "\n",
    "    Example usage:\n",
    "        integers_in = [1, 2, 3]\n",
    "        integers_out = [7, 8, 9]\n",
    "        dataset = NextLetterDataset(integers_in, integers_in2, integers_out)\n",
    "        print(len(dataset))  # Output: 3\n",
    "        print(dataset[0])  # Output: (tensor(1), tensor(7))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, integers_in, integers_out):\n",
    "        self.integers_in = integers_in\n",
    "        self.integers_out = integers_out\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data pin dataset\n",
    "        return len(self.integers_in)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.integers_in[idx]\n",
    "        data_label = self.integers_out[idx]\n",
    "        return torch.tensor(data_point), torch.tensor(data_label, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c429c6f7840ff7d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class RandomMaskLetterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    A class that represents a random mask letter dataset.\n",
    "\n",
    "    Attributes:\n",
    "        integers_in (list): A list of input sequences.\n",
    "        mask_token_id (int): The ID of the mask token.\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of data points in the dataset.\n",
    "        __getitem__(idx): Returns the data point with random token replace by the mask token\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, integers_in, mask_token_id):\n",
    "        self.integers_in = integers_in\n",
    "        self.mask_token_id = mask_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data pin dataset\n",
    "        return len(self.integers_in)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Copy the input sequences\n",
    "        data_point = self.integers_in[idx].copy()\n",
    "        # Pick random index from the input list\n",
    "        random_index = random.randint(0, len(data_point) - 1)\n",
    "        # Get the label\n",
    "        data_label = data_point[random_index]\n",
    "        # Replace with mask token\n",
    "        data_point[random_index] = self.mask_token_id\n",
    "        return torch.tensor(data_point), torch.tensor(data_label, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "320394bd022557fb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mask_dataset = RandomMaskLetterDataset(integers_in, mapping['[MASK]'])\n",
    "print('Input before masking: ', integers_in[0])\n",
    "print('Input with masking: ', mask_dataset[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b4c71b3a04d4942",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff39e9ea8bdf38b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class NextLetterPrediction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    This class represents a next letter prediction model based on a neural network architecture.\n",
    "\n",
    "    Attributes:\n",
    "    - embedding: An instance of torch.nn.Embedding representing the embedding layer.\n",
    "    - fc: An instance of torch.nn.Linear representing the fully connected layer.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, vocab_size, embedding_size):\n",
    "        Initializes the NextLetterPrediction class with the given vocabulary size and embedding size.\n",
    "\n",
    "        Parameters:\n",
    "            - vocab_size: An integer representing the size of the vocabulary (number of characters).\n",
    "            - input_sequence_len: Number of characters in the input sequence\n",
    "            - embedding_size: An integer representing the size of the embedding layer.\n",
    "\n",
    "    - forward(self, x):\n",
    "        Performs a forward pass on the model.\n",
    "\n",
    "        Parameters:\n",
    "            x: The input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            The output tensor of shape (batch_size, vocab_size).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, input_sequence_len, embedding_size):\n",
    "        super(NextLetterPrediction, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc = torch.nn.Linear(embedding_size * input_sequence_len, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input char tensor of shape (batch_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Next letter prediction tensor of shape (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        # shape: (batch_size, sequence_len, embedding_size)\n",
    "        x = F.relu(self.embedding(x))\n",
    "        # shape: (batch_size, sequence_len * embedding_size)\n",
    "        # Start dim 1 because we dont want to flatten et the batch size\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84f9768e5c34a040",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Init the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f20fce5cdb1c6180"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = NextLetterPrediction(vocab_size=len(mapping),\n",
    "                             input_sequence_len=INPUT_SEQUENCE_LENGTH,\n",
    "                             # For X and Y plot\n",
    "                             embedding_size=50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69c75609ee1557e3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RandomMaskLetterDataset train\n",
    "\n",
    "Do a random masking\n",
    "\n",
    "**Note** Run this before the NextLetterPrediction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569062e5d490b64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "dataset = RandomMaskLetterDataset(integers_in, mapping['[MASK]'])\n",
    "trainloader = data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f39658ebaa656b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NextLetterPrediction train\n",
    "\n",
    "Do the next letter prediction training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36e6eab599f936b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Init the dataset into the DataLoader\n",
    "dataset = NextLetterDataset(integers_in, integers_out)\n",
    "trainloader = data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c5e9554e3c1e9d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's see the output of our dataset class\n",
    "# Expected shape: (3 token ids, The id to predict)\n",
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67556eea0dfd4a88",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10a244d56a255f49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "epoches = 2\n",
    "\n",
    "# Get cpu/gpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Load the model to cuda device in train mode\n",
    "model.to(device)\n",
    "model.train()\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(epoches):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "        # shape: (batch_size, input_sequence_len)\n",
    "        inputs = inputs.to(device)\n",
    "        # shape: (batch_size)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f170182c3d760747",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "idx_to_calc = list(mapping.values())\n",
    "idx_to_calc = np.array([idx_to_calc]).T\n",
    "\n",
    "translator = {v: k for k, v in mapping.items()}\n",
    "preds = model.embedding(torch.tensor(idx_to_calc).to(device)).cpu().detach().numpy()\n",
    "plt.scatter(preds[:, 0, 0], preds[:, 0, 1], alpha=0)\n",
    "for i, idx in enumerate(idx_to_calc):\n",
    "    plt.text(preds[i, 0, 0], preds[i, 0, 1], translator[idx[0]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f069bc53b9aa906b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.mask_token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b763591438d98b89",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

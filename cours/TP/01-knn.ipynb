{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3520f4ff4b4f589",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# TP en Traitement Automatique du Langage Naturel : Classification des Sentiments sur des Critiques de Films\n",
    "\n",
    "## Objectif\n",
    "Développer un système de classification des sentiments en utilisant des critiques de films. Vous utiliserez l'ensemble de données IMDb et appliquerez un modèle **K-Nearest Neighbors (KNN)** pour classer les critiques en catégories positives ou négatives.\n",
    "\n",
    "---\n",
    "\n",
    "## Questions\n",
    "1. Comment la réduction du nombre de caractéristiques (`max_features`) influence-t-elle la performance du modèle ?\n",
    "2. Quel est l'impact du choix du **nombre de voisins** dans KNN sur les résultats ?\n",
    "3. Comparez les performances du modèle KNN avec un autre classificateur (par exemple, [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) ou [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)). Lequel fonctionne le mieux et pourquoi ?\n",
    "4. Le prétraitement améliore-t-il la classification ? Justifiez votre réponse avec des résultats expérimentaux.\n",
    "\n",
    "---\n",
    "\n",
    "## Ressources Utiles\n",
    "- [Ensemble de données IMDb](https://huggingface.co/datasets/imdb)\n",
    "- [Documentation Scikit-learn](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ccbdee81a84e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T09:48:03.859353Z",
     "start_time": "2025-02-05T09:47:52.279452Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting en-core-web-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "!pip install -q -U datasets scikit-learn spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bf4b70f36e01ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:01:23.673126Z",
     "start_time": "2025-02-05T10:01:01.733610Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import copy\n",
    "\n",
    "# On split plusieur fois le dataset afin de réduire le temps de calcule\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "original_dataset = dataset.train_test_split(stratify_by_column=\"label\", test_size=0.5, seed=42)\n",
    "clean_dataset = copy.deepcopy(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfbe9abf021534d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:01:26.178350Z",
     "start_time": "2025-02-05T10:01:25.727723Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "180fe80be0f906cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:01:28.051987Z",
     "start_time": "2025-02-05T10:01:28.047730Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_sentence_batch(texts):\n",
    "    # Utilise le pipeline NLP pour traiter un lot de textes (texts) en désactivant les composants inutiles (analyseur et reconnaissance d'entités nommées négligés pour améliorer la performance)\n",
    "    docs = nlp.pipe(texts, disable=[\"parser\", \"ner\"])\n",
    "\n",
    "    # Initialisation d'une liste pour stocker les résultats des textes nettoyés\n",
    "    result = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Parcours de chaque document renvoyé par le pipeline NLP\n",
    "        clean_doc = []  # Initialisation d'une liste pour le document nettoyé\n",
    "        for token in doc:\n",
    "            # Pour chaque token du document :\n",
    "            # - Le token est ignoré s'il est un mot vide (stopword) ou un signe de ponctuation\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                # Ajoute le lemme (forme de base du mot) au document nettoyé\n",
    "                clean_doc.append(token.lemma_)\n",
    "        # Joint les lemmes en une chaîne et ajoute le résultat à la liste finale\n",
    "        result.append(\" \".join(clean_doc))\n",
    "\n",
    "    # Retourne la liste contenant les phrases nettoyées\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe430cdc1baa5bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:01:29.838939Z",
     "start_time": "2025-02-05T10:01:29.824808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence', 'sentence', 'sentence stopword punctuation number']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence_batch([\"This is a sentence\",\n",
    "                      \"This is another sentence\",\n",
    "                      \"This is a third sentence with stopwords, punctuation, and numbers.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd6ce8c108974af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:06:03.561433Z",
     "start_time": "2025-02-05T10:01:32.281291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b89459d027485cb6a411e42cfce319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4842c996ab4d27aa6d0b7239fa642b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nettoie les phrases dans le jeu de données d'entraînement en utilisant la fonction `clean_sentence_batch`\n",
    "clean_dataset['train'] = clean_dataset['train'].map(\n",
    "        lambda x: {\"text\": clean_sentence_batch(x[\"text\"])},\n",
    "        # Applique le nettoyage sur la colonne \"text\" en mode batch\n",
    "        batched=True  # Permet le traitement par lots pour améliorer les performances\n",
    "        )\n",
    "\n",
    "# Nettoie les phrases dans le jeu de données de test de la même manière\n",
    "clean_dataset['test'] = clean_dataset['test'].map(\n",
    "        lambda x: {\"text\": clean_sentence_batch(x[\"text\"])},  # Applique la même transformation au jeu de test\n",
    "        batched=True  # Assure également un traitement par lots ici\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2de1ee9aa775312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:06:05.359821Z",
     "start_time": "2025-02-05T10:06:05.356142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c771147747d7f0be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:06:12.460720Z",
     "start_time": "2025-02-05T10:06:12.456760Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really enjoyed the performances of the main cast. Emma Lung is courageous and interesting. The director has developed performances where the characters are not one dimensional. A complex story with the changing between eras. Also appreciated the underlying story of the unions losing power and the effect of a large employer closing on a small town. I do not agree with the comment that the older man has to be attractive. There have be many relationships with older men and younger women - without the male being good looking. Depth of character can be appealing to the not so shallow. The film has a good look and the cinematography is also good.\n",
      "\n",
      "enjoy performance main cast Emma Lung courageous interesting director develop performance character dimensional complex story changing era appreciate underlie story union lose power effect large employer close small town agree comment old man attractive relationship old man young woman male good look depth character appeal shallow film good look cinematography good\n"
     ]
    }
   ],
   "source": [
    "print(original_dataset['train'][0]['text'])\n",
    "print()\n",
    "print(clean_dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3775e39bb47f4b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:06:24.261391Z",
     "start_time": "2025-02-05T10:06:24.255412Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Constantes\n",
    "TFIDF_MAX_FEATURES = 1000  # Nombre maximal de caractéristiques à extraire en mode TF-IDF\n",
    "KNN_NEIGHBORS = 100  # Nombre de voisins à considérer dans l'algorithme KNN\n",
    "\n",
    "\n",
    "def vectorize_text(train_texts, test_texts, max_features):\n",
    "    \"\"\"\n",
    "    Vectorise les textes d'entraînement et de test à l'aide de TF-IDF.\n",
    "    Chaque document est représenté par un vecteur de caractéristiques numériques.\n",
    "\n",
    "    Paramètres :\n",
    "    - train_texts (list): Liste des textes de l'ensemble d'entraînement.\n",
    "    - test_texts (list): Liste des textes de l'ensemble de test.\n",
    "    - max_features (int): Nombre maximal de caractéristiques considérées par le vectoriseur.\n",
    "\n",
    "    Retourne :\n",
    "    - X_train (sparse matrix): Matrice de caractéristiques pour les données d'entraînement.\n",
    "    - X_test (sparse matrix): Matrice de caractéristiques pour les données de test.\n",
    "    - vectorizer (TfidfVectorizer): Instance du vectoriseur TF-IDF entraînée pour réutilisation.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features)  # Initialisation du vectoriseur avec un seuil max de caractéristiques\n",
    "    X_train = vectorizer.fit_transform(train_texts)  # Calcul et transformation des textes d'entraînement\n",
    "    X_test = vectorizer.transform(test_texts)  # Transformation des textes de test avec le même vectoriseur\n",
    "    return X_train, X_test, vectorizer\n",
    "\n",
    "\n",
    "def train_and_evaluate_knn(X_train, y_train, X_test, y_test, n_neighbors):\n",
    "    \"\"\"\n",
    "    Entraîne et évalue un modèle KNN pour la classification des textes.\n",
    "\n",
    "    Paramètres :\n",
    "    - X_train (sparse matrix): Matrice des caractéristiques pour l'entraînement.\n",
    "    - y_train (array): Étiquettes associées aux données d'entraînement.\n",
    "    - X_test (sparse matrix): Matrice des caractéristiques pour les données de test.\n",
    "    - y_test (array): Étiquettes associées aux données de test.\n",
    "    - n_neighbors (int): Nombre de voisins pris en compte dans l'algorithme KNN.\n",
    "\n",
    "    Comportement :\n",
    "    - Entraîne le modèle sur les données d'entraînement.\n",
    "    - Effectue des prédictions sur les données de test.\n",
    "    - Affiche un rapport de classification basé sur les prédictions.\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(\n",
    "            n_neighbors=n_neighbors)  # Initialisation du classificateur KNN avec un certain nombre de voisins\n",
    "    knn.fit(X_train, y_train)  # Entraînement du modèle sur les données d'entrée\n",
    "    y_pred = knn.predict(X_test)  # Prédictions sur les données de test\n",
    "    print(classification_report(y_test, y_pred))  # Affichage des métriques de classification (précision, rappel, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e03508b900892f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:06:47.241663Z",
     "start_time": "2025-02-05T10:06:28.802486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.62      0.70      6250\n",
      "           1       0.69      0.83      0.75      6250\n",
      "\n",
      "    accuracy                           0.73     12500\n",
      "   macro avg       0.74      0.73      0.73     12500\n",
      "weighted avg       0.74      0.73      0.73     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flux principal du script\n",
    "# Vectorisation des textes d'entraînement et de test\n",
    "X_train, X_test, vectorizer = vectorize_text(original_dataset['train']['text'], original_dataset['test']['text'], TFIDF_MAX_FEATURES)\n",
    "\n",
    "# Séparation des étiquettes pour les ensembles d'entraînement et de test\n",
    "y_train, y_test = original_dataset['train']['label'], original_dataset['test']['label']\n",
    "\n",
    "# Formation et évaluation du modèle KNN avec les données vectorisées\n",
    "train_and_evaluate_knn(X_train, y_train, X_test, y_test, KNN_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21c61509c001fd10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:06:56.440950Z",
     "start_time": "2025-02-05T10:06:47.338713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.77      6250\n",
      "           1       0.80      0.69      0.74      6250\n",
      "\n",
      "    accuracy                           0.76     12500\n",
      "   macro avg       0.76      0.76      0.76     12500\n",
      "weighted avg       0.76      0.76      0.76     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flux principal du script\n",
    "# Vectorisation des textes d'entraînement et de test\n",
    "X_train, X_test, vectorizer = vectorize_text(clean_dataset['train']['text'], clean_dataset['test']['text'], TFIDF_MAX_FEATURES)\n",
    "\n",
    "# Séparation des étiquettes pour les ensembles d'entraînement et de test\n",
    "y_train, y_test = clean_dataset['train']['label'], clean_dataset['test']['label']\n",
    "\n",
    "# Formation et évaluation du modèle KNN avec les données vectorisées\n",
    "train_and_evaluate_knn(X_train, y_train, X_test, y_test, KNN_NEIGHBORS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
